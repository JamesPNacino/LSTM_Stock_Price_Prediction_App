<!DOCTYPE html>

<html lang="en">
<head><meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Data Analytics Report and Executive Summaryv4</title><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>
<style type="text/css">
    pre { line-height: 125%; }
td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
.highlight .hll { background-color: var(--jp-cell-editor-active-background) }
.highlight { background: var(--jp-cell-editor-background); color: var(--jp-mirror-editor-variable-color) }
.highlight .c { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment */
.highlight .err { color: var(--jp-mirror-editor-error-color) } /* Error */
.highlight .k { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword */
.highlight .o { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator */
.highlight .p { color: var(--jp-mirror-editor-punctuation-color) } /* Punctuation */
.highlight .ch { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Multiline */
.highlight .cp { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Preproc */
.highlight .cpf { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Single */
.highlight .cs { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Special */
.highlight .kc { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Pseudo */
.highlight .kr { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Type */
.highlight .m { color: var(--jp-mirror-editor-number-color) } /* Literal.Number */
.highlight .s { color: var(--jp-mirror-editor-string-color) } /* Literal.String */
.highlight .ow { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator.Word */
.highlight .pm { color: var(--jp-mirror-editor-punctuation-color) } /* Punctuation.Marker */
.highlight .w { color: var(--jp-mirror-editor-variable-color) } /* Text.Whitespace */
.highlight .mb { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Bin */
.highlight .mf { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Float */
.highlight .mh { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Hex */
.highlight .mi { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer */
.highlight .mo { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Oct */
.highlight .sa { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Affix */
.highlight .sb { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Backtick */
.highlight .sc { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Char */
.highlight .dl { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Delimiter */
.highlight .sd { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Doc */
.highlight .s2 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Double */
.highlight .se { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Escape */
.highlight .sh { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Heredoc */
.highlight .si { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Interpol */
.highlight .sx { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Other */
.highlight .sr { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Regex */
.highlight .s1 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Single */
.highlight .ss { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Symbol */
.highlight .il { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer.Long */
  </style>
<style type="text/css">
/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*
 * Mozilla scrollbar styling
 */

/* use standard opaque scrollbars for most nodes */
[data-jp-theme-scrollbars='true'] {
  scrollbar-color: rgb(var(--jp-scrollbar-thumb-color))
    var(--jp-scrollbar-background-color);
}

/* for code nodes, use a transparent style of scrollbar. These selectors
 * will match lower in the tree, and so will override the above */
[data-jp-theme-scrollbars='true'] .CodeMirror-hscrollbar,
[data-jp-theme-scrollbars='true'] .CodeMirror-vscrollbar {
  scrollbar-color: rgba(var(--jp-scrollbar-thumb-color), 0.5) transparent;
}

/* tiny scrollbar */

.jp-scrollbar-tiny {
  scrollbar-color: rgba(var(--jp-scrollbar-thumb-color), 0.5) transparent;
  scrollbar-width: thin;
}

/* tiny scrollbar */

.jp-scrollbar-tiny::-webkit-scrollbar,
.jp-scrollbar-tiny::-webkit-scrollbar-corner {
  background-color: transparent;
  height: 4px;
  width: 4px;
}

.jp-scrollbar-tiny::-webkit-scrollbar-thumb {
  background: rgba(var(--jp-scrollbar-thumb-color), 0.5);
}

.jp-scrollbar-tiny::-webkit-scrollbar-track:horizontal {
  border-left: 0 solid transparent;
  border-right: 0 solid transparent;
}

.jp-scrollbar-tiny::-webkit-scrollbar-track:vertical {
  border-top: 0 solid transparent;
  border-bottom: 0 solid transparent;
}

/*
 * Lumino
 */

.lm-ScrollBar[data-orientation='horizontal'] {
  min-height: 16px;
  max-height: 16px;
  min-width: 45px;
  border-top: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='vertical'] {
  min-width: 16px;
  max-width: 16px;
  min-height: 45px;
  border-left: 1px solid #a0a0a0;
}

.lm-ScrollBar-button {
  background-color: #f0f0f0;
  background-position: center center;
  min-height: 15px;
  max-height: 15px;
  min-width: 15px;
  max-width: 15px;
}

.lm-ScrollBar-button:hover {
  background-color: #dadada;
}

.lm-ScrollBar-button.lm-mod-active {
  background-color: #cdcdcd;
}

.lm-ScrollBar-track {
  background: #f0f0f0;
}

.lm-ScrollBar-thumb {
  background: #cdcdcd;
}

.lm-ScrollBar-thumb:hover {
  background: #bababa;
}

.lm-ScrollBar-thumb.lm-mod-active {
  background: #a0a0a0;
}

.lm-ScrollBar[data-orientation='horizontal'] .lm-ScrollBar-thumb {
  height: 100%;
  min-width: 15px;
  border-left: 1px solid #a0a0a0;
  border-right: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='vertical'] .lm-ScrollBar-thumb {
  width: 100%;
  min-height: 15px;
  border-top: 1px solid #a0a0a0;
  border-bottom: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='horizontal']
  .lm-ScrollBar-button[data-action='decrement'] {
  background-image: var(--jp-icon-caret-left);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='horizontal']
  .lm-ScrollBar-button[data-action='increment'] {
  background-image: var(--jp-icon-caret-right);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='vertical']
  .lm-ScrollBar-button[data-action='decrement'] {
  background-image: var(--jp-icon-caret-up);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='vertical']
  .lm-ScrollBar-button[data-action='increment'] {
  background-image: var(--jp-icon-caret-down);
  background-size: 17px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-Widget {
  box-sizing: border-box;
  position: relative;
  overflow: hidden;
}

.lm-Widget.lm-mod-hidden {
  display: none !important;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.lm-AccordionPanel[data-orientation='horizontal'] > .lm-AccordionPanel-title {
  /* Title is rotated for horizontal accordion panel using CSS */
  display: block;
  transform-origin: top left;
  transform: rotate(-90deg) translate(-100%);
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-CommandPalette {
  display: flex;
  flex-direction: column;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-CommandPalette-search {
  flex: 0 0 auto;
}

.lm-CommandPalette-content {
  flex: 1 1 auto;
  margin: 0;
  padding: 0;
  min-height: 0;
  overflow: auto;
  list-style-type: none;
}

.lm-CommandPalette-header {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.lm-CommandPalette-item {
  display: flex;
  flex-direction: row;
}

.lm-CommandPalette-itemIcon {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemContent {
  flex: 1 1 auto;
  overflow: hidden;
}

.lm-CommandPalette-itemShortcut {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemLabel {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.lm-close-icon {
  border: 1px solid transparent;
  background-color: transparent;
  position: absolute;
  z-index: 1;
  right: 3%;
  top: 0;
  bottom: 0;
  margin: auto;
  padding: 7px 0;
  display: none;
  vertical-align: middle;
  outline: 0;
  cursor: pointer;
}
.lm-close-icon:after {
  content: 'X';
  display: block;
  width: 15px;
  height: 15px;
  text-align: center;
  color: #000;
  font-weight: normal;
  font-size: 12px;
  cursor: pointer;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-DockPanel {
  z-index: 0;
}

.lm-DockPanel-widget {
  z-index: 0;
}

.lm-DockPanel-tabBar {
  z-index: 1;
}

.lm-DockPanel-handle {
  z-index: 2;
}

.lm-DockPanel-handle.lm-mod-hidden {
  display: none !important;
}

.lm-DockPanel-handle:after {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  content: '';
}

.lm-DockPanel-handle[data-orientation='horizontal'] {
  cursor: ew-resize;
}

.lm-DockPanel-handle[data-orientation='vertical'] {
  cursor: ns-resize;
}

.lm-DockPanel-handle[data-orientation='horizontal']:after {
  left: 50%;
  min-width: 8px;
  transform: translateX(-50%);
}

.lm-DockPanel-handle[data-orientation='vertical']:after {
  top: 50%;
  min-height: 8px;
  transform: translateY(-50%);
}

.lm-DockPanel-overlay {
  z-index: 3;
  box-sizing: border-box;
  pointer-events: none;
}

.lm-DockPanel-overlay.lm-mod-hidden {
  display: none !important;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-Menu {
  z-index: 10000;
  position: absolute;
  white-space: nowrap;
  overflow-x: hidden;
  overflow-y: auto;
  outline: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-Menu-content {
  margin: 0;
  padding: 0;
  display: table;
  list-style-type: none;
}

.lm-Menu-item {
  display: table-row;
}

.lm-Menu-item.lm-mod-hidden,
.lm-Menu-item.lm-mod-collapsed {
  display: none !important;
}

.lm-Menu-itemIcon,
.lm-Menu-itemSubmenuIcon {
  display: table-cell;
  text-align: center;
}

.lm-Menu-itemLabel {
  display: table-cell;
  text-align: left;
}

.lm-Menu-itemShortcut {
  display: table-cell;
  text-align: right;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-MenuBar {
  outline: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-MenuBar-content {
  margin: 0;
  padding: 0;
  display: flex;
  flex-direction: row;
  list-style-type: none;
}

.lm-MenuBar-item {
  box-sizing: border-box;
}

.lm-MenuBar-itemIcon,
.lm-MenuBar-itemLabel {
  display: inline-block;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-ScrollBar {
  display: flex;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-ScrollBar[data-orientation='horizontal'] {
  flex-direction: row;
}

.lm-ScrollBar[data-orientation='vertical'] {
  flex-direction: column;
}

.lm-ScrollBar-button {
  box-sizing: border-box;
  flex: 0 0 auto;
}

.lm-ScrollBar-track {
  box-sizing: border-box;
  position: relative;
  overflow: hidden;
  flex: 1 1 auto;
}

.lm-ScrollBar-thumb {
  box-sizing: border-box;
  position: absolute;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-SplitPanel-child {
  z-index: 0;
}

.lm-SplitPanel-handle {
  z-index: 1;
}

.lm-SplitPanel-handle.lm-mod-hidden {
  display: none !important;
}

.lm-SplitPanel-handle:after {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  content: '';
}

.lm-SplitPanel[data-orientation='horizontal'] > .lm-SplitPanel-handle {
  cursor: ew-resize;
}

.lm-SplitPanel[data-orientation='vertical'] > .lm-SplitPanel-handle {
  cursor: ns-resize;
}

.lm-SplitPanel[data-orientation='horizontal'] > .lm-SplitPanel-handle:after {
  left: 50%;
  min-width: 8px;
  transform: translateX(-50%);
}

.lm-SplitPanel[data-orientation='vertical'] > .lm-SplitPanel-handle:after {
  top: 50%;
  min-height: 8px;
  transform: translateY(-50%);
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-TabBar {
  display: flex;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-TabBar[data-orientation='horizontal'] {
  flex-direction: row;
  align-items: flex-end;
}

.lm-TabBar[data-orientation='vertical'] {
  flex-direction: column;
  align-items: flex-end;
}

.lm-TabBar-content {
  margin: 0;
  padding: 0;
  display: flex;
  flex: 1 1 auto;
  list-style-type: none;
}

.lm-TabBar[data-orientation='horizontal'] > .lm-TabBar-content {
  flex-direction: row;
}

.lm-TabBar[data-orientation='vertical'] > .lm-TabBar-content {
  flex-direction: column;
}

.lm-TabBar-tab {
  display: flex;
  flex-direction: row;
  box-sizing: border-box;
  overflow: hidden;
  touch-action: none; /* Disable native Drag/Drop */
}

.lm-TabBar-tabIcon,
.lm-TabBar-tabCloseIcon {
  flex: 0 0 auto;
}

.lm-TabBar-tabLabel {
  flex: 1 1 auto;
  overflow: hidden;
  white-space: nowrap;
}

.lm-TabBar-tabInput {
  user-select: all;
  width: 100%;
  box-sizing: border-box;
}

.lm-TabBar-tab.lm-mod-hidden {
  display: none !important;
}

.lm-TabBar-addButton.lm-mod-hidden {
  display: none !important;
}

.lm-TabBar.lm-mod-dragging .lm-TabBar-tab {
  position: relative;
}

.lm-TabBar.lm-mod-dragging[data-orientation='horizontal'] .lm-TabBar-tab {
  left: 0;
  transition: left 150ms ease;
}

.lm-TabBar.lm-mod-dragging[data-orientation='vertical'] .lm-TabBar-tab {
  top: 0;
  transition: top 150ms ease;
}

.lm-TabBar.lm-mod-dragging .lm-TabBar-tab.lm-mod-dragging {
  transition: none;
}

.lm-TabBar-tabLabel .lm-TabBar-tabInput {
  user-select: all;
  width: 100%;
  box-sizing: border-box;
  background: inherit;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-TabPanel-tabBar {
  z-index: 1;
}

.lm-TabPanel-stackedPanel {
  z-index: 0;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Collapse {
  display: flex;
  flex-direction: column;
  align-items: stretch;
}

.jp-Collapse-header {
  padding: 1px 12px;
  background-color: var(--jp-layout-color1);
  border-bottom: solid var(--jp-border-width) var(--jp-border-color2);
  color: var(--jp-ui-font-color1);
  cursor: pointer;
  display: flex;
  align-items: center;
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  text-transform: uppercase;
  user-select: none;
}

.jp-Collapser-icon {
  height: 16px;
}

.jp-Collapse-header-collapsed .jp-Collapser-icon {
  transform: rotate(-90deg);
  margin: auto 0;
}

.jp-Collapser-title {
  line-height: 25px;
}

.jp-Collapse-contents {
  padding: 0 12px;
  background-color: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  overflow: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* This file was auto-generated by ensureUiComponents() in @jupyterlab/buildutils */

/**
 * (DEPRECATED) Support for consuming icons as CSS background images
 */

/* Icons urls */

:root {
  --jp-icon-add-above: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGcgY2xpcC1wYXRoPSJ1cmwoI2NsaXAwXzEzN18xOTQ5MikiPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGQ9Ik00Ljc1IDQuOTMwNjZINi42MjVWNi44MDU2NkM2LjYyNSA3LjAxMTkxIDYuNzkzNzUgNy4xODA2NiA3IDcuMTgwNjZDNy4yMDYyNSA3LjE4MDY2IDcuMzc1IDcuMDExOTEgNy4zNzUgNi44MDU2NlY0LjkzMDY2SDkuMjVDOS40NTYyNSA0LjkzMDY2IDkuNjI1IDQuNzYxOTEgOS42MjUgNC41NTU2NkM5LjYyNSA0LjM0OTQxIDkuNDU2MjUgNC4xODA2NiA5LjI1IDQuMTgwNjZINy4zNzVWMi4zMDU2NkM3LjM3NSAyLjA5OTQxIDcuMjA2MjUgMS45MzA2NiA3IDEuOTMwNjZDNi43OTM3NSAxLjkzMDY2IDYuNjI1IDIuMDk5NDEgNi42MjUgMi4zMDU2NlY0LjE4MDY2SDQuNzVDNC41NDM3NSA0LjE4MDY2IDQuMzc1IDQuMzQ5NDEgNC4zNzUgNC41NTU2NkM0LjM3NSA0Ljc2MTkxIDQuNTQzNzUgNC45MzA2NiA0Ljc1IDQuOTMwNjZaIiBmaWxsPSIjNjE2MTYxIiBzdHJva2U9IiM2MTYxNjEiIHN0cm9rZS13aWR0aD0iMC43Ii8+CjwvZz4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTExLjUgOS41VjExLjVMMi41IDExLjVWOS41TDExLjUgOS41Wk0xMiA4QzEyLjU1MjMgOCAxMyA4LjQ0NzcyIDEzIDlWMTJDMTMgMTIuNTUyMyAxMi41NTIzIDEzIDEyIDEzTDIgMTNDMS40NDc3MiAxMyAxIDEyLjU1MjMgMSAxMlY5QzEgOC40NDc3MiAxLjQ0NzcxIDggMiA4TDEyIDhaIiBmaWxsPSIjNjE2MTYxIi8+CjxkZWZzPgo8Y2xpcFBhdGggaWQ9ImNsaXAwXzEzN18xOTQ5MiI+CjxyZWN0IGNsYXNzPSJqcC1pY29uMyIgd2lkdGg9IjYiIGhlaWdodD0iNiIgZmlsbD0id2hpdGUiIHRyYW5zZm9ybT0ibWF0cml4KC0xIDAgMCAxIDEwIDEuNTU1NjYpIi8+CjwvY2xpcFBhdGg+CjwvZGVmcz4KPC9zdmc+Cg==);
  --jp-icon-add-below: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGcgY2xpcC1wYXRoPSJ1cmwoI2NsaXAwXzEzN18xOTQ5OCkiPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGQ9Ik05LjI1IDEwLjA2OTNMNy4zNzUgMTAuMDY5M0w3LjM3NSA4LjE5NDM0QzcuMzc1IDcuOTg4MDkgNy4yMDYyNSA3LjgxOTM0IDcgNy44MTkzNEM2Ljc5Mzc1IDcuODE5MzQgNi42MjUgNy45ODgwOSA2LjYyNSA4LjE5NDM0TDYuNjI1IDEwLjA2OTNMNC43NSAxMC4wNjkzQzQuNTQzNzUgMTAuMDY5MyA0LjM3NSAxMC4yMzgxIDQuMzc1IDEwLjQ0NDNDNC4zNzUgMTAuNjUwNiA0LjU0Mzc1IDEwLjgxOTMgNC43NSAxMC44MTkzTDYuNjI1IDEwLjgxOTNMNi42MjUgMTIuNjk0M0M2LjYyNSAxMi45MDA2IDYuNzkzNzUgMTMuMDY5MyA3IDEzLjA2OTNDNy4yMDYyNSAxMy4wNjkzIDcuMzc1IDEyLjkwMDYgNy4zNzUgMTIuNjk0M0w3LjM3NSAxMC44MTkzTDkuMjUgMTAuODE5M0M5LjQ1NjI1IDEwLjgxOTMgOS42MjUgMTAuNjUwNiA5LjYyNSAxMC40NDQzQzkuNjI1IDEwLjIzODEgOS40NTYyNSAxMC4wNjkzIDkuMjUgMTAuMDY5M1oiIGZpbGw9IiM2MTYxNjEiIHN0cm9rZT0iIzYxNjE2MSIgc3Ryb2tlLXdpZHRoPSIwLjciLz4KPC9nPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMi41IDUuNUwyLjUgMy41TDExLjUgMy41TDExLjUgNS41TDIuNSA1LjVaTTIgN0MxLjQ0NzcyIDcgMSA2LjU1MjI4IDEgNkwxIDNDMSAyLjQ0NzcyIDEuNDQ3NzIgMiAyIDJMMTIgMkMxMi41NTIzIDIgMTMgMi40NDc3MiAxMyAzTDEzIDZDMTMgNi41NTIyOSAxMi41NTIzIDcgMTIgN0wyIDdaIiBmaWxsPSIjNjE2MTYxIi8+CjxkZWZzPgo8Y2xpcFBhdGggaWQ9ImNsaXAwXzEzN18xOTQ5OCI+CjxyZWN0IGNsYXNzPSJqcC1pY29uMyIgd2lkdGg9IjYiIGhlaWdodD0iNiIgZmlsbD0id2hpdGUiIHRyYW5zZm9ybT0ibWF0cml4KDEgMS43NDg0NmUtMDcgMS43NDg0NmUtMDcgLTEgNCAxMy40NDQzKSIvPgo8L2NsaXBQYXRoPgo8L2RlZnM+Cjwvc3ZnPgo=);
  --jp-icon-add: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDEzaC02djZoLTJ2LTZINXYtMmg2VjVoMnY2aDZ2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-bell: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE2IDE2IiB2ZXJzaW9uPSIxLjEiPgogICA8cGF0aCBjbGFzcz0ianAtaWNvbjIganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMzMzMzMzIgogICAgICBkPSJtOCAwLjI5Yy0xLjQgMC0yLjcgMC43My0zLjYgMS44LTEuMiAxLjUtMS40IDMuNC0xLjUgNS4yLTAuMTggMi4yLTAuNDQgNC0yLjMgNS4zbDAuMjggMS4zaDVjMC4wMjYgMC42NiAwLjMyIDEuMSAwLjcxIDEuNSAwLjg0IDAuNjEgMiAwLjYxIDIuOCAwIDAuNTItMC40IDAuNi0xIDAuNzEtMS41aDVsMC4yOC0xLjNjLTEuOS0wLjk3LTIuMi0zLjMtMi4zLTUuMy0wLjEzLTEuOC0wLjI2LTMuNy0xLjUtNS4yLTAuODUtMS0yLjItMS44LTMuNi0xLjh6bTAgMS40YzAuODggMCAxLjkgMC41NSAyLjUgMS4zIDAuODggMS4xIDEuMSAyLjcgMS4yIDQuNCAwLjEzIDEuNyAwLjIzIDMuNiAxLjMgNS4yaC0xMGMxLjEtMS42IDEuMi0zLjQgMS4zLTUuMiAwLjEzLTEuNyAwLjMtMy4zIDEuMi00LjQgMC41OS0wLjcyIDEuNi0xLjMgMi41LTEuM3ptLTAuNzQgMTJoMS41Yy0wLjAwMTUgMC4yOCAwLjAxNSAwLjc5LTAuNzQgMC43OS0wLjczIDAuMDAxNi0wLjcyLTAuNTMtMC43NC0wLjc5eiIgLz4KPC9zdmc+Cg==);
  --jp-icon-bug-dot: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiPgogICAgICAgIDxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMTcuMTkgOEgyMFYxMEgxNy45MUMxNy45NiAxMC4zMyAxOCAxMC42NiAxOCAxMVYxMkgyMFYxNEgxOC41SDE4VjE0LjAyNzVDMTUuNzUgMTQuMjc2MiAxNCAxNi4xODM3IDE0IDE4LjVDMTQgMTkuMjA4IDE0LjE2MzUgMTkuODc3OSAxNC40NTQ5IDIwLjQ3MzlDMTMuNzA2MyAyMC44MTE3IDEyLjg3NTcgMjEgMTIgMjFDOS43OCAyMSA3Ljg1IDE5Ljc5IDYuODEgMThINFYxNkg2LjA5QzYuMDQgMTUuNjcgNiAxNS4zNCA2IDE1VjE0SDRWMTJINlYxMUM2IDEwLjY2IDYuMDQgMTAuMzMgNi4wOSAxMEg0VjhINi44MUM3LjI2IDcuMjIgNy44OCA2LjU1IDguNjIgNi4wNEw3IDQuNDFMOC40MSAzTDEwLjU5IDUuMTdDMTEuMDQgNS4wNiAxMS41MSA1IDEyIDVDMTIuNDkgNSAxMi45NiA1LjA2IDEzLjQyIDUuMTdMMTUuNTkgM0wxNyA0LjQxTDE1LjM3IDYuMDRDMTYuMTIgNi41NSAxNi43NCA3LjIyIDE3LjE5IDhaTTEwIDE2SDE0VjE0SDEwVjE2Wk0xMCAxMkgxNFYxMEgxMFYxMloiIGZpbGw9IiM2MTYxNjEiLz4KICAgICAgICA8cGF0aCBkPSJNMjIgMTguNUMyMiAyMC40MzMgMjAuNDMzIDIyIDE4LjUgMjJDMTYuNTY3IDIyIDE1IDIwLjQzMyAxNSAxOC41QzE1IDE2LjU2NyAxNi41NjcgMTUgMTguNSAxNUMyMC40MzMgMTUgMjIgMTYuNTY3IDIyIDE4LjVaIiBmaWxsPSIjNjE2MTYxIi8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-bug: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0yMCA4aC0yLjgxYy0uNDUtLjc4LTEuMDctMS40NS0xLjgyLTEuOTZMMTcgNC40MSAxNS41OSAzbC0yLjE3IDIuMTdDMTIuOTYgNS4wNiAxMi40OSA1IDEyIDVjLS40OSAwLS45Ni4wNi0xLjQxLjE3TDguNDEgMyA3IDQuNDFsMS42MiAxLjYzQzcuODggNi41NSA3LjI2IDcuMjIgNi44MSA4SDR2MmgyLjA5Yy0uMDUuMzMtLjA5LjY2LS4wOSAxdjFINHYyaDJ2MWMwIC4zNC4wNC42Ny4wOSAxSDR2MmgyLjgxYzEuMDQgMS43OSAyLjk3IDMgNS4xOSAzczQuMTUtMS4yMSA1LjE5LTNIMjB2LTJoLTIuMDljLjA1LS4zMy4wOS0uNjYuMDktMXYtMWgydi0yaC0ydi0xYzAtLjM0LS4wNC0uNjctLjA5LTFIMjBWOHptLTYgOGgtNHYtMmg0djJ6bTAtNGgtNHYtMmg0djJ6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-build: url(data:image/svg+xml;base64,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);
  --jp-icon-caret-down-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iOS45LDEzLjYgMy42LDcuNCA0LjQsNi42IDkuOSwxMi4yIDE1LjQsNi43IDE2LjEsNy40ICIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-caret-down-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNS45TDksOS43bDMuOC0zLjhsMS4yLDEuMmwtNC45LDVsLTQuOS01TDUuMiw1Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-caret-down: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNy41TDksMTEuMmwzLjgtMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-caret-left: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik0xMC44LDEyLjhMNy4xLDlsMy44LTMuOGwwLDcuNkgxMC44eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-caret-right: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik03LjIsNS4yTDEwLjksOWwtMy44LDMuOFY1LjJINy4yeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-caret-up-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iMTUuNCwxMy4zIDkuOSw3LjcgNC40LDEzLjIgMy42LDEyLjUgOS45LDYuMyAxNi4xLDEyLjYgIi8+Cgk8L2c+Cjwvc3ZnPgo=);
  --jp-icon-caret-up: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik01LjIsMTAuNUw5LDYuOGwzLjgsMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-case-sensitive: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgogIDxnIGNsYXNzPSJqcC1pY29uLWFjY2VudDIiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTcuNiw4aDAuOWwzLjUsOGgtMS4xTDEwLDE0SDZsLTAuOSwySDRMNy42LDh6IE04LDkuMUw2LjQsMTNoMy4yTDgsOS4xeiIvPgogICAgPHBhdGggZD0iTTE2LjYsOS44Yy0wLjIsMC4xLTAuNCwwLjEtMC43LDAuMWMtMC4yLDAtMC40LTAuMS0wLjYtMC4yYy0wLjEtMC4xLTAuMi0wLjQtMC4yLTAuNyBjLTAuMywwLjMtMC42LDAuNS0wLjksMC43Yy0wLjMsMC4xLTAuNywwLjItMS4xLDAuMmMtMC4zLDAtMC41LDAtMC43LTAuMWMtMC4yLTAuMS0wLjQtMC4yLTAuNi0wLjNjLTAuMi0wLjEtMC4zLTAuMy0wLjQtMC41IGMtMC4xLTAuMi0wLjEtMC40LTAuMS0wLjdjMC0wLjMsMC4xLTAuNiwwLjItMC44YzAuMS0wLjIsMC4zLTAuNCwwLjQtMC41QzEyLDcsMTIuMiw2LjksMTIuNSw2LjhjMC4yLTAuMSwwLjUtMC4xLDAuNy0wLjIgYzAuMy0wLjEsMC41LTAuMSwwLjctMC4xYzAuMiwwLDAuNC0wLjEsMC42LTAuMWMwLjIsMCwwLjMtMC4xLDAuNC0wLjJjMC4xLTAuMSwwLjItMC4yLDAuMi0wLjRjMC0xLTEuMS0xLTEuMy0xIGMtMC40LDAtMS40LDAtMS40LDEuMmgtMC45YzAtMC40LDAuMS0wLjcsMC4yLTFjMC4xLTAuMiwwLjMtMC40LDAuNS0wLjZjMC4yLTAuMiwwLjUtMC4zLDAuOC0wLjNDMTMuMyw0LDEzLjYsNCwxMy45LDQgYzAuMywwLDAuNSwwLDAuOCwwLjFjMC4zLDAsMC41LDAuMSwwLjcsMC4yYzAuMiwwLjEsMC40LDAuMywwLjUsMC41QzE2LDUsMTYsNS4yLDE2LDUuNnYyLjljMCwwLjIsMCwwLjQsMCwwLjUgYzAsMC4xLDAuMSwwLjIsMC4zLDAuMmMwLjEsMCwwLjIsMCwwLjMsMFY5Ljh6IE0xNS4yLDYuOWMtMS4yLDAuNi0zLjEsMC4yLTMuMSwxLjRjMCwxLjQsMy4xLDEsMy4xLTAuNVY2Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-check: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik05IDE2LjE3TDQuODMgMTJsLTEuNDIgMS40MUw5IDE5IDIxIDdsLTEuNDEtMS40MXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-circle-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyIDJDNi40NyAyIDIgNi40NyAyIDEyczQuNDcgMTAgMTAgMTAgMTAtNC40NyAxMC0xMFMxNy41MyAyIDEyIDJ6bTAgMThjLTQuNDEgMC04LTMuNTktOC04czMuNTktOCA4LTggOCAzLjU5IDggOC0zLjU5IDgtOCA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-circle: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iOSIgY3k9IjkiIHI9IjgiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-clear: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8bWFzayBpZD0iZG9udXRIb2xlIj4KICAgIDxyZWN0IHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgZmlsbD0id2hpdGUiIC8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSI4IiBmaWxsPSJibGFjayIvPgogIDwvbWFzaz4KCiAgPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxyZWN0IGhlaWdodD0iMTgiIHdpZHRoPSIyIiB4PSIxMSIgeT0iMyIgdHJhbnNmb3JtPSJyb3RhdGUoMzE1LCAxMiwgMTIpIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIxMCIgbWFzaz0idXJsKCNkb251dEhvbGUpIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-close: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1ub25lIGpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIGpwLWljb24zLWhvdmVyIiBmaWxsPSJub25lIj4KICAgIDxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjExIi8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIGpwLWljb24tYWNjZW50Mi1ob3ZlciIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMTkgNi40MUwxNy41OSA1IDEyIDEwLjU5IDYuNDEgNSA1IDYuNDEgMTAuNTkgMTIgNSAxNy41OSA2LjQxIDE5IDEyIDEzLjQxIDE3LjU5IDE5IDE5IDE3LjU5IDEzLjQxIDEyeiIvPgogIDwvZz4KCiAgPGcgY2xhc3M9ImpwLWljb24tbm9uZSBqcC1pY29uLWJ1c3kiIGZpbGw9Im5vbmUiPgogICAgPGNpcmNsZSBjeD0iMTIiIGN5PSIxMiIgcj0iNyIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-code-check: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBzaGFwZS1yZW5kZXJpbmc9Imdlb21ldHJpY1ByZWNpc2lvbiI+CiAgICA8cGF0aCBkPSJNNi41OSwzLjQxTDIsOEw2LjU5LDEyLjZMOCwxMS4xOEw0LjgyLDhMOCw0LjgyTDYuNTksMy40MU0xMi40MSwzLjQxTDExLDQuODJMMTQuMTgsOEwxMSwxMS4xOEwxMi40MSwxMi42TDE3LDhMMTIuNDEsMy40MU0yMS41OSwxMS41OUwxMy41LDE5LjY4TDkuODMsMTZMOC40MiwxNy40MUwxMy41LDIyLjVMMjMsMTNMMjEuNTksMTEuNTlaIiAvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-code: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIiIGhlaWdodD0iMjIiIHZpZXdCb3g9IjAgMCAyOCAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTExLjQgMTguNkw2LjggMTRMMTEuNCA5LjRMMTAgOEw0IDE0TDEwIDIwTDExLjQgMTguNlpNMTYuNiAxOC42TDIxLjIgMTRMMTYuNiA5LjRMMTggOEwyNCAxNEwxOCAyMEwxNi42IDE4LjZWMTguNloiLz4KCTwvZz4KPC9zdmc+Cg==);
  --jp-icon-collapse-all: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTggMmMxIDAgMTEgMCAxMiAwczIgMSAyIDJjMCAxIDAgMTEgMCAxMnMwIDItMiAyQzIwIDE0IDIwIDQgMjAgNFMxMCA0IDYgNGMwLTIgMS0yIDItMnoiIC8+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTE4IDhjMC0xLTEtMi0yLTJTNSA2IDQgNnMtMiAxLTIgMmMwIDEgMCAxMSAwIDEyczEgMiAyIDJjMSAwIDExIDAgMTIgMHMyLTEgMi0yYzAtMSAwLTExIDAtMTJ6bS0yIDB2MTJINFY4eiIgLz4KICAgICAgICA8cGF0aCBkPSJNNiAxM3YyaDh2LTJ6IiAvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-console: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwMCAyMDAiPgogIDxnIGNsYXNzPSJqcC1jb25zb2xlLWljb24tYmFja2dyb3VuZC1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMjg4RDEiPgogICAgPHBhdGggZD0iTTIwIDE5LjhoMTYwdjE1OS45SDIweiIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtY29uc29sZS1pY29uLWNvbG9yIGpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIiBmaWxsPSIjZmZmIj4KICAgIDxwYXRoIGQ9Ik0xMDUgMTI3LjNoNDB2MTIuOGgtNDB6TTUxLjEgNzdMNzQgOTkuOWwtMjMuMyAyMy4zIDEwLjUgMTAuNSAyMy4zLTIzLjNMOTUgOTkuOSA4NC41IDg5LjQgNjEuNiA2Ni41eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-copy: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTExLjksMUgzLjJDMi40LDEsMS43LDEuNywxLjcsMi41djEwLjJoMS41VjIuNWg4LjdWMXogTTE0LjEsMy45aC04Yy0wLjgsMC0xLjUsMC43LTEuNSwxLjV2MTAuMmMwLDAuOCwwLjcsMS41LDEuNSwxLjVoOCBjMC44LDAsMS41LTAuNywxLjUtMS41VjUuNEMxNS41LDQuNiwxNC45LDMuOSwxNC4xLDMuOXogTTE0LjEsMTUuNWgtOFY1LjRoOFYxNS41eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-copyright: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGVuYWJsZS1iYWNrZ3JvdW5kPSJuZXcgMCAwIDI0IDI0IiBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCI+CiAgPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0xMS44OCw5LjE0YzEuMjgsMC4wNiwxLjYxLDEuMTUsMS42MywxLjY2aDEuNzljLTAuMDgtMS45OC0xLjQ5LTMuMTktMy40NS0zLjE5QzkuNjQsNy42MSw4LDksOCwxMi4xNCBjMCwxLjk0LDAuOTMsNC4yNCwzLjg0LDQuMjRjMi4yMiwwLDMuNDEtMS42NSwzLjQ0LTIuOTVoLTEuNzljLTAuMDMsMC41OS0wLjQ1LDEuMzgtMS42MywxLjQ0QzEwLjU1LDE0LjgzLDEwLDEzLjgxLDEwLDEyLjE0IEMxMCw5LjI1LDExLjI4LDkuMTYsMTEuODgsOS4xNHogTTEyLDJDNi40OCwyLDIsNi40OCwyLDEyczQuNDgsMTAsMTAsMTBzMTAtNC40OCwxMC0xMFMxNy41MiwyLDEyLDJ6IE0xMiwyMGMtNC40MSwwLTgtMy41OS04LTggczMuNTktOCw4LThzOCwzLjU5LDgsOFMxNi40MSwyMCwxMiwyMHoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-cut: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkuNjQgNy42NGMuMjMtLjUuMzYtMS4wNS4zNi0xLjY0IDAtMi4yMS0xLjc5LTQtNC00UzIgMy43OSAyIDZzMS43OSA0IDQgNGMuNTkgMCAxLjE0LS4xMyAxLjY0LS4zNkwxMCAxMmwtMi4zNiAyLjM2QzcuMTQgMTQuMTMgNi41OSAxNCA2IDE0Yy0yLjIxIDAtNCAxLjc5LTQgNHMxLjc5IDQgNCA0IDQtMS43OSA0LTRjMC0uNTktLjEzLTEuMTQtLjM2LTEuNjRMMTIgMTRsNyA3aDN2LTFMOS42NCA3LjY0ek02IDhjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTAgMTJjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTYtNy41Yy0uMjggMC0uNS0uMjItLjUtLjVzLjIyLS41LjUtLjUuNS4yMi41LjUtLjIyLjUtLjUuNXpNMTkgM2wtNiA2IDIgMiA3LTdWM3oiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-delete: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2cHgiIGhlaWdodD0iMTZweCI+CiAgICA8cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIiAvPgogICAgPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjI2MjYyIiBkPSJNNiAxOWMwIDEuMS45IDIgMiAyaDhjMS4xIDAgMi0uOSAyLTJWN0g2djEyek0xOSA0aC0zLjVsLTEtMWgtNWwtMSAxSDV2MmgxNFY0eiIgLz4KPC9zdmc+Cg==);
  --jp-icon-download: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDloLTRWM0g5djZINWw3IDcgNy03ek01IDE4djJoMTR2LTJINXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-duplicate: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTIuNzk5OTggMC44NzVIOC44OTU4MkM5LjIwMDYxIDAuODc1IDkuNDQ5OTggMS4xMzkxNCA5LjQ0OTk4IDEuNDYxOThDOS40NDk5OCAxLjc4NDgyIDkuMjAwNjEgMi4wNDg5NiA4Ljg5NTgyIDIuMDQ4OTZIMy4zNTQxNUMzLjA0OTM2IDIuMDQ4OTYgMi43OTk5OCAyLjMxMzEgMi43OTk5OCAyLjYzNTk0VjkuNjc5NjlDMi43OTk5OCAxMC4wMDI1IDIuNTUwNjEgMTAuMjY2NyAyLjI0NTgyIDEwLjI2NjdDMS45NDEwMyAxMC4yNjY3IDEuNjkxNjUgMTAuMDAyNSAxLjY5MTY1IDkuNjc5NjlWMi4wNDg5NkMxLjY5MTY1IDEuNDAzMjggMi4xOTA0IDAuODc1IDIuNzk5OTggMC44NzVaTTUuMzY2NjUgMTEuOVY0LjU1SDExLjA4MzNWMTEuOUg1LjM2NjY1Wk00LjE0MTY1IDQuMTQxNjdDNC4xNDE2NSAzLjY5MDYzIDQuNTA3MjggMy4zMjUgNC45NTgzMiAzLjMyNUgxMS40OTE3QzExLjk0MjcgMy4zMjUgMTIuMzA4MyAzLjY5MDYzIDEyLjMwODMgNC4xNDE2N1YxMi4zMDgzQzEyLjMwODMgMTIuNzU5NCAxMS45NDI3IDEzLjEyNSAxMS40OTE3IDEzLjEyNUg0Ljk1ODMyQzQuNTA3MjggMTMuMTI1IDQuMTQxNjUgMTIuNzU5NCA0LjE0MTY1IDEyLjMwODNWNC4xNDE2N1oiIGZpbGw9IiM2MTYxNjEiLz4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNOS40MzU3NCA4LjI2NTA3SDguMzY0MzFWOS4zMzY1QzguMzY0MzEgOS40NTQzNSA4LjI2Nzg4IDkuNTUwNzggOC4xNTAwMiA5LjU1MDc4QzguMDMyMTcgOS41NTA3OCA3LjkzNTc0IDkuNDU0MzUgNy45MzU3NCA5LjMzNjVWOC4yNjUwN0g2Ljg2NDMxQzYuNzQ2NDUgOC4yNjUwNyA2LjY1MDAyIDguMTY4NjQgNi42NTAwMiA4LjA1MDc4QzYuNjUwMDIgNy45MzI5MiA2Ljc0NjQ1IDcuODM2NSA2Ljg2NDMxIDcuODM2NUg3LjkzNTc0VjYuNzY1MDdDNy45MzU3NCA2LjY0NzIxIDguMDMyMTcgNi41NTA3OCA4LjE1MDAyIDYuNTUwNzhDOC4yNjc4OCA2LjU1MDc4IDguMzY0MzEgNi42NDcyMSA4LjM2NDMxIDYuNzY1MDdWNy44MzY1SDkuNDM1NzRDOS41NTM2IDcuODM2NSA5LjY1MDAyIDcuOTMyOTIgOS42NTAwMiA4LjA1MDc4QzkuNjUwMDIgOC4xNjg2NCA5LjU1MzYgOC4yNjUwNyA5LjQzNTc0IDguMjY1MDdaIiBmaWxsPSIjNjE2MTYxIiBzdHJva2U9IiM2MTYxNjEiIHN0cm9rZS13aWR0aD0iMC41Ii8+Cjwvc3ZnPgo=);
  --jp-icon-edit: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMgMTcuMjVWMjFoMy43NUwxNy44MSA5Ljk0bC0zLjc1LTMuNzVMMyAxNy4yNXpNMjAuNzEgNy4wNGMuMzktLjM5LjM5LTEuMDIgMC0xLjQxbC0yLjM0LTIuMzRjLS4zOS0uMzktMS4wMi0uMzktMS40MSAwbC0xLjgzIDEuODMgMy43NSAzLjc1IDEuODMtMS44M3oiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-ellipses: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iNSIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxOSIgY3k9IjEyIiByPSIyIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-error: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj48Y2lyY2xlIGN4PSIxMiIgY3k9IjE5IiByPSIyIi8+PHBhdGggZD0iTTEwIDNoNHYxMmgtNHoiLz48L2c+CjxwYXRoIGZpbGw9Im5vbmUiIGQ9Ik0wIDBoMjR2MjRIMHoiLz4KPC9zdmc+Cg==);
  --jp-icon-expand-all: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTggMmMxIDAgMTEgMCAxMiAwczIgMSAyIDJjMCAxIDAgMTEgMCAxMnMwIDItMiAyQzIwIDE0IDIwIDQgMjAgNFMxMCA0IDYgNGMwLTIgMS0yIDItMnoiIC8+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTE4IDhjMC0xLTEtMi0yLTJTNSA2IDQgNnMtMiAxLTIgMmMwIDEgMCAxMSAwIDEyczEgMiAyIDJjMSAwIDExIDAgMTIgMHMyLTEgMi0yYzAtMSAwLTExIDAtMTJ6bS0yIDB2MTJINFY4eiIgLz4KICAgICAgICA8cGF0aCBkPSJNMTEgMTBIOXYzSDZ2MmgzdjNoMnYtM2gzdi0yaC0zeiIgLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-extension: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwLjUgMTFIMTlWN2MwLTEuMS0uOS0yLTItMmgtNFYzLjVDMTMgMi4xMiAxMS44OCAxIDEwLjUgMVM4IDIuMTIgOCAzLjVWNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAydjMuOEgzLjVjMS40OSAwIDIuNyAxLjIxIDIuNyAyLjdzLTEuMjEgMi43LTIuNyAyLjdIMlYyMGMwIDEuMS45IDIgMiAyaDMuOHYtMS41YzAtMS40OSAxLjIxLTIuNyAyLjctMi43IDEuNDkgMCAyLjcgMS4yMSAyLjcgMi43VjIySDE3YzEuMSAwIDItLjkgMi0ydi00aDEuNWMxLjM4IDAgMi41LTEuMTIgMi41LTIuNVMyMS44OCAxMSAyMC41IDExeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-fast-forward: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTQgMThsOC41LTZMNCA2djEyem05LTEydjEybDguNS02TDEzIDZ6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-file-upload: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkgMTZoNnYtNmg0bC03LTctNyA3aDR6bS00IDJoMTR2Mkg1eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-file: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuMyA4LjJsLTUuNS01LjVjLS4zLS4zLS43LS41LTEuMi0uNUgzLjljLS44LjEtMS42LjktMS42IDEuOHYxNC4xYzAgLjkuNyAxLjYgMS42IDEuNmgxNC4yYy45IDAgMS42LS43IDEuNi0xLjZWOS40Yy4xLS41LS4xLS45LS40LTEuMnptLTUuOC0zLjNsMy40IDMuNmgtMy40VjQuOXptMy45IDEyLjdINC43Yy0uMSAwLS4yIDAtLjItLjJWNC43YzAtLjIuMS0uMy4yLS4zaDcuMnY0LjRzMCAuOC4zIDEuMWMuMy4zIDEuMS4zIDEuMS4zaDQuM3Y3LjJzLS4xLjItLjIuMnoiLz4KPC9zdmc+Cg==);
  --jp-icon-filter-dot: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTE0LDEyVjE5Ljg4QzE0LjA0LDIwLjE4IDEzLjk0LDIwLjUgMTMuNzEsMjAuNzFDMTMuMzIsMjEuMSAxMi42OSwyMS4xIDEyLjMsMjAuNzFMMTAuMjksMTguN0MxMC4wNiwxOC40NyA5Ljk2LDE4LjE2IDEwLDE3Ljg3VjEySDkuOTdMNC4yMSw0LjYyQzMuODcsNC4xOSAzLjk1LDMuNTYgNC4zOCwzLjIyQzQuNTcsMy4wOCA0Ljc4LDMgNSwzVjNIMTlWM0MxOS4yMiwzIDE5LjQzLDMuMDggMTkuNjIsMy4yMkMyMC4wNSwzLjU2IDIwLjEzLDQuMTkgMTkuNzksNC42MkwxNC4wMywxMkgxNFoiIC8+CiAgPC9nPgogIDxnIGNsYXNzPSJqcC1pY29uLWRvdCIgZmlsbD0iI0ZGRiI+CiAgICA8Y2lyY2xlIGN4PSIxOCIgY3k9IjE3IiByPSIzIj48L2NpcmNsZT4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-filter-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEwIDE4aDR2LTJoLTR2MnpNMyA2djJoMThWNkgzem0zIDdoMTJ2LTJINnYyeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-filter: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTE0LDEyVjE5Ljg4QzE0LjA0LDIwLjE4IDEzLjk0LDIwLjUgMTMuNzEsMjAuNzFDMTMuMzIsMjEuMSAxMi42OSwyMS4xIDEyLjMsMjAuNzFMMTAuMjksMTguN0MxMC4wNiwxOC40NyA5Ljk2LDE4LjE2IDEwLDE3Ljg3VjEySDkuOTdMNC4yMSw0LjYyQzMuODcsNC4xOSAzLjk1LDMuNTYgNC4zOCwzLjIyQzQuNTcsMy4wOCA0Ljc4LDMgNSwzVjNIMTlWM0MxOS4yMiwzIDE5LjQzLDMuMDggMTkuNjIsMy4yMkMyMC4wNSwzLjU2IDIwLjEzLDQuMTkgMTkuNzksNC42MkwxNC4wMywxMkgxNFoiIC8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-folder-favorite: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjRweCIgdmlld0JveD0iMCAwIDI0IDI0IiB3aWR0aD0iMjRweCIgZmlsbD0iIzAwMDAwMCI+CiAgPHBhdGggZD0iTTAgMGgyNHYyNEgwVjB6IiBmaWxsPSJub25lIi8+PHBhdGggY2xhc3M9ImpwLWljb24zIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzYxNjE2MSIgZD0iTTIwIDZoLThsLTItMkg0Yy0xLjEgMC0yIC45LTIgMnYxMmMwIDEuMS45IDIgMiAyaDE2YzEuMSAwIDItLjkgMi0yVjhjMC0xLjEtLjktMi0yLTJ6bS0yLjA2IDExTDE1IDE1LjI4IDEyLjA2IDE3bC43OC0zLjMzLTIuNTktMi4yNCAzLjQxLS4yOUwxNSA4bDEuMzQgMy4xNCAzLjQxLjI5LTIuNTkgMi4yNC43OCAzLjMzeiIvPgo8L3N2Zz4K);
  --jp-icon-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY4YzAtMS4xLS45LTItMi0yaC04bC0yLTJ6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-home: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjRweCIgdmlld0JveD0iMCAwIDI0IDI0IiB3aWR0aD0iMjRweCIgZmlsbD0iIzAwMDAwMCI+CiAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPjxwYXRoIGNsYXNzPSJqcC1pY29uMyBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xMCAyMHYtNmg0djZoNXYtOGgzTDEyIDMgMiAxMmgzdjh6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-html5: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uMCBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMDAiIGQ9Ik0xMDguNCAwaDIzdjIyLjhoMjEuMlYwaDIzdjY5aC0yM1Y0NmgtMjF2MjNoLTIzLjJNMjA2IDIzaC0yMC4zVjBoNjMuN3YyM0gyMjl2NDZoLTIzbTUzLjUtNjloMjQuMWwxNC44IDI0LjNMMzEzLjIgMGgyNC4xdjY5aC0yM1YzNC44bC0xNi4xIDI0LjgtMTYuMS0yNC44VjY5aC0yMi42bTg5LjItNjloMjN2NDYuMmgzMi42VjY5aC01NS42Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI2U0NGQyNiIgZD0iTTEwNy42IDQ3MWwtMzMtMzcwLjRoMzYyLjhsLTMzIDM3MC4yTDI1NS43IDUxMiIvPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNmMTY1MjkiIGQ9Ik0yNTYgNDgwLjVWMTMxaDE0OC4zTDM3NiA0NDciLz4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNlYmViZWIiIGQ9Ik0xNDIgMTc2LjNoMTE0djQ1LjRoLTY0LjJsNC4yIDQ2LjVoNjB2NDUuM0gxNTQuNG0yIDIyLjhIMjAybDMuMiAzNi4zIDUwLjggMTMuNnY0Ny40bC05My4yLTI2Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIiBmaWxsPSIjZmZmIiBkPSJNMzY5LjYgMTc2LjNIMjU1Ljh2NDUuNGgxMDkuNm0tNC4xIDQ2LjVIMjU1Ljh2NDUuNGg1NmwtNS4zIDU5LTUwLjcgMTMuNnY0Ny4ybDkzLTI1LjgiLz4KPC9zdmc+Cg==);
  --jp-icon-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1icmFuZDQganAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNGRkYiIGQ9Ik0yLjIgMi4yaDE3LjV2MTcuNUgyLjJ6Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzNGNTFCNSIgZD0iTTIuMiAyLjJ2MTcuNWgxNy41bC4xLTE3LjVIMi4yem0xMi4xIDIuMmMxLjIgMCAyLjIgMSAyLjIgMi4ycy0xIDIuMi0yLjIgMi4yLTIuMi0xLTIuMi0yLjIgMS0yLjIgMi4yLTIuMnpNNC40IDE3LjZsMy4zLTguOCAzLjMgNi42IDIuMi0zLjIgNC40IDUuNEg0LjR6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-info: url(data:image/svg+xml;base64,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);
  --jp-icon-inspector: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaW5zcGVjdG9yLWljb24tY29sb3IganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY2YzAtMS4xLS45LTItMi0yem0tNSAxNEg0di00aDExdjR6bTAtNUg0VjloMTF2NHptNSA1aC00VjloNHY5eiIvPgo8L3N2Zz4K);
  --jp-icon-json: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtanNvbi1pY29uLWNvbG9yIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI0Y5QTgyNSI+CiAgICA8cGF0aCBkPSJNMjAuMiAxMS44Yy0xLjYgMC0xLjcuNS0xLjcgMSAwIC40LjEuOS4xIDEuMy4xLjUuMS45LjEgMS4zIDAgMS43LTEuNCAyLjMtMy41IDIuM2gtLjl2LTEuOWguNWMxLjEgMCAxLjQgMCAxLjQtLjggMC0uMyAwLS42LS4xLTEgMC0uNC0uMS0uOC0uMS0xLjIgMC0xLjMgMC0xLjggMS4zLTItMS4zLS4yLTEuMy0uNy0xLjMtMiAwLS40LjEtLjguMS0xLjIuMS0uNC4xLS43LjEtMSAwLS44LS40LS43LTEuNC0uOGgtLjVWNC4xaC45YzIuMiAwIDMuNS43IDMuNSAyLjMgMCAuNC0uMS45LS4xIDEuMy0uMS41LS4xLjktLjEgMS4zIDAgLjUuMiAxIDEuNyAxdjEuOHpNMS44IDEwLjFjMS42IDAgMS43LS41IDEuNy0xIDAtLjQtLjEtLjktLjEtMS4zLS4xLS41LS4xLS45LS4xLTEuMyAwLTEuNiAxLjQtMi4zIDMuNS0yLjNoLjl2MS45aC0uNWMtMSAwLTEuNCAwLTEuNC44IDAgLjMgMCAuNi4xIDEgMCAuMi4xLjYuMSAxIDAgMS4zIDAgMS44LTEuMyAyQzYgMTEuMiA2IDExLjcgNiAxM2MwIC40LS4xLjgtLjEgMS4yLS4xLjMtLjEuNy0uMSAxIDAgLjguMy44IDEuNC44aC41djEuOWgtLjljLTIuMSAwLTMuNS0uNi0zLjUtMi4zIDAtLjQuMS0uOS4xLTEuMy4xLS41LjEtLjkuMS0xLjMgMC0uNS0uMi0xLTEuNy0xdi0xLjl6Ii8+CiAgICA8Y2lyY2xlIGN4PSIxMSIgY3k9IjEzLjgiIHI9IjIuMSIvPgogICAgPGNpcmNsZSBjeD0iMTEiIGN5PSI4LjIiIHI9IjIuMSIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-julia: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDMyNSAzMDAiPgogIDxnIGNsYXNzPSJqcC1icmFuZDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjY2IzYzMzIj4KICAgIDxwYXRoIGQ9Ik0gMTUwLjg5ODQzOCAyMjUgQyAxNTAuODk4NDM4IDI2Ni40MjE4NzUgMTE3LjMyMDMxMiAzMDAgNzUuODk4NDM4IDMwMCBDIDM0LjQ3NjU2MiAzMDAgMC44OTg0MzggMjY2LjQyMTg3NSAwLjg5ODQzOCAyMjUgQyAwLjg5ODQzOCAxODMuNTc4MTI1IDM0LjQ3NjU2MiAxNTAgNzUuODk4NDM4IDE1MCBDIDExNy4zMjAzMTIgMTUwIDE1MC44OTg0MzggMTgzLjU3ODEyNSAxNTAuODk4NDM4IDIyNSIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzM4OTgyNiI+CiAgICA8cGF0aCBkPSJNIDIzNy41IDc1IEMgMjM3LjUgMTE2LjQyMTg3NSAyMDMuOTIxODc1IDE1MCAxNjIuNSAxNTAgQyAxMjEuMDc4MTI1IDE1MCA4Ny41IDExNi40MjE4NzUgODcuNSA3NSBDIDg3LjUgMzMuNTc4MTI1IDEyMS4wNzgxMjUgMCAxNjIuNSAwIEMgMjAzLjkyMTg3NSAwIDIzNy41IDMzLjU3ODEyNSAyMzcuNSA3NSIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzk1NThiMiI+CiAgICA8cGF0aCBkPSJNIDMyNC4xMDE1NjIgMjI1IEMgMzI0LjEwMTU2MiAyNjYuNDIxODc1IDI5MC41MjM0MzggMzAwIDI0OS4xMDE1NjIgMzAwIEMgMjA3LjY3OTY4OCAzMDAgMTc0LjEwMTU2MiAyNjYuNDIxODc1IDE3NC4xMDE1NjIgMjI1IEMgMTc0LjEwMTU2MiAxODMuNTc4MTI1IDIwNy42Nzk2ODggMTUwIDI0OS4xMDE1NjIgMTUwIEMgMjkwLjUyMzQzOCAxNTAgMzI0LjEwMTU2MiAxODMuNTc4MTI1IDMyNC4xMDE1NjIgMjI1Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-jupyter-favicon: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTUyIiBoZWlnaHQ9IjE2NSIgdmlld0JveD0iMCAwIDE1MiAxNjUiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgPGcgY2xhc3M9ImpwLWp1cHl0ZXItaWNvbi1jb2xvciIgZmlsbD0iI0YzNzcyNiI+CiAgICA8cGF0aCB0cmFuc2Zvcm09InRyYW5zbGF0ZSgwLjA3ODk0NywgMTEwLjU4MjkyNykiIGQ9Ik03NS45NDIyODQyLDI5LjU4MDQ1NjEgQzQzLjMwMjM5NDcsMjkuNTgwNDU2MSAxNC43OTY3ODMyLDE3LjY1MzQ2MzQgMCwwIEM1LjUxMDgzMjExLDE1Ljg0MDY4MjkgMTUuNzgxNTM4OSwyOS41NjY3NzMyIDI5LjM5MDQ5NDcsMzkuMjc4NDE3MSBDNDIuOTk5Nyw0OC45ODk4NTM3IDU5LjI3MzcsNTQuMjA2NzgwNSA3NS45NjA1Nzg5LDU0LjIwNjc4MDUgQzkyLjY0NzQ1NzksNTQuMjA2NzgwNSAxMDguOTIxNDU4LDQ4Ljk4OTg1MzcgMTIyLjUzMDY2MywzOS4yNzg0MTcxIEMxMzYuMTM5NDUzLDI5LjU2Njc3MzIgMTQ2LjQxMDI4NCwxNS44NDA2ODI5IDE1MS45MjExNTgsMCBDMTM3LjA4Nzg2OCwxNy42NTM0NjM0IDEwOC41ODI1ODksMjkuNTgwNDU2MSA3NS45NDIyODQyLDI5LjU4MDQ1NjEgTDc1Ljk0MjI4NDIsMjkuNTgwNDU2MSBaIiAvPgogICAgPHBhdGggdHJhbnNmb3JtPSJ0cmFuc2xhdGUoMC4wMzczNjgsIDAuNzA0ODc4KSIgZD0iTTc1Ljk3ODQ1NzksMjQuNjI2NDA3MyBDMTA4LjYxODc2MywyNC42MjY0MDczIDEzNy4xMjQ0NTgsMzYuNTUzNDQxNSAxNTEuOTIxMTU4LDU0LjIwNjc4MDUgQzE0Ni40MTAyODQsMzguMzY2MjIyIDEzNi4xMzk0NTMsMjQuNjQwMTMxNyAxMjIuNTMwNjYzLDE0LjkyODQ4NzggQzEwOC45MjE0NTgsNS4yMTY4NDM5IDkyLjY0NzQ1NzksMCA3NS45NjA1Nzg5LDAgQzU5LjI3MzcsMCA0Mi45OTk3LDUuMjE2ODQzOSAyOS4zOTA0OTQ3LDE0LjkyODQ4NzggQzE1Ljc4MTUzODksMjQuNjQwMTMxNyA1LjUxMDgzMjExLDM4LjM2NjIyMiAwLDU0LjIwNjc4MDUgQzE0LjgzMzA4MTYsMzYuNTg5OTI5MyA0My4zMzg1Njg0LDI0LjYyNjQwNzMgNzUuOTc4NDU3OSwyNC42MjY0MDczIEw3NS45Nzg0NTc5LDI0LjYyNjQwNzMgWiIgLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-jupyter: url(data:image/svg+xml;base64,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);
  --jp-icon-jupyterlab-wordmark: url(data:image/svg+xml;base64,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);
  --jp-icon-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgZmlsbD0iIzYxNjE2MSIgZD0iTTE1IDlIOXY2aDZWOXptLTIgNGgtMnYtMmgydjJ6bTgtMlY5aC0yVjdjMC0xLjEtLjktMi0yLTJoLTJWM2gtMnYyaC0yVjNIOXYySDdjLTEuMSAwLTIgLjktMiAydjJIM3YyaDJ2MkgzdjJoMnYyYzAgMS4xLjkgMiAyIDJoMnYyaDJ2LTJoMnYyaDJ2LTJoMmMxLjEgMCAyLS45IDItMnYtMmgydi0yaC0ydi0yaDJ6bS00IDZIN1Y3aDEwdjEweiIvPgo8L3N2Zz4K);
  --jp-icon-keyboard: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMTdjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY3YzAtMS4xLS45LTItMi0yem0tOSAzaDJ2MmgtMlY4em0wIDNoMnYyaC0ydi0yek04IDhoMnYySDhWOHptMCAzaDJ2Mkg4di0yem0tMSAySDV2LTJoMnYyem0wLTNINVY4aDJ2MnptOSA3SDh2LTJoOHYyem0wLTRoLTJ2LTJoMnYyem0wLTNoLTJWOGgydjJ6bTMgM2gtMnYtMmgydjJ6bTAtM2gtMlY4aDJ2MnoiLz4KPC9zdmc+Cg==);
  --jp-icon-launch: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMzIgMzIiIHdpZHRoPSIzMiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0yNiwyOEg2YTIuMDAyNywyLjAwMjcsMCwwLDEtMi0yVjZBMi4wMDI3LDIuMDAyNywwLDAsMSw2LDRIMTZWNkg2VjI2SDI2VjE2aDJWMjZBMi4wMDI3LDIuMDAyNywwLDAsMSwyNiwyOFoiLz4KICAgIDxwb2x5Z29uIHBvaW50cz0iMjAgMiAyMCA0IDI2LjU4NiA0IDE4IDEyLjU4NiAxOS40MTQgMTQgMjggNS40MTQgMjggMTIgMzAgMTIgMzAgMiAyMCAyIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-launcher: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkgMTlINVY1aDdWM0g1YTIgMiAwIDAwLTIgMnYxNGEyIDIgMCAwMDIgMmgxNGMxLjEgMCAyLS45IDItMnYtN2gtMnY3ek0xNCAzdjJoMy41OWwtOS44MyA5LjgzIDEuNDEgMS40MUwxOSA2LjQxVjEwaDJWM2gtN3oiLz4KPC9zdmc+Cg==);
  --jp-icon-line-form: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGZpbGw9IndoaXRlIiBkPSJNNS44OCA0LjEyTDEzLjc2IDEybC03Ljg4IDcuODhMOCAyMmwxMC0xMEw4IDJ6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-link: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMuOSAxMmMwLTEuNzEgMS4zOS0zLjEgMy4xLTMuMWg0VjdIN2MtMi43NiAwLTUgMi4yNC01IDVzMi4yNCA1IDUgNWg0di0xLjlIN2MtMS43MSAwLTMuMS0xLjM5LTMuMS0zLjF6TTggMTNoOHYtMkg4djJ6bTktNmgtNHYxLjloNGMxLjcxIDAgMy4xIDEuMzkgMy4xIDMuMXMtMS4zOSAzLjEtMy4xIDMuMWgtNFYxN2g0YzIuNzYgMCA1LTIuMjQgNS01cy0yLjI0LTUtNS01eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xOSA1djE0SDVWNWgxNG0xLjEtMkgzLjljLS41IDAtLjkuNC0uOS45djE2LjJjMCAuNC40LjkuOS45aDE2LjJjLjQgMCAuOS0uNS45LS45VjMuOWMwLS41LS41LS45LS45LS45ek0xMSA3aDZ2MmgtNlY3em0wIDRoNnYyaC02di0yem0wIDRoNnYyaC02ek03IDdoMnYySDd6bTAgNGgydjJIN3ptMCA0aDJ2Mkg3eiIvPgo8L3N2Zz4K);
  --jp-icon-markdown: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjN0IxRkEyIiBkPSJNNSAxNC45aDEybC02LjEgNnptOS40LTYuOGMwLTEuMy0uMS0yLjktLjEtNC41LS40IDEuNC0uOSAyLjktMS4zIDQuM2wtMS4zIDQuM2gtMkw4LjUgNy45Yy0uNC0xLjMtLjctMi45LTEtNC4zLS4xIDEuNi0uMSAzLjItLjIgNC42TDcgMTIuNEg0LjhsLjctMTFoMy4zTDEwIDVjLjQgMS4yLjcgMi43IDEgMy45LjMtMS4yLjctMi42IDEtMy45bDEuMi0zLjdoMy4zbC42IDExaC0yLjRsLS4zLTQuMnoiLz4KPC9zdmc+Cg==);
  --jp-icon-move-down: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNMTIuNDcxIDcuNTI4OTlDMTIuNzYzMiA3LjIzNjg0IDEyLjc2MzIgNi43NjMxNiAxMi40NzEgNi40NzEwMVY2LjQ3MTAxQzEyLjE3OSA2LjE3OTA1IDExLjcwNTcgNi4xNzg4NCAxMS40MTM1IDYuNDcwNTRMNy43NSAxMC4xMjc1VjEuNzVDNy43NSAxLjMzNTc5IDcuNDE0MjEgMSA3IDFWMUM2LjU4NTc5IDEgNi4yNSAxLjMzNTc5IDYuMjUgMS43NVYxMC4xMjc1TDIuNTk3MjYgNi40NjgyMkMyLjMwMzM4IDYuMTczODEgMS44MjY0MSA2LjE3MzU5IDEuNTMyMjYgNi40Njc3NFY2LjQ2Nzc0QzEuMjM4MyA2Ljc2MTcgMS4yMzgzIDcuMjM4MyAxLjUzMjI2IDcuNTMyMjZMNi4yOTI4OSAxMi4yOTI5QzYuNjgzNDIgMTIuNjgzNCA3LjMxNjU4IDEyLjY4MzQgNy43MDcxMSAxMi4yOTI5TDEyLjQ3MSA3LjUyODk5WiIgZmlsbD0iIzYxNjE2MSIvPgo8L3N2Zz4K);
  --jp-icon-move-up: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNMS41Mjg5OSA2LjQ3MTAxQzEuMjM2ODQgNi43NjMxNiAxLjIzNjg0IDcuMjM2ODQgMS41Mjg5OSA3LjUyODk5VjcuNTI4OTlDMS44MjA5NSA3LjgyMDk1IDIuMjk0MjYgNy44MjExNiAyLjU4NjQ5IDcuNTI5NDZMNi4yNSAzLjg3MjVWMTIuMjVDNi4yNSAxMi42NjQyIDYuNTg1NzkgMTMgNyAxM1YxM0M3LjQxNDIxIDEzIDcuNzUgMTIuNjY0MiA3Ljc1IDEyLjI1VjMuODcyNUwxMS40MDI3IDcuNTMxNzhDMTEuNjk2NiA3LjgyNjE5IDEyLjE3MzYgNy44MjY0MSAxMi40Njc3IDcuNTMyMjZWNy41MzIyNkMxMi43NjE3IDcuMjM4MyAxMi43NjE3IDYuNzYxNyAxMi40Njc3IDYuNDY3NzRMNy43MDcxMSAxLjcwNzExQzcuMzE2NTggMS4zMTY1OCA2LjY4MzQyIDEuMzE2NTggNi4yOTI4OSAxLjcwNzExTDEuNTI4OTkgNi40NzEwMVoiIGZpbGw9IiM2MTYxNjEiLz4KPC9zdmc+Cg==);
  --jp-icon-new-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwIDZoLThsLTItMkg0Yy0xLjExIDAtMS45OS44OS0xLjk5IDJMMiAxOGMwIDEuMTEuODkgMiAyIDJoMTZjMS4xMSAwIDItLjg5IDItMlY4YzAtMS4xMS0uODktMi0yLTJ6bS0xIDhoLTN2M2gtMnYtM2gtM3YtMmgzVjloMnYzaDN2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-not-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI1IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDMgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMTkgMTcuMTg0NCAyLjk2OTY4IDE0LjMwMzIgMS44NjA5NCAxMS40NDA5WiIvPgogICAgPHBhdGggY2xhc3M9ImpwLWljb24yIiBzdHJva2U9IiMzMzMzMzMiIHN0cm9rZS13aWR0aD0iMiIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOS4zMTU5MiA5LjMyMDMxKSIgZD0iTTcuMzY4NDIgMEwwIDcuMzY0NzkiLz4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDkuMzE1OTIgMTYuNjgzNikgc2NhbGUoMSAtMSkiIGQ9Ik03LjM2ODQyIDBMMCA3LjM2NDc5Ii8+Cjwvc3ZnPgo=);
  --jp-icon-notebook: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtbm90ZWJvb2staWNvbi1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNFRjZDMDAiPgogICAgPHBhdGggZD0iTTE4LjcgMy4zdjE1LjRIMy4zVjMuM2gxNS40bTEuNS0xLjVIMS44djE4LjNoMTguM2wuMS0xOC4zeiIvPgogICAgPHBhdGggZD0iTTE2LjUgMTYuNWwtNS40LTQuMy01LjYgNC4zdi0xMWgxMXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-numbering: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIiIGhlaWdodD0iMjIiIHZpZXdCb3g9IjAgMCAyOCAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTQgMTlINlYxOS41SDVWMjAuNUg2VjIxSDRWMjJIN1YxOEg0VjE5Wk01IDEwSDZWNkg0VjdINVYxMFpNNCAxM0g1LjhMNCAxNS4xVjE2SDdWMTVINS4yTDcgMTIuOVYxMkg0VjEzWk05IDdWOUgyM1Y3SDlaTTkgMjFIMjNWMTlIOVYyMVpNOSAxNUgyM1YxM0g5VjE1WiIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-offline-bolt: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyIDIuMDJjLTUuNTEgMC05Ljk4IDQuNDctOS45OCA5Ljk4czQuNDcgOS45OCA5Ljk4IDkuOTggOS45OC00LjQ3IDkuOTgtOS45OFMxNy41MSAyLjAyIDEyIDIuMDJ6TTExLjQ4IDIwdi02LjI2SDhMMTMgNHY2LjI2aDMuMzVMMTEuNDggMjB6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-palette: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE4IDEzVjIwSDRWNkg5LjAyQzkuMDcgNS4yOSA5LjI0IDQuNjIgOS41IDRINEMyLjkgNCAyIDQuOSAyIDZWMjBDMiAyMS4xIDIuOSAyMiA0IDIySDE4QzE5LjEgMjIgMjAgMjEuMSAyMCAyMFYxNUwxOCAxM1pNMTkuMyA4Ljg5QzE5Ljc0IDguMTkgMjAgNy4zOCAyMCA2LjVDMjAgNC4wMSAxNy45OSAyIDE1LjUgMkMxMy4wMSAyIDExIDQuMDEgMTEgNi41QzExIDguOTkgMTMuMDEgMTEgMTUuNDkgMTFDMTYuMzcgMTEgMTcuMTkgMTAuNzQgMTcuODggMTAuM0wyMSAxMy40MkwyMi40MiAxMkwxOS4zIDguODlaTTE1LjUgOUMxNC4xMiA5IDEzIDcuODggMTMgNi41QzEzIDUuMTIgMTQuMTIgNCAxNS41IDRDMTYuODggNCAxOCA1LjEyIDE4IDYuNUMxOCA3Ljg4IDE2Ljg4IDkgMTUuNSA5WiIvPgogICAgPHBhdGggZmlsbC1ydWxlPSJldmVub2RkIiBjbGlwLXJ1bGU9ImV2ZW5vZGQiIGQ9Ik00IDZIOS4wMTg5NEM5LjAwNjM5IDYuMTY1MDIgOSA2LjMzMTc2IDkgNi41QzkgOC44MTU3NyAxMC4yMTEgMTAuODQ4NyAxMi4wMzQzIDEySDlWMTRIMTZWMTIuOTgxMUMxNi41NzAzIDEyLjkzNzcgMTcuMTIgMTIuODIwNyAxNy42Mzk2IDEyLjYzOTZMMTggMTNWMjBINFY2Wk04IDhINlYxMEg4VjhaTTYgMTJIOFYxNEg2VjEyWk04IDE2SDZWMThIOFYxNlpNOSAxNkgxNlYxOEg5VjE2WiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-paste: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE5IDJoLTQuMThDMTQuNC44NCAxMy4zIDAgMTIgMGMtMS4zIDAtMi40Ljg0LTIuODIgMkg1Yy0xLjEgMC0yIC45LTIgMnYxNmMwIDEuMS45IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjRjMC0xLjEtLjktMi0yLTJ6bS03IDBjLjU1IDAgMSAuNDUgMSAxcy0uNDUgMS0xIDEtMS0uNDUtMS0xIC40NS0xIDEtMXptNyAxOEg1VjRoMnYzaDEwVjRoMnYxNnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-pdf: url(data:image/svg+xml;base64,PHN2ZwogICB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyMiAyMiIgd2lkdGg9IjE2Ij4KICAgIDxwYXRoIHRyYW5zZm9ybT0icm90YXRlKDQ1KSIgY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI0ZGMkEyQSIKICAgICAgIGQ9Im0gMjIuMzQ0MzY5LC0zLjAxNjM2NDIgaCA1LjYzODYwNCB2IDEuNTc5MjQzMyBoIC0zLjU0OTIyNyB2IDEuNTA4NjkyOTkgaCAzLjMzNzU3NiBWIDEuNjUwODE1NCBoIC0zLjMzNzU3NiB2IDMuNDM1MjYxMyBoIC0yLjA4OTM3NyB6IG0gLTcuMTM2NDQ0LDEuNTc5MjQzMyB2IDQuOTQzOTU0MyBoIDAuNzQ4OTIgcSAxLjI4MDc2MSwwIDEuOTUzNzAzLC0wLjYzNDk1MzUgMC42NzgzNjksLTAuNjM0OTUzNSAwLjY3ODM2OSwtMS44NDUxNjQxIDAsLTEuMjA0NzgzNTUgLTAuNjcyOTQyLC0xLjgzNDMxMDExIC0wLjY3Mjk0MiwtMC42Mjk1MjY1OSAtMS45NTkxMywtMC42Mjk1MjY1OSB6IG0gLTIuMDg5Mzc3LC0xLjU3OTI0MzMgaCAyLjIwMzM0MyBxIDEuODQ1MTY0LDAgMi43NDYwMzksMC4yNjU5MjA3IDAuOTA2MzAxLDAuMjYwNDkzNyAxLjU1MjEwOCwwLjg5MDAyMDMgMC41Njk4MywwLjU0ODEyMjMgMC44NDY2MDUsMS4yNjQ0ODAwNiAwLjI3Njc3NCwwLjcxNjM1NzgxIDAuMjc2Nzc0LDEuNjIyNjU4OTQgMCwwLjkxNzE1NTEgLTAuMjc2Nzc0LDEuNjM4OTM5OSAtMC4yNzY3NzUsMC43MTYzNTc4IC0wLjg0NjYwNSwxLjI2NDQ4IC0wLjY1MTIzNCwwLjYyOTUyNjYgLTEuNTYyOTYyLDAuODk1NDQ3MyAtMC45MTE3MjgsMC4yNjA0OTM3IC0yLjczNTE4NSwwLjI2MDQ5MzcgaCAtMi4yMDMzNDMgeiBtIC04LjE0NTg1NjUsMCBoIDMuNDY3ODIzIHEgMS41NDY2ODE2LDAgMi4zNzE1Nzg1LDAuNjg5MjIzIDAuODMwMzI0LDAuNjgzNzk2MSAwLjgzMDMyNCwxLjk1MzcwMzE0IDAsMS4yNzUzMzM5NyAtMC44MzAzMjQsMS45NjQ1NTcwNiBRIDkuOTg3MTk2MSwyLjI3NDkxNSA4LjQ0MDUxNDUsMi4yNzQ5MTUgSCA3LjA2MjA2ODQgViA1LjA4NjA3NjcgSCA0Ljk3MjY5MTUgWiBtIDIuMDg5Mzc2OSwxLjUxNDExOTkgdiAyLjI2MzAzOTQzIGggMS4xNTU5NDEgcSAwLjYwNzgxODgsMCAwLjkzODg2MjksLTAuMjkzMDU1NDcgMC4zMzEwNDQxLC0wLjI5ODQ4MjQxIDAuMzMxMDQ0MSwtMC44NDExNzc3MiAwLC0wLjU0MjY5NTMxIC0wLjMzMTA0NDEsLTAuODM1NzUwNzQgLTAuMzMxMDQ0MSwtMC4yOTMwNTU1IC0wLjkzODg2MjksLTAuMjkzMDU1NSB6IgovPgo8L3N2Zz4K);
  --jp-icon-python: url(data:image/svg+xml;base64,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);
  --jp-icon-r-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjE5NkYzIiBkPSJNNC40IDIuNWMxLjItLjEgMi45LS4zIDQuOS0uMyAyLjUgMCA0LjEuNCA1LjIgMS4zIDEgLjcgMS41IDEuOSAxLjUgMy41IDAgMi0xLjQgMy41LTIuOSA0LjEgMS4yLjQgMS43IDEuNiAyLjIgMyAuNiAxLjkgMSAzLjkgMS4zIDQuNmgtMy44Yy0uMy0uNC0uOC0xLjctMS4yLTMuN3MtMS4yLTIuNi0yLjYtMi42aC0uOXY2LjRINC40VjIuNXptMy43IDYuOWgxLjRjMS45IDAgMi45LS45IDIuOS0yLjNzLTEtMi4zLTIuOC0yLjNjLS43IDAtMS4zIDAtMS42LjJ2NC41aC4xdi0uMXoiLz4KPC9zdmc+Cg==);
  --jp-icon-react: url(data:image/svg+xml;base64,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);
  --jp-icon-redo: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgICA8cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIi8+PHBhdGggZD0iTTE4LjQgMTAuNkMxNi41NSA4Ljk5IDE0LjE1IDggMTEuNSA4Yy00LjY1IDAtOC41OCAzLjAzLTkuOTYgNy4yMkwzLjkgMTZjMS4wNS0zLjE5IDQuMDUtNS41IDcuNi01LjUgMS45NSAwIDMuNzMuNzIgNS4xMiAxLjg4TDEzIDE2aDlWN2wtMy42IDMuNnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-refresh: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTkgMTMuNWMtMi40OSAwLTQuNS0yLjAxLTQuNS00LjVTNi41MSA0LjUgOSA0LjVjMS4yNCAwIDIuMzYuNTIgMy4xNyAxLjMzTDEwIDhoNVYzbC0xLjc2IDEuNzZDMTIuMTUgMy42OCAxMC42NiAzIDkgMyA1LjY5IDMgMy4wMSA1LjY5IDMuMDEgOVM1LjY5IDE1IDkgMTVjMi45NyAwIDUuNDMtMi4xNiA1LjktNWgtMS41MmMtLjQ2IDItMi4yNCAzLjUtNC4zOCAzLjV6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-regex: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbi1hY2NlbnQyIiBmaWxsPSIjRkZGIj4KICAgIDxjaXJjbGUgY2xhc3M9InN0MiIgY3g9IjUuNSIgY3k9IjE0LjUiIHI9IjEuNSIvPgogICAgPHJlY3QgeD0iMTIiIHk9IjQiIGNsYXNzPSJzdDIiIHdpZHRoPSIxIiBoZWlnaHQ9IjgiLz4KICAgIDxyZWN0IHg9IjguNSIgeT0iNy41IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjg2NiAtMC41IDAuNSAwLjg2NiAtMi4zMjU1IDcuMzIxOSkiIGNsYXNzPSJzdDIiIHdpZHRoPSI4IiBoZWlnaHQ9IjEiLz4KICAgIDxyZWN0IHg9IjEyIiB5PSI0IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjUgLTAuODY2IDAuODY2IDAuNSAtMC42Nzc5IDE0LjgyNTIpIiBjbGFzcz0ic3QyIiB3aWR0aD0iMSIgaGVpZ2h0PSI4Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-run: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTggNXYxNGwxMS03eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-running: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMjU2IDhDMTE5IDggOCAxMTkgOCAyNTZzMTExIDI0OCAyNDggMjQ4IDI0OC0xMTEgMjQ4LTI0OFMzOTMgOCAyNTYgOHptOTYgMzI4YzAgOC44LTcuMiAxNi0xNiAxNkgxNzZjLTguOCAwLTE2LTcuMi0xNi0xNlYxNzZjMC04LjggNy4yLTE2IDE2LTE2aDE2MGM4LjggMCAxNiA3LjIgMTYgMTZ2MTYweiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-save: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE3IDNINWMtMS4xMSAwLTIgLjktMiAydjE0YzAgMS4xLjg5IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjdsLTQtNHptLTUgMTZjLTEuNjYgMC0zLTEuMzQtMy0zczEuMzQtMyAzLTMgMyAxLjM0IDMgMy0xLjM0IDMtMyAzem0zLTEwSDVWNWgxMHY0eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-search: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjEsMTAuOWgtMC43bC0wLjItMC4yYzAuOC0wLjksMS4zLTIuMiwxLjMtMy41YzAtMy0yLjQtNS40LTUuNC01LjRTMS44LDQuMiwxLjgsNy4xczIuNCw1LjQsNS40LDUuNCBjMS4zLDAsMi41LTAuNSwzLjUtMS4zbDAuMiwwLjJ2MC43bDQuMSw0LjFsMS4yLTEuMkwxMi4xLDEwLjl6IE03LjEsMTAuOWMtMi4xLDAtMy43LTEuNy0zLjctMy43czEuNy0zLjcsMy43LTMuN3MzLjcsMS43LDMuNywzLjcgUzkuMiwxMC45LDcuMSwxMC45eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-settings: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuNDMgMTIuOThjLjA0LS4zMi4wNy0uNjQuMDctLjk4cy0uMDMtLjY2LS4wNy0uOThsMi4xMS0xLjY1Yy4xOS0uMTUuMjQtLjQyLjEyLS42NGwtMi0zLjQ2Yy0uMTItLjIyLS4zOS0uMy0uNjEtLjIybC0yLjQ5IDFjLS41Mi0uNC0xLjA4LS43My0xLjY5LS45OGwtLjM4LTIuNjVBLjQ4OC40ODggMCAwMDE0IDJoLTRjLS4yNSAwLS40Ni4xOC0uNDkuNDJsLS4zOCAyLjY1Yy0uNjEuMjUtMS4xNy41OS0xLjY5Ljk4bC0yLjQ5LTFjLS4yMy0uMDktLjQ5IDAtLjYxLjIybC0yIDMuNDZjLS4xMy4yMi0uMDcuNDkuMTIuNjRsMi4xMSAxLjY1Yy0uMDQuMzItLjA3LjY1LS4wNy45OHMuMDMuNjYuMDcuOThsLTIuMTEgMS42NWMtLjE5LjE1LS4yNC40Mi0uMTIuNjRsMiAzLjQ2Yy4xMi4yMi4zOS4zLjYxLjIybDIuNDktMWMuNTIuNCAxLjA4LjczIDEuNjkuOThsLjM4IDIuNjVjLjAzLjI0LjI0LjQyLjQ5LjQyaDRjLjI1IDAgLjQ2LS4xOC40OS0uNDJsLjM4LTIuNjVjLjYxLS4yNSAxLjE3LS41OSAxLjY5LS45OGwyLjQ5IDFjLjIzLjA5LjQ5IDAgLjYxLS4yMmwyLTMuNDZjLjEyLS4yMi4wNy0uNDktLjEyLS42NGwtMi4xMS0xLjY1ek0xMiAxNS41Yy0xLjkzIDAtMy41LTEuNTctMy41LTMuNXMxLjU3LTMuNSAzLjUtMy41IDMuNSAxLjU3IDMuNSAzLjUtMS41NyAzLjUtMy41IDMuNXoiLz4KPC9zdmc+Cg==);
  --jp-icon-share: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIHZpZXdCb3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTSAxOCAyIEMgMTYuMzU0OTkgMiAxNSAzLjM1NDk5MDQgMTUgNSBDIDE1IDUuMTkwOTUyOSAxNS4wMjE3OTEgNS4zNzcxMjI0IDE1LjA1NjY0MSA1LjU1ODU5MzggTCA3LjkyMTg3NSA5LjcyMDcwMzEgQyA3LjM5ODUzOTkgOS4yNzc4NTM5IDYuNzMyMDc3MSA5IDYgOSBDIDQuMzU0OTkwNCA5IDMgMTAuMzU0OTkgMyAxMiBDIDMgMTMuNjQ1MDEgNC4zNTQ5OTA0IDE1IDYgMTUgQyA2LjczMjA3NzEgMTUgNy4zOTg1Mzk5IDE0LjcyMjE0NiA3LjkyMTg3NSAxNC4yNzkyOTcgTCAxNS4wNTY2NDEgMTguNDM5NDUzIEMgMTUuMDIxNTU1IDE4LjYyMTUxNCAxNSAxOC44MDgzODYgMTUgMTkgQyAxNSAyMC42NDUwMSAxNi4zNTQ5OSAyMiAxOCAyMiBDIDE5LjY0NTAxIDIyIDIxIDIwLjY0NTAxIDIxIDE5IEMgMjEgMTcuMzU0OTkgMTkuNjQ1MDEgMTYgMTggMTYgQyAxNy4yNjc0OCAxNiAxNi42MDE1OTMgMTYuMjc5MzI4IDE2LjA3ODEyNSAxNi43MjI2NTYgTCA4Ljk0MzM1OTQgMTIuNTU4NTk0IEMgOC45NzgyMDk1IDEyLjM3NzEyMiA5IDEyLjE5MDk1MyA5IDEyIEMgOSAxMS44MDkwNDcgOC45NzgyMDk1IDExLjYyMjg3OCA4Ljk0MzM1OTQgMTEuNDQxNDA2IEwgMTYuMDc4MTI1IDcuMjc5Mjk2OSBDIDE2LjYwMTQ2IDcuNzIyMTQ2MSAxNy4yNjc5MjMgOCAxOCA4IEMgMTkuNjQ1MDEgOCAyMSA2LjY0NTAwOTYgMjEgNSBDIDIxIDMuMzU0OTkwNCAxOS42NDUwMSAyIDE4IDIgeiBNIDE4IDQgQyAxOC41NjQxMjkgNCAxOSA0LjQzNTg3MDYgMTkgNSBDIDE5IDUuNTY0MTI5NCAxOC41NjQxMjkgNiAxOCA2IEMgMTcuNDM1ODcxIDYgMTcgNS41NjQxMjk0IDE3IDUgQyAxNyA0LjQzNTg3MDYgMTcuNDM1ODcxIDQgMTggNCB6IE0gNiAxMSBDIDYuNTY0MTI5NCAxMSA3IDExLjQzNTg3MSA3IDEyIEMgNyAxMi41NjQxMjkgNi41NjQxMjk0IDEzIDYgMTMgQyA1LjQzNTg3MDYgMTMgNSAxMi41NjQxMjkgNSAxMiBDIDUgMTEuNDM1ODcxIDUuNDM1ODcwNiAxMSA2IDExIHogTSAxOCAxOCBDIDE4LjU2NDEyOSAxOCAxOSAxOC40MzU4NzEgMTkgMTkgQyAxOSAxOS41NjQxMjkgMTguNTY0MTI5IDIwIDE4IDIwIEMgMTcuNDM1ODcxIDIwIDE3IDE5LjU2NDEyOSAxNyAxOSBDIDE3IDE4LjQzNTg3MSAxNy40MzU4NzEgMTggMTggMTggeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-spreadsheet: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNENBRjUwIiBkPSJNMi4yIDIuMnYxNy42aDE3LjZWMi4ySDIuMnptMTUuNCA3LjdoLTUuNVY0LjRoNS41djUuNXpNOS45IDQuNHY1LjVINC40VjQuNGg1LjV6bS01LjUgNy43aDUuNXY1LjVINC40di01LjV6bTcuNyA1LjV2LTUuNWg1LjV2NS41aC01LjV6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-stop: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik02IDZoMTJ2MTJINnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-tab: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIxIDNIM2MtMS4xIDAtMiAuOS0yIDJ2MTRjMCAxLjEuOSAyIDIgMmgxOGMxLjEgMCAyLS45IDItMlY1YzAtMS4xLS45LTItMi0yem0wIDE2SDNWNWgxMHY0aDh2MTB6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-table-rows: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik0yMSw4SDNWNGgxOFY4eiBNMjEsMTBIM3Y0aDE4VjEweiBNMjEsMTZIM3Y0aDE4VjE2eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-tag: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjgiIGhlaWdodD0iMjgiIHZpZXdCb3g9IjAgMCA0MyAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTI4LjgzMzIgMTIuMzM0TDMyLjk5OTggMTYuNTAwN0wzNy4xNjY1IDEyLjMzNEgyOC44MzMyWiIvPgoJCTxwYXRoIGQ9Ik0xNi4yMDk1IDIxLjYxMDRDMTUuNjg3MyAyMi4xMjk5IDE0Ljg0NDMgMjIuMTI5OSAxNC4zMjQ4IDIxLjYxMDRMNi45ODI5IDE0LjcyNDVDNi41NzI0IDE0LjMzOTQgNi4wODMxMyAxMy42MDk4IDYuMDQ3ODYgMTMuMDQ4MkM1Ljk1MzQ3IDExLjUyODggNi4wMjAwMiA4LjYxOTQ0IDYuMDY2MjEgNy4wNzY5NUM2LjA4MjgxIDYuNTE0NzcgNi41NTU0OCA2LjA0MzQ3IDcuMTE4MDQgNi4wMzA1NUM5LjA4ODYzIDUuOTg0NzMgMTMuMjYzOCA1LjkzNTc5IDEzLjY1MTggNi4zMjQyNUwyMS43MzY5IDEzLjYzOUMyMi4yNTYgMTQuMTU4NSAyMS43ODUxIDE1LjQ3MjQgMjEuMjYyIDE1Ljk5NDZMMTYuMjA5NSAyMS42MTA0Wk05Ljc3NTg1IDguMjY1QzkuMzM1NTEgNy44MjU2NiA4LjYyMzUxIDcuODI1NjYgOC4xODI4IDguMjY1QzcuNzQzNDYgOC43MDU3MSA3Ljc0MzQ2IDkuNDE3MzMgOC4xODI4IDkuODU2NjdDOC42MjM4MiAxMC4yOTY0IDkuMzM1ODIgMTAuMjk2NCA5Ljc3NTg1IDkuODU2NjdDMTAuMjE1NiA5LjQxNzMzIDEwLjIxNTYgOC43MDUzMyA5Ljc3NTg1IDguMjY1WiIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-terminal: url(data:image/svg+xml;base64,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);
  --jp-icon-text-editor: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtdGV4dC1lZGl0b3ItaWNvbi1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xNSAxNUgzdjJoMTJ2LTJ6bTAtOEgzdjJoMTJWN3pNMyAxM2gxOHYtMkgzdjJ6bTAgOGgxOHYtMkgzdjJ6TTMgM3YyaDE4VjNIM3oiLz4KPC9zdmc+Cg==);
  --jp-icon-toc: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik03LDVIMjFWN0g3VjVNNywxM1YxMUgyMVYxM0g3TTQsNC41QTEuNSwxLjUgMCAwLDEgNS41LDZBMS41LDEuNSAwIDAsMSA0LDcuNUExLjUsMS41IDAgMCwxIDIuNSw2QTEuNSwxLjUgMCAwLDEgNCw0LjVNNCwxMC41QTEuNSwxLjUgMCAwLDEgNS41LDEyQTEuNSwxLjUgMCAwLDEgNCwxMy41QTEuNSwxLjUgMCAwLDEgMi41LDEyQTEuNSwxLjUgMCAwLDEgNCwxMC41TTcsMTlWMTdIMjFWMTlIN000LDE2LjVBMS41LDEuNSAwIDAsMSA1LjUsMThBMS41LDEuNSAwIDAsMSA0LDE5LjVBMS41LDEuNSAwIDAsMSAyLjUsMThBMS41LDEuNSAwIDAsMSA0LDE2LjVaIiAvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-tree-view: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik0yMiAxMVYzaC03djNIOVYzSDJ2OGg3VjhoMnYxMGg0djNoN3YtOGgtN3YzaC0yVjhoMnYzeiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDIgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMiAxNy4xODQ0IDIuOTY5NjggMTQuMzAzMiAxLjg2MDk0IDExLjQ0MDlaIi8+CiAgICA8cGF0aCBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiMzMzMzMzMiIHN0cm9rZT0iIzMzMzMzMyIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOCA5Ljg2NzE5KSIgZD0iTTIuODYwMTUgNC44NjUzNUwwLjcyNjU0OSAyLjk5OTU5TDAgMy42MzA0NUwyLjg2MDE1IDYuMTMxNTdMOCAwLjYzMDg3Mkw3LjI3ODU3IDBMMi44NjAxNSA0Ljg2NTM1WiIvPgo8L3N2Zz4K);
  --jp-icon-undo: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjUgOGMtMi42NSAwLTUuMDUuOTktNi45IDIuNkwyIDd2OWg5bC0zLjYyLTMuNjJjMS4zOS0xLjE2IDMuMTYtMS44OCA1LjEyLTEuODggMy41NCAwIDYuNTUgMi4zMSA3LjYgNS41bDIuMzctLjc4QzIxLjA4IDExLjAzIDE3LjE1IDggMTIuNSA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-user: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIHZpZXdCb3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE2IDdhNCA0IDAgMTEtOCAwIDQgNCAwIDAxOCAwek0xMiAxNGE3IDcgMCAwMC03IDdoMTRhNyA3IDAgMDAtNy03eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-users: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZlcnNpb249IjEuMSIgdmlld0JveD0iMCAwIDM2IDI0IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogPGcgY2xhc3M9ImpwLWljb24zIiB0cmFuc2Zvcm09Im1hdHJpeCgxLjczMjcgMCAwIDEuNzMyNyAtMy42MjgyIC4wOTk1NzcpIiBmaWxsPSIjNjE2MTYxIj4KICA8cGF0aCB0cmFuc2Zvcm09Im1hdHJpeCgxLjUsMCwwLDEuNSwwLC02KSIgZD0ibTEyLjE4NiA3LjUwOThjLTEuMDUzNSAwLTEuOTc1NyAwLjU2NjUtMi40Nzg1IDEuNDEwMiAwLjc1MDYxIDAuMzEyNzcgMS4zOTc0IDAuODI2NDggMS44NzMgMS40NzI3aDMuNDg2M2MwLTEuNTkyLTEuMjg4OS0yLjg4MjgtMi44ODA5LTIuODgyOHoiLz4KICA8cGF0aCBkPSJtMjAuNDY1IDIuMzg5NWEyLjE4ODUgMi4xODg1IDAgMCAxLTIuMTg4NCAyLjE4ODUgMi4xODg1IDIuMTg4NSAwIDAgMS0yLjE4ODUtMi4xODg1IDIuMTg4NSAyLjE4ODUgMCAwIDEgMi4xODg1LTIuMTg4NSAyLjE4ODUgMi4xODg1IDAgMCAxIDIuMTg4NCAyLjE4ODV6Ii8+CiAgPHBhdGggdHJhbnNmb3JtPSJtYXRyaXgoMS41LDAsMCwxLjUsMCwtNikiIGQ9Im0zLjU4OTggOC40MjE5Yy0xLjExMjYgMC0yLjAxMzcgMC45MDExMS0yLjAxMzcgMi4wMTM3aDIuODE0NWMwLjI2Nzk3LTAuMzczMDkgMC41OTA3LTAuNzA0MzUgMC45NTg5OC0wLjk3ODUyLTAuMzQ0MzMtMC42MTY4OC0xLjAwMzEtMS4wMzUyLTEuNzU5OC0xLjAzNTJ6Ii8+CiAgPHBhdGggZD0ibTYuOTE1NCA0LjYyM2ExLjUyOTQgMS41Mjk0IDAgMCAxLTEuNTI5NCAxLjUyOTQgMS41Mjk0IDEuNTI5NCAwIDAgMS0xLjUyOTQtMS41Mjk0IDEuNTI5NCAxLjUyOTQgMCAwIDEgMS41Mjk0LTEuNTI5NCAxLjUyOTQgMS41Mjk0IDAgMCAxIDEuNTI5NCAxLjUyOTR6Ii8+CiAgPHBhdGggZD0ibTYuMTM1IDEzLjUzNWMwLTMuMjM5MiAyLjYyNTktNS44NjUgNS44NjUtNS44NjUgMy4yMzkyIDAgNS44NjUgMi42MjU5IDUuODY1IDUuODY1eiIvPgogIDxjaXJjbGUgY3g9IjEyIiBjeT0iMy43Njg1IiByPSIyLjk2ODUiLz4KIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-vega: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbjEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjEyMTIxIj4KICAgIDxwYXRoIGQ9Ik0xMC42IDUuNGwyLjItMy4ySDIuMnY3LjNsNC02LjZ6Ii8+CiAgICA8cGF0aCBkPSJNMTUuOCAyLjJsLTQuNCA2LjZMNyA2LjNsLTQuOCA4djUuNWgxNy42VjIuMmgtNHptLTcgMTUuNEg1LjV2LTQuNGgzLjN2NC40em00LjQgMEg5LjhWOS44aDMuNHY3Ljh6bTQuNCAwaC0zLjRWNi41aDMuNHYxMS4xeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-word: url(data:image/svg+xml;base64,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);
  --jp-icon-yaml: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1jb250cmFzdDIganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjRDgxQjYwIj4KICAgIDxwYXRoIGQ9Ik03LjIgMTguNnYtNS40TDMgNS42aDMuM2wxLjQgMy4xYy4zLjkuNiAxLjYgMSAyLjUuMy0uOC42LTEuNiAxLTIuNWwxLjQtMy4xaDMuNGwtNC40IDcuNnY1LjVsLTIuOS0uMXoiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxNi41IiByPSIyLjEiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxMSIgcj0iMi4xIi8+CiAgPC9nPgo8L3N2Zz4K);
}

/* Icon CSS class declarations */

.jp-AddAboveIcon {
  background-image: var(--jp-icon-add-above);
}

.jp-AddBelowIcon {
  background-image: var(--jp-icon-add-below);
}

.jp-AddIcon {
  background-image: var(--jp-icon-add);
}

.jp-BellIcon {
  background-image: var(--jp-icon-bell);
}

.jp-BugDotIcon {
  background-image: var(--jp-icon-bug-dot);
}

.jp-BugIcon {
  background-image: var(--jp-icon-bug);
}

.jp-BuildIcon {
  background-image: var(--jp-icon-build);
}

.jp-CaretDownEmptyIcon {
  background-image: var(--jp-icon-caret-down-empty);
}

.jp-CaretDownEmptyThinIcon {
  background-image: var(--jp-icon-caret-down-empty-thin);
}

.jp-CaretDownIcon {
  background-image: var(--jp-icon-caret-down);
}

.jp-CaretLeftIcon {
  background-image: var(--jp-icon-caret-left);
}

.jp-CaretRightIcon {
  background-image: var(--jp-icon-caret-right);
}

.jp-CaretUpEmptyThinIcon {
  background-image: var(--jp-icon-caret-up-empty-thin);
}

.jp-CaretUpIcon {
  background-image: var(--jp-icon-caret-up);
}

.jp-CaseSensitiveIcon {
  background-image: var(--jp-icon-case-sensitive);
}

.jp-CheckIcon {
  background-image: var(--jp-icon-check);
}

.jp-CircleEmptyIcon {
  background-image: var(--jp-icon-circle-empty);
}

.jp-CircleIcon {
  background-image: var(--jp-icon-circle);
}

.jp-ClearIcon {
  background-image: var(--jp-icon-clear);
}

.jp-CloseIcon {
  background-image: var(--jp-icon-close);
}

.jp-CodeCheckIcon {
  background-image: var(--jp-icon-code-check);
}

.jp-CodeIcon {
  background-image: var(--jp-icon-code);
}

.jp-CollapseAllIcon {
  background-image: var(--jp-icon-collapse-all);
}

.jp-ConsoleIcon {
  background-image: var(--jp-icon-console);
}

.jp-CopyIcon {
  background-image: var(--jp-icon-copy);
}

.jp-CopyrightIcon {
  background-image: var(--jp-icon-copyright);
}

.jp-CutIcon {
  background-image: var(--jp-icon-cut);
}

.jp-DeleteIcon {
  background-image: var(--jp-icon-delete);
}

.jp-DownloadIcon {
  background-image: var(--jp-icon-download);
}

.jp-DuplicateIcon {
  background-image: var(--jp-icon-duplicate);
}

.jp-EditIcon {
  background-image: var(--jp-icon-edit);
}

.jp-EllipsesIcon {
  background-image: var(--jp-icon-ellipses);
}

.jp-ErrorIcon {
  background-image: var(--jp-icon-error);
}

.jp-ExpandAllIcon {
  background-image: var(--jp-icon-expand-all);
}

.jp-ExtensionIcon {
  background-image: var(--jp-icon-extension);
}

.jp-FastForwardIcon {
  background-image: var(--jp-icon-fast-forward);
}

.jp-FileIcon {
  background-image: var(--jp-icon-file);
}

.jp-FileUploadIcon {
  background-image: var(--jp-icon-file-upload);
}

.jp-FilterDotIcon {
  background-image: var(--jp-icon-filter-dot);
}

.jp-FilterIcon {
  background-image: var(--jp-icon-filter);
}

.jp-FilterListIcon {
  background-image: var(--jp-icon-filter-list);
}

.jp-FolderFavoriteIcon {
  background-image: var(--jp-icon-folder-favorite);
}

.jp-FolderIcon {
  background-image: var(--jp-icon-folder);
}

.jp-HomeIcon {
  background-image: var(--jp-icon-home);
}

.jp-Html5Icon {
  background-image: var(--jp-icon-html5);
}

.jp-ImageIcon {
  background-image: var(--jp-icon-image);
}

.jp-InfoIcon {
  background-image: var(--jp-icon-info);
}

.jp-InspectorIcon {
  background-image: var(--jp-icon-inspector);
}

.jp-JsonIcon {
  background-image: var(--jp-icon-json);
}

.jp-JuliaIcon {
  background-image: var(--jp-icon-julia);
}

.jp-JupyterFaviconIcon {
  background-image: var(--jp-icon-jupyter-favicon);
}

.jp-JupyterIcon {
  background-image: var(--jp-icon-jupyter);
}

.jp-JupyterlabWordmarkIcon {
  background-image: var(--jp-icon-jupyterlab-wordmark);
}

.jp-KernelIcon {
  background-image: var(--jp-icon-kernel);
}

.jp-KeyboardIcon {
  background-image: var(--jp-icon-keyboard);
}

.jp-LaunchIcon {
  background-image: var(--jp-icon-launch);
}

.jp-LauncherIcon {
  background-image: var(--jp-icon-launcher);
}

.jp-LineFormIcon {
  background-image: var(--jp-icon-line-form);
}

.jp-LinkIcon {
  background-image: var(--jp-icon-link);
}

.jp-ListIcon {
  background-image: var(--jp-icon-list);
}

.jp-MarkdownIcon {
  background-image: var(--jp-icon-markdown);
}

.jp-MoveDownIcon {
  background-image: var(--jp-icon-move-down);
}

.jp-MoveUpIcon {
  background-image: var(--jp-icon-move-up);
}

.jp-NewFolderIcon {
  background-image: var(--jp-icon-new-folder);
}

.jp-NotTrustedIcon {
  background-image: var(--jp-icon-not-trusted);
}

.jp-NotebookIcon {
  background-image: var(--jp-icon-notebook);
}

.jp-NumberingIcon {
  background-image: var(--jp-icon-numbering);
}

.jp-OfflineBoltIcon {
  background-image: var(--jp-icon-offline-bolt);
}

.jp-PaletteIcon {
  background-image: var(--jp-icon-palette);
}

.jp-PasteIcon {
  background-image: var(--jp-icon-paste);
}

.jp-PdfIcon {
  background-image: var(--jp-icon-pdf);
}

.jp-PythonIcon {
  background-image: var(--jp-icon-python);
}

.jp-RKernelIcon {
  background-image: var(--jp-icon-r-kernel);
}

.jp-ReactIcon {
  background-image: var(--jp-icon-react);
}

.jp-RedoIcon {
  background-image: var(--jp-icon-redo);
}

.jp-RefreshIcon {
  background-image: var(--jp-icon-refresh);
}

.jp-RegexIcon {
  background-image: var(--jp-icon-regex);
}

.jp-RunIcon {
  background-image: var(--jp-icon-run);
}

.jp-RunningIcon {
  background-image: var(--jp-icon-running);
}

.jp-SaveIcon {
  background-image: var(--jp-icon-save);
}

.jp-SearchIcon {
  background-image: var(--jp-icon-search);
}

.jp-SettingsIcon {
  background-image: var(--jp-icon-settings);
}

.jp-ShareIcon {
  background-image: var(--jp-icon-share);
}

.jp-SpreadsheetIcon {
  background-image: var(--jp-icon-spreadsheet);
}

.jp-StopIcon {
  background-image: var(--jp-icon-stop);
}

.jp-TabIcon {
  background-image: var(--jp-icon-tab);
}

.jp-TableRowsIcon {
  background-image: var(--jp-icon-table-rows);
}

.jp-TagIcon {
  background-image: var(--jp-icon-tag);
}

.jp-TerminalIcon {
  background-image: var(--jp-icon-terminal);
}

.jp-TextEditorIcon {
  background-image: var(--jp-icon-text-editor);
}

.jp-TocIcon {
  background-image: var(--jp-icon-toc);
}

.jp-TreeViewIcon {
  background-image: var(--jp-icon-tree-view);
}

.jp-TrustedIcon {
  background-image: var(--jp-icon-trusted);
}

.jp-UndoIcon {
  background-image: var(--jp-icon-undo);
}

.jp-UserIcon {
  background-image: var(--jp-icon-user);
}

.jp-UsersIcon {
  background-image: var(--jp-icon-users);
}

.jp-VegaIcon {
  background-image: var(--jp-icon-vega);
}

.jp-WordIcon {
  background-image: var(--jp-icon-word);
}

.jp-YamlIcon {
  background-image: var(--jp-icon-yaml);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * (DEPRECATED) Support for consuming icons as CSS background images
 */

.jp-Icon,
.jp-MaterialIcon {
  background-position: center;
  background-repeat: no-repeat;
  background-size: 16px;
  min-width: 16px;
  min-height: 16px;
}

.jp-Icon-cover {
  background-position: center;
  background-repeat: no-repeat;
  background-size: cover;
}

/**
 * (DEPRECATED) Support for specific CSS icon sizes
 */

.jp-Icon-16 {
  background-size: 16px;
  min-width: 16px;
  min-height: 16px;
}

.jp-Icon-18 {
  background-size: 18px;
  min-width: 18px;
  min-height: 18px;
}

.jp-Icon-20 {
  background-size: 20px;
  min-width: 20px;
  min-height: 20px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.lm-TabBar .lm-TabBar-addButton {
  align-items: center;
  display: flex;
  padding: 4px;
  padding-bottom: 5px;
  margin-right: 1px;
  background-color: var(--jp-layout-color2);
}

.lm-TabBar .lm-TabBar-addButton:hover {
  background-color: var(--jp-layout-color1);
}

.lm-DockPanel-tabBar .lm-TabBar-tab {
  width: var(--jp-private-horizontal-tab-width);
}

.lm-DockPanel-tabBar .lm-TabBar-content {
  flex: unset;
}

.lm-DockPanel-tabBar[data-orientation='horizontal'] {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * Support for icons as inline SVG HTMLElements
 */

/* recolor the primary elements of an icon */
.jp-icon0[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon1[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon2[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon3[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon4[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon0[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon1[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon2[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon3[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon4[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/* recolor the accent elements of an icon */
.jp-icon-accent0[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-accent1[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-accent2[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-accent3[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-accent4[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-accent0[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-accent1[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-accent2[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-accent3[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-accent4[stroke] {
  stroke: var(--jp-layout-color4);
}

/* set the color of an icon to transparent */
.jp-icon-none[fill] {
  fill: none;
}

.jp-icon-none[stroke] {
  stroke: none;
}

/* brand icon colors. Same for light and dark */
.jp-icon-brand0[fill] {
  fill: var(--jp-brand-color0);
}

.jp-icon-brand1[fill] {
  fill: var(--jp-brand-color1);
}

.jp-icon-brand2[fill] {
  fill: var(--jp-brand-color2);
}

.jp-icon-brand3[fill] {
  fill: var(--jp-brand-color3);
}

.jp-icon-brand4[fill] {
  fill: var(--jp-brand-color4);
}

.jp-icon-brand0[stroke] {
  stroke: var(--jp-brand-color0);
}

.jp-icon-brand1[stroke] {
  stroke: var(--jp-brand-color1);
}

.jp-icon-brand2[stroke] {
  stroke: var(--jp-brand-color2);
}

.jp-icon-brand3[stroke] {
  stroke: var(--jp-brand-color3);
}

.jp-icon-brand4[stroke] {
  stroke: var(--jp-brand-color4);
}

/* warn icon colors. Same for light and dark */
.jp-icon-warn0[fill] {
  fill: var(--jp-warn-color0);
}

.jp-icon-warn1[fill] {
  fill: var(--jp-warn-color1);
}

.jp-icon-warn2[fill] {
  fill: var(--jp-warn-color2);
}

.jp-icon-warn3[fill] {
  fill: var(--jp-warn-color3);
}

.jp-icon-warn0[stroke] {
  stroke: var(--jp-warn-color0);
}

.jp-icon-warn1[stroke] {
  stroke: var(--jp-warn-color1);
}

.jp-icon-warn2[stroke] {
  stroke: var(--jp-warn-color2);
}

.jp-icon-warn3[stroke] {
  stroke: var(--jp-warn-color3);
}

/* icon colors that contrast well with each other and most backgrounds */
.jp-icon-contrast0[fill] {
  fill: var(--jp-icon-contrast-color0);
}

.jp-icon-contrast1[fill] {
  fill: var(--jp-icon-contrast-color1);
}

.jp-icon-contrast2[fill] {
  fill: var(--jp-icon-contrast-color2);
}

.jp-icon-contrast3[fill] {
  fill: var(--jp-icon-contrast-color3);
}

.jp-icon-contrast0[stroke] {
  stroke: var(--jp-icon-contrast-color0);
}

.jp-icon-contrast1[stroke] {
  stroke: var(--jp-icon-contrast-color1);
}

.jp-icon-contrast2[stroke] {
  stroke: var(--jp-icon-contrast-color2);
}

.jp-icon-contrast3[stroke] {
  stroke: var(--jp-icon-contrast-color3);
}

.jp-icon-dot[fill] {
  fill: var(--jp-warn-color0);
}

.jp-jupyter-icon-color[fill] {
  fill: var(--jp-jupyter-icon-color, var(--jp-warn-color0));
}

.jp-notebook-icon-color[fill] {
  fill: var(--jp-notebook-icon-color, var(--jp-warn-color0));
}

.jp-json-icon-color[fill] {
  fill: var(--jp-json-icon-color, var(--jp-warn-color1));
}

.jp-console-icon-color[fill] {
  fill: var(--jp-console-icon-color, white);
}

.jp-console-icon-background-color[fill] {
  fill: var(--jp-console-icon-background-color, var(--jp-brand-color1));
}

.jp-terminal-icon-color[fill] {
  fill: var(--jp-terminal-icon-color, var(--jp-layout-color2));
}

.jp-terminal-icon-background-color[fill] {
  fill: var(
    --jp-terminal-icon-background-color,
    var(--jp-inverse-layout-color2)
  );
}

.jp-text-editor-icon-color[fill] {
  fill: var(--jp-text-editor-icon-color, var(--jp-inverse-layout-color3));
}

.jp-inspector-icon-color[fill] {
  fill: var(--jp-inspector-icon-color, var(--jp-inverse-layout-color3));
}

/* CSS for icons in selected filebrowser listing items */
.jp-DirListing-item.jp-mod-selected .jp-icon-selectable[fill] {
  fill: #fff;
}

.jp-DirListing-item.jp-mod-selected .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}

/* stylelint-disable selector-max-class, selector-max-compound-selectors */

/**
* TODO: come up with non css-hack solution for showing the busy icon on top
*  of the close icon
* CSS for complex behavior of close icon of tabs in the main area tabbar
*/
.lm-DockPanel-tabBar
  .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon3[fill] {
  fill: none;
}

.lm-DockPanel-tabBar
  .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon-busy[fill] {
  fill: var(--jp-inverse-layout-color3);
}

/* stylelint-enable selector-max-class, selector-max-compound-selectors */

/* CSS for icons in status bar */
#jp-main-statusbar .jp-mod-selected .jp-icon-selectable[fill] {
  fill: #fff;
}

#jp-main-statusbar .jp-mod-selected .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}

/* special handling for splash icon CSS. While the theme CSS reloads during
   splash, the splash icon can loose theming. To prevent that, we set a
   default for its color variable */
:root {
  --jp-warn-color0: var(--md-orange-700);
}

/* not sure what to do with this one, used in filebrowser listing */
.jp-DragIcon {
  margin-right: 4px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * Support for alt colors for icons as inline SVG HTMLElements
 */

/* alt recolor the primary elements of an icon */
.jp-icon-alt .jp-icon0[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-alt .jp-icon1[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-alt .jp-icon2[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-alt .jp-icon3[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-alt .jp-icon4[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-alt .jp-icon0[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-alt .jp-icon1[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-alt .jp-icon2[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-alt .jp-icon3[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-alt .jp-icon4[stroke] {
  stroke: var(--jp-layout-color4);
}

/* alt recolor the accent elements of an icon */
.jp-icon-alt .jp-icon-accent0[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-alt .jp-icon-accent1[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-alt .jp-icon-accent2[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-alt .jp-icon-accent3[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-alt .jp-icon-accent4[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-alt .jp-icon-accent0[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-alt .jp-icon-accent1[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-alt .jp-icon-accent2[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-alt .jp-icon-accent3[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-alt .jp-icon-accent4[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-icon-hoverShow:not(:hover) .jp-icon-hoverShow-content {
  display: none !important;
}

/**
 * Support for hover colors for icons as inline SVG HTMLElements
 */

/**
 * regular colors
 */

/* recolor the primary elements of an icon */
.jp-icon-hover :hover .jp-icon0-hover[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-hover :hover .jp-icon1-hover[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-hover :hover .jp-icon2-hover[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-hover :hover .jp-icon3-hover[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-hover :hover .jp-icon4-hover[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-hover :hover .jp-icon0-hover[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-hover :hover .jp-icon1-hover[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-hover :hover .jp-icon2-hover[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-hover :hover .jp-icon3-hover[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-hover :hover .jp-icon4-hover[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/* recolor the accent elements of an icon */
.jp-icon-hover :hover .jp-icon-accent0-hover[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-hover :hover .jp-icon-accent1-hover[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-hover :hover .jp-icon-accent2-hover[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-hover :hover .jp-icon-accent3-hover[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-hover :hover .jp-icon-accent4-hover[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-hover :hover .jp-icon-accent0-hover[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-hover :hover .jp-icon-accent1-hover[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-hover :hover .jp-icon-accent2-hover[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-hover :hover .jp-icon-accent3-hover[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-hover :hover .jp-icon-accent4-hover[stroke] {
  stroke: var(--jp-layout-color4);
}

/* set the color of an icon to transparent */
.jp-icon-hover :hover .jp-icon-none-hover[fill] {
  fill: none;
}

.jp-icon-hover :hover .jp-icon-none-hover[stroke] {
  stroke: none;
}

/**
 * inverse colors
 */

/* inverse recolor the primary elements of an icon */
.jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[stroke] {
  stroke: var(--jp-layout-color4);
}

/* inverse recolor the accent elements of an icon */
.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-IFrame {
  width: 100%;
  height: 100%;
}

.jp-IFrame > iframe {
  border: none;
}

/*
When drag events occur, `lm-mod-override-cursor` is added to the body.
Because iframes steal all cursor events, the following two rules are necessary
to suppress pointer events while resize drags are occurring. There may be a
better solution to this problem.
*/
body.lm-mod-override-cursor .jp-IFrame {
  position: relative;
}

body.lm-mod-override-cursor .jp-IFrame::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-HoverBox {
  position: fixed;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-FormGroup-content fieldset {
  border: none;
  padding: 0;
  min-width: 0;
  width: 100%;
}

/* stylelint-disable selector-max-type */

.jp-FormGroup-content fieldset .jp-inputFieldWrapper input,
.jp-FormGroup-content fieldset .jp-inputFieldWrapper select,
.jp-FormGroup-content fieldset .jp-inputFieldWrapper textarea {
  font-size: var(--jp-content-font-size2);
  border-color: var(--jp-input-border-color);
  border-style: solid;
  border-radius: var(--jp-border-radius);
  border-width: 1px;
  padding: 6px 8px;
  background: none;
  color: var(--jp-ui-font-color0);
  height: inherit;
}

.jp-FormGroup-content fieldset input[type='checkbox'] {
  position: relative;
  top: 2px;
  margin-left: 0;
}

.jp-FormGroup-content button.jp-mod-styled {
  cursor: pointer;
}

.jp-FormGroup-content .checkbox label {
  cursor: pointer;
  font-size: var(--jp-content-font-size1);
}

.jp-FormGroup-content .jp-root > fieldset > legend {
  display: none;
}

.jp-FormGroup-content .jp-root > fieldset > p {
  display: none;
}

/** copy of `input.jp-mod-styled:focus` style */
.jp-FormGroup-content fieldset input:focus,
.jp-FormGroup-content fieldset select:focus {
  -moz-outline-radius: unset;
  outline: var(--jp-border-width) solid var(--md-blue-500);
  outline-offset: -1px;
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-FormGroup-content fieldset input:hover:not(:focus),
.jp-FormGroup-content fieldset select:hover:not(:focus) {
  background-color: var(--jp-border-color2);
}

/* stylelint-enable selector-max-type */

.jp-FormGroup-content .checkbox .field-description {
  /* Disable default description field for checkbox:
   because other widgets do not have description fields,
   we add descriptions to each widget on the field level.
  */
  display: none;
}

.jp-FormGroup-content #root__description {
  display: none;
}

.jp-FormGroup-content .jp-modifiedIndicator {
  width: 5px;
  background-color: var(--jp-brand-color2);
  margin-top: 0;
  margin-left: calc(var(--jp-private-settingeditor-modifier-indent) * -1);
  flex-shrink: 0;
}

.jp-FormGroup-content .jp-modifiedIndicator.jp-errorIndicator {
  background-color: var(--jp-error-color0);
  margin-right: 0.5em;
}

/* RJSF ARRAY style */

.jp-arrayFieldWrapper legend {
  font-size: var(--jp-content-font-size2);
  color: var(--jp-ui-font-color0);
  flex-basis: 100%;
  padding: 4px 0;
  font-weight: var(--jp-content-heading-font-weight);
  border-bottom: 1px solid var(--jp-border-color2);
}

.jp-arrayFieldWrapper .field-description {
  padding: 4px 0;
  white-space: pre-wrap;
}

.jp-arrayFieldWrapper .array-item {
  width: 100%;
  border: 1px solid var(--jp-border-color2);
  border-radius: 4px;
  margin: 4px;
}

.jp-ArrayOperations {
  display: flex;
  margin-left: 8px;
}

.jp-ArrayOperationsButton {
  margin: 2px;
}

.jp-ArrayOperationsButton .jp-icon3[fill] {
  fill: var(--jp-ui-font-color0);
}

button.jp-ArrayOperationsButton.jp-mod-styled:disabled {
  cursor: not-allowed;
  opacity: 0.5;
}

/* RJSF form validation error */

.jp-FormGroup-content .validationErrors {
  color: var(--jp-error-color0);
}

/* Hide panel level error as duplicated the field level error */
.jp-FormGroup-content .panel.errors {
  display: none;
}

/* RJSF normal content (settings-editor) */

.jp-FormGroup-contentNormal {
  display: flex;
  align-items: center;
  flex-wrap: wrap;
}

.jp-FormGroup-contentNormal .jp-FormGroup-contentItem {
  margin-left: 7px;
  color: var(--jp-ui-font-color0);
}

.jp-FormGroup-contentNormal .jp-FormGroup-description {
  flex-basis: 100%;
  padding: 4px 7px;
}

.jp-FormGroup-contentNormal .jp-FormGroup-default {
  flex-basis: 100%;
  padding: 4px 7px;
}

.jp-FormGroup-contentNormal .jp-FormGroup-fieldLabel {
  font-size: var(--jp-content-font-size1);
  font-weight: normal;
  min-width: 120px;
}

.jp-FormGroup-contentNormal fieldset:not(:first-child) {
  margin-left: 7px;
}

.jp-FormGroup-contentNormal .field-array-of-string .array-item {
  /* Display `jp-ArrayOperations` buttons side-by-side with content except
    for small screens where flex-wrap will place them one below the other.
  */
  display: flex;
  align-items: center;
  flex-wrap: wrap;
}

.jp-FormGroup-contentNormal .jp-objectFieldWrapper .form-group {
  padding: 2px 8px 2px var(--jp-private-settingeditor-modifier-indent);
  margin-top: 2px;
}

/* RJSF compact content (metadata-form) */

.jp-FormGroup-content.jp-FormGroup-contentCompact {
  width: 100%;
}

.jp-FormGroup-contentCompact .form-group {
  display: flex;
  padding: 0.5em 0.2em 0.5em 0;
}

.jp-FormGroup-contentCompact
  .jp-FormGroup-compactTitle
  .jp-FormGroup-description {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color2);
}

.jp-FormGroup-contentCompact .jp-FormGroup-fieldLabel {
  padding-bottom: 0.3em;
}

.jp-FormGroup-contentCompact .jp-inputFieldWrapper .form-control {
  width: 100%;
  box-sizing: border-box;
}

.jp-FormGroup-contentCompact .jp-arrayFieldWrapper .jp-FormGroup-compactTitle {
  padding-bottom: 7px;
}

.jp-FormGroup-contentCompact
  .jp-objectFieldWrapper
  .jp-objectFieldWrapper
  .form-group {
  padding: 2px 8px 2px var(--jp-private-settingeditor-modifier-indent);
  margin-top: 2px;
}

.jp-FormGroup-contentCompact ul.error-detail {
  margin-block-start: 0.5em;
  margin-block-end: 0.5em;
  padding-inline-start: 1em;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-SidePanel {
  display: flex;
  flex-direction: column;
  min-width: var(--jp-sidebar-min-width);
  overflow-y: auto;
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);
  font-size: var(--jp-ui-font-size1);
}

.jp-SidePanel-header {
  flex: 0 0 auto;
  display: flex;
  border-bottom: var(--jp-border-width) solid var(--jp-border-color2);
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  letter-spacing: 1px;
  margin: 0;
  padding: 2px;
  text-transform: uppercase;
}

.jp-SidePanel-toolbar {
  flex: 0 0 auto;
}

.jp-SidePanel-content {
  flex: 1 1 auto;
}

.jp-SidePanel-toolbar,
.jp-AccordionPanel-toolbar {
  height: var(--jp-private-toolbar-height);
}

.jp-SidePanel-toolbar.jp-Toolbar-micro {
  display: none;
}

.lm-AccordionPanel .jp-AccordionPanel-title {
  box-sizing: border-box;
  line-height: 25px;
  margin: 0;
  display: flex;
  align-items: center;
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  font-size: var(--jp-ui-font-size0);
}

.jp-AccordionPanel-title {
  cursor: pointer;
  user-select: none;
  -moz-user-select: none;
  -webkit-user-select: none;
  text-transform: uppercase;
}

.lm-AccordionPanel[data-orientation='horizontal'] > .jp-AccordionPanel-title {
  /* Title is rotated for horizontal accordion panel using CSS */
  display: block;
  transform-origin: top left;
  transform: rotate(-90deg) translate(-100%);
}

.jp-AccordionPanel-title .lm-AccordionPanel-titleLabel {
  user-select: none;
  text-overflow: ellipsis;
  white-space: nowrap;
  overflow: hidden;
}

.jp-AccordionPanel-title .lm-AccordionPanel-titleCollapser {
  transform: rotate(-90deg);
  margin: auto 0;
  height: 16px;
}

.jp-AccordionPanel-title.lm-mod-expanded .lm-AccordionPanel-titleCollapser {
  transform: rotate(0deg);
}

.lm-AccordionPanel .jp-AccordionPanel-toolbar {
  background: none;
  box-shadow: none;
  border: none;
  margin-left: auto;
}

.lm-AccordionPanel .lm-SplitPanel-handle:hover {
  background: var(--jp-layout-color3);
}

.jp-text-truncated {
  overflow: hidden;
  text-overflow: ellipsis;
  white-space: nowrap;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Spinner {
  position: absolute;
  display: flex;
  justify-content: center;
  align-items: center;
  z-index: 10;
  left: 0;
  top: 0;
  width: 100%;
  height: 100%;
  background: var(--jp-layout-color0);
  outline: none;
}

.jp-SpinnerContent {
  font-size: 10px;
  margin: 50px auto;
  text-indent: -9999em;
  width: 3em;
  height: 3em;
  border-radius: 50%;
  background: var(--jp-brand-color3);
  background: linear-gradient(
    to right,
    #f37626 10%,
    rgba(255, 255, 255, 0) 42%
  );
  position: relative;
  animation: load3 1s infinite linear, fadeIn 1s;
}

.jp-SpinnerContent::before {
  width: 50%;
  height: 50%;
  background: #f37626;
  border-radius: 100% 0 0;
  position: absolute;
  top: 0;
  left: 0;
  content: '';
}

.jp-SpinnerContent::after {
  background: var(--jp-layout-color0);
  width: 75%;
  height: 75%;
  border-radius: 50%;
  content: '';
  margin: auto;
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  right: 0;
}

@keyframes fadeIn {
  0% {
    opacity: 0;
  }

  100% {
    opacity: 1;
  }
}

@keyframes load3 {
  0% {
    transform: rotate(0deg);
  }

  100% {
    transform: rotate(360deg);
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

button.jp-mod-styled {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  border: none;
  box-sizing: border-box;
  text-align: center;
  line-height: 32px;
  height: 32px;
  padding: 0 12px;
  letter-spacing: 0.8px;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

input.jp-mod-styled {
  background: var(--jp-input-background);
  height: 28px;
  box-sizing: border-box;
  border: var(--jp-border-width) solid var(--jp-border-color1);
  padding-left: 7px;
  padding-right: 7px;
  font-size: var(--jp-ui-font-size2);
  color: var(--jp-ui-font-color0);
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

input[type='checkbox'].jp-mod-styled {
  appearance: checkbox;
  -webkit-appearance: checkbox;
  -moz-appearance: checkbox;
  height: auto;
}

input.jp-mod-styled:focus {
  border: var(--jp-border-width) solid var(--md-blue-500);
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-select-wrapper {
  display: flex;
  position: relative;
  flex-direction: column;
  padding: 1px;
  background-color: var(--jp-layout-color1);
  box-sizing: border-box;
  margin-bottom: 12px;
}

.jp-select-wrapper:not(.multiple) {
  height: 28px;
}

.jp-select-wrapper.jp-mod-focused select.jp-mod-styled {
  border: var(--jp-border-width) solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
  background-color: var(--jp-input-active-background);
}

select.jp-mod-styled:hover {
  cursor: pointer;
  color: var(--jp-ui-font-color0);
  background-color: var(--jp-input-hover-background);
  box-shadow: inset 0 0 1px rgba(0, 0, 0, 0.5);
}

select.jp-mod-styled {
  flex: 1 1 auto;
  width: 100%;
  font-size: var(--jp-ui-font-size2);
  background: var(--jp-input-background);
  color: var(--jp-ui-font-color0);
  padding: 0 25px 0 8px;
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  border-radius: 0;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

select.jp-mod-styled:not([multiple]) {
  height: 32px;
}

select.jp-mod-styled[multiple] {
  max-height: 200px;
  overflow-y: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-switch {
  display: flex;
  align-items: center;
  padding-left: 4px;
  padding-right: 4px;
  font-size: var(--jp-ui-font-size1);
  background-color: transparent;
  color: var(--jp-ui-font-color1);
  border: none;
  height: 20px;
}

.jp-switch:hover {
  background-color: var(--jp-layout-color2);
}

.jp-switch-label {
  margin-right: 5px;
  font-family: var(--jp-ui-font-family);
}

.jp-switch-track {
  cursor: pointer;
  background-color: var(--jp-switch-color, var(--jp-border-color1));
  -webkit-transition: 0.4s;
  transition: 0.4s;
  border-radius: 34px;
  height: 16px;
  width: 35px;
  position: relative;
}

.jp-switch-track::before {
  content: '';
  position: absolute;
  height: 10px;
  width: 10px;
  margin: 3px;
  left: 0;
  background-color: var(--jp-ui-inverse-font-color1);
  -webkit-transition: 0.4s;
  transition: 0.4s;
  border-radius: 50%;
}

.jp-switch[aria-checked='true'] .jp-switch-track {
  background-color: var(--jp-switch-true-position-color, var(--jp-warn-color0));
}

.jp-switch[aria-checked='true'] .jp-switch-track::before {
  /* track width (35) - margins (3 + 3) - thumb width (10) */
  left: 19px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

:root {
  --jp-private-toolbar-height: calc(
    28px + var(--jp-border-width)
  ); /* leave 28px for content */
}

.jp-Toolbar {
  color: var(--jp-ui-font-color1);
  flex: 0 0 auto;
  display: flex;
  flex-direction: row;
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  background: var(--jp-toolbar-background);
  min-height: var(--jp-toolbar-micro-height);
  padding: 2px;
  z-index: 8;
  overflow-x: hidden;
}

/* Toolbar items */

.jp-Toolbar > .jp-Toolbar-item.jp-Toolbar-spacer {
  flex-grow: 1;
  flex-shrink: 1;
}

.jp-Toolbar-item.jp-Toolbar-kernelStatus {
  display: inline-block;
  width: 32px;
  background-repeat: no-repeat;
  background-position: center;
  background-size: 16px;
}

.jp-Toolbar > .jp-Toolbar-item {
  flex: 0 0 auto;
  display: flex;
  padding-left: 1px;
  padding-right: 1px;
  font-size: var(--jp-ui-font-size1);
  line-height: var(--jp-private-toolbar-height);
  height: 100%;
}

/* Toolbar buttons */

/* This is the div we use to wrap the react component into a Widget */
div.jp-ToolbarButton {
  color: transparent;
  border: none;
  box-sizing: border-box;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
  padding: 0;
  margin: 0;
}

button.jp-ToolbarButtonComponent {
  background: var(--jp-layout-color1);
  border: none;
  box-sizing: border-box;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
  padding: 0 6px;
  margin: 0;
  height: 24px;
  border-radius: var(--jp-border-radius);
  display: flex;
  align-items: center;
  text-align: center;
  font-size: 14px;
  min-width: unset;
  min-height: unset;
}

button.jp-ToolbarButtonComponent:disabled {
  opacity: 0.4;
}

button.jp-ToolbarButtonComponent > span {
  padding: 0;
  flex: 0 0 auto;
}

button.jp-ToolbarButtonComponent .jp-ToolbarButtonComponent-label {
  font-size: var(--jp-ui-font-size1);
  line-height: 100%;
  padding-left: 2px;
  color: var(--jp-ui-font-color1);
  font-family: var(--jp-ui-font-family);
}

#jp-main-dock-panel[data-mode='single-document']
  .jp-MainAreaWidget
  > .jp-Toolbar.jp-Toolbar-micro {
  padding: 0;
  min-height: 0;
}

#jp-main-dock-panel[data-mode='single-document']
  .jp-MainAreaWidget
  > .jp-Toolbar {
  border: none;
  box-shadow: none;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-WindowedPanel-outer {
  position: relative;
  overflow-y: auto;
}

.jp-WindowedPanel-inner {
  position: relative;
}

.jp-WindowedPanel-window {
  position: absolute;
  left: 0;
  right: 0;
  overflow: visible;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* Sibling imports */

body {
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
}

/* Disable native link decoration styles everywhere outside of dialog boxes */
a {
  text-decoration: unset;
  color: unset;
}

a:hover {
  text-decoration: unset;
  color: unset;
}

/* Accessibility for links inside dialog box text */
.jp-Dialog-content a {
  text-decoration: revert;
  color: var(--jp-content-link-color);
}

.jp-Dialog-content a:hover {
  text-decoration: revert;
}

/* Styles for ui-components */
.jp-Button {
  color: var(--jp-ui-font-color2);
  border-radius: var(--jp-border-radius);
  padding: 0 12px;
  font-size: var(--jp-ui-font-size1);

  /* Copy from blueprint 3 */
  display: inline-flex;
  flex-direction: row;
  border: none;
  cursor: pointer;
  align-items: center;
  justify-content: center;
  text-align: left;
  vertical-align: middle;
  min-height: 30px;
  min-width: 30px;
}

.jp-Button:disabled {
  cursor: not-allowed;
}

.jp-Button:empty {
  padding: 0 !important;
}

.jp-Button.jp-mod-small {
  min-height: 24px;
  min-width: 24px;
  font-size: 12px;
  padding: 0 7px;
}

/* Use our own theme for hover styles */
.jp-Button.jp-mod-minimal:hover {
  background-color: var(--jp-layout-color2);
}

.jp-Button.jp-mod-minimal {
  background: none;
}

.jp-InputGroup {
  display: block;
  position: relative;
}

.jp-InputGroup input {
  box-sizing: border-box;
  border: none;
  border-radius: 0;
  background-color: transparent;
  color: var(--jp-ui-font-color0);
  box-shadow: inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color);
  padding-bottom: 0;
  padding-top: 0;
  padding-left: 10px;
  padding-right: 28px;
  position: relative;
  width: 100%;
  -webkit-appearance: none;
  -moz-appearance: none;
  appearance: none;
  font-size: 14px;
  font-weight: 400;
  height: 30px;
  line-height: 30px;
  outline: none;
  vertical-align: middle;
}

.jp-InputGroup input:focus {
  box-shadow: inset 0 0 0 var(--jp-border-width)
      var(--jp-input-active-box-shadow-color),
    inset 0 0 0 3px var(--jp-input-active-box-shadow-color);
}

.jp-InputGroup input:disabled {
  cursor: not-allowed;
  resize: block;
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color2);
}

.jp-InputGroup input:disabled ~ span {
  cursor: not-allowed;
  color: var(--jp-ui-font-color2);
}

.jp-InputGroup input::placeholder,
input::placeholder {
  color: var(--jp-ui-font-color2);
}

.jp-InputGroupAction {
  position: absolute;
  bottom: 1px;
  right: 0;
  padding: 6px;
}

.jp-HTMLSelect.jp-DefaultStyle select {
  background-color: initial;
  border: none;
  border-radius: 0;
  box-shadow: none;
  color: var(--jp-ui-font-color0);
  display: block;
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  height: 24px;
  line-height: 14px;
  padding: 0 25px 0 10px;
  text-align: left;
  -moz-appearance: none;
  -webkit-appearance: none;
}

.jp-HTMLSelect.jp-DefaultStyle select:disabled {
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color2);
  cursor: not-allowed;
  resize: block;
}

.jp-HTMLSelect.jp-DefaultStyle select:disabled ~ span {
  cursor: not-allowed;
}

/* Use our own theme for hover and option styles */
/* stylelint-disable-next-line selector-max-type */
.jp-HTMLSelect.jp-DefaultStyle select:hover,
.jp-HTMLSelect.jp-DefaultStyle select > option {
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color0);
}

select {
  box-sizing: border-box;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-StatusBar-Widget {
  display: flex;
  align-items: center;
  background: var(--jp-layout-color2);
  min-height: var(--jp-statusbar-height);
  justify-content: space-between;
  padding: 0 10px;
}

.jp-StatusBar-Left {
  display: flex;
  align-items: center;
  flex-direction: row;
}

.jp-StatusBar-Middle {
  display: flex;
  align-items: center;
}

.jp-StatusBar-Right {
  display: flex;
  align-items: center;
  flex-direction: row-reverse;
}

.jp-StatusBar-Item {
  max-height: var(--jp-statusbar-height);
  margin: 0 2px;
  height: var(--jp-statusbar-height);
  white-space: nowrap;
  text-overflow: ellipsis;
  color: var(--jp-ui-font-color1);
  padding: 0 6px;
}

.jp-mod-highlighted:hover {
  background-color: var(--jp-layout-color3);
}

.jp-mod-clicked {
  background-color: var(--jp-brand-color1);
}

.jp-mod-clicked:hover {
  background-color: var(--jp-brand-color0);
}

.jp-mod-clicked .jp-StatusBar-TextItem {
  color: var(--jp-ui-inverse-font-color1);
}

.jp-StatusBar-HoverItem {
  box-shadow: '0px 4px 4px rgba(0, 0, 0, 0.25)';
}

.jp-StatusBar-TextItem {
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  line-height: 24px;
  color: var(--jp-ui-font-color1);
}

.jp-StatusBar-GroupItem {
  display: flex;
  align-items: center;
  flex-direction: row;
}

.jp-Statusbar-ProgressCircle svg {
  display: block;
  margin: 0 auto;
  width: 16px;
  height: 24px;
  align-self: normal;
}

.jp-Statusbar-ProgressCircle path {
  fill: var(--jp-inverse-layout-color3);
}

.jp-Statusbar-ProgressBar-progress-bar {
  height: 10px;
  width: 100px;
  border: solid 0.25px var(--jp-brand-color2);
  border-radius: 3px;
  overflow: hidden;
  align-self: center;
}

.jp-Statusbar-ProgressBar-progress-bar > div {
  background-color: var(--jp-brand-color2);
  background-image: linear-gradient(
    -45deg,
    rgba(255, 255, 255, 0.2) 25%,
    transparent 25%,
    transparent 50%,
    rgba(255, 255, 255, 0.2) 50%,
    rgba(255, 255, 255, 0.2) 75%,
    transparent 75%,
    transparent
  );
  background-size: 40px 40px;
  float: left;
  width: 0%;
  height: 100%;
  font-size: 12px;
  line-height: 14px;
  color: #fff;
  text-align: center;
  animation: jp-Statusbar-ExecutionTime-progress-bar 2s linear infinite;
}

.jp-Statusbar-ProgressBar-progress-bar p {
  color: var(--jp-ui-font-color1);
  font-family: var(--jp-ui-font-family);
  font-size: var(--jp-ui-font-size1);
  line-height: 10px;
  width: 100px;
}

@keyframes jp-Statusbar-ExecutionTime-progress-bar {
  0% {
    background-position: 0 0;
  }

  100% {
    background-position: 40px 40px;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-commandpalette-search-height: 28px;
}

/*-----------------------------------------------------------------------------
| Overall styles
|----------------------------------------------------------------------------*/

.lm-CommandPalette {
  padding-bottom: 0;
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);

  /* This is needed so that all font sizing of children done in ems is
   * relative to this base size */
  font-size: var(--jp-ui-font-size1);
}

/*-----------------------------------------------------------------------------
| Modal variant
|----------------------------------------------------------------------------*/

.jp-ModalCommandPalette {
  position: absolute;
  z-index: 10000;
  top: 38px;
  left: 30%;
  margin: 0;
  padding: 4px;
  width: 40%;
  box-shadow: var(--jp-elevation-z4);
  border-radius: 4px;
  background: var(--jp-layout-color0);
}

.jp-ModalCommandPalette .lm-CommandPalette {
  max-height: 40vh;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-close-icon::after {
  display: none;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-CommandPalette-header {
  display: none;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-CommandPalette-item {
  margin-left: 4px;
  margin-right: 4px;
}

.jp-ModalCommandPalette
  .lm-CommandPalette
  .lm-CommandPalette-item.lm-mod-disabled {
  display: none;
}

/*-----------------------------------------------------------------------------
| Search
|----------------------------------------------------------------------------*/

.lm-CommandPalette-search {
  padding: 4px;
  background-color: var(--jp-layout-color1);
  z-index: 2;
}

.lm-CommandPalette-wrapper {
  overflow: overlay;
  padding: 0 9px;
  background-color: var(--jp-input-active-background);
  height: 30px;
  box-shadow: inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color);
}

.lm-CommandPalette.lm-mod-focused .lm-CommandPalette-wrapper {
  box-shadow: inset 0 0 0 1px var(--jp-input-active-box-shadow-color),
    inset 0 0 0 3px var(--jp-input-active-box-shadow-color);
}

.jp-SearchIconGroup {
  color: white;
  background-color: var(--jp-brand-color1);
  position: absolute;
  top: 4px;
  right: 4px;
  padding: 5px 5px 1px;
}

.jp-SearchIconGroup svg {
  height: 20px;
  width: 20px;
}

.jp-SearchIconGroup .jp-icon3[fill] {
  fill: var(--jp-layout-color0);
}

.lm-CommandPalette-input {
  background: transparent;
  width: calc(100% - 18px);
  float: left;
  border: none;
  outline: none;
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  line-height: var(--jp-private-commandpalette-search-height);
}

.lm-CommandPalette-input::-webkit-input-placeholder,
.lm-CommandPalette-input::-moz-placeholder,
.lm-CommandPalette-input:-ms-input-placeholder {
  color: var(--jp-ui-font-color2);
  font-size: var(--jp-ui-font-size1);
}

/*-----------------------------------------------------------------------------
| Results
|----------------------------------------------------------------------------*/

.lm-CommandPalette-header:first-child {
  margin-top: 0;
}

.lm-CommandPalette-header {
  border-bottom: solid var(--jp-border-width) var(--jp-border-color2);
  color: var(--jp-ui-font-color1);
  cursor: pointer;
  display: flex;
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  letter-spacing: 1px;
  margin-top: 8px;
  padding: 8px 0 8px 12px;
  text-transform: uppercase;
}

.lm-CommandPalette-header.lm-mod-active {
  background: var(--jp-layout-color2);
}

.lm-CommandPalette-header > mark {
  background-color: transparent;
  font-weight: bold;
  color: var(--jp-ui-font-color1);
}

.lm-CommandPalette-item {
  padding: 4px 12px 4px 4px;
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  font-weight: 400;
  display: flex;
}

.lm-CommandPalette-item.lm-mod-disabled {
  color: var(--jp-ui-font-color2);
}

.lm-CommandPalette-item.lm-mod-active {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.lm-CommandPalette-item.lm-mod-active .lm-CommandPalette-itemLabel > mark {
  color: var(--jp-ui-inverse-font-color0);
}

.lm-CommandPalette-item.lm-mod-active .jp-icon-selectable[fill] {
  fill: var(--jp-layout-color0);
}

.lm-CommandPalette-item.lm-mod-active:hover:not(.lm-mod-disabled) {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.lm-CommandPalette-item:hover:not(.lm-mod-active):not(.lm-mod-disabled) {
  background: var(--jp-layout-color2);
}

.lm-CommandPalette-itemContent {
  overflow: hidden;
}

.lm-CommandPalette-itemLabel > mark {
  color: var(--jp-ui-font-color0);
  background-color: transparent;
  font-weight: bold;
}

.lm-CommandPalette-item.lm-mod-disabled mark {
  color: var(--jp-ui-font-color2);
}

.lm-CommandPalette-item .lm-CommandPalette-itemIcon {
  margin: 0 4px 0 0;
  position: relative;
  width: 16px;
  top: 2px;
  flex: 0 0 auto;
}

.lm-CommandPalette-item.lm-mod-disabled .lm-CommandPalette-itemIcon {
  opacity: 0.6;
}

.lm-CommandPalette-item .lm-CommandPalette-itemShortcut {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemCaption {
  display: none;
}

.lm-CommandPalette-content {
  background-color: var(--jp-layout-color1);
}

.lm-CommandPalette-content:empty::after {
  content: 'No results';
  margin: auto;
  margin-top: 20px;
  width: 100px;
  display: block;
  font-size: var(--jp-ui-font-size2);
  font-family: var(--jp-ui-font-family);
  font-weight: lighter;
}

.lm-CommandPalette-emptyMessage {
  text-align: center;
  margin-top: 24px;
  line-height: 1.32;
  padding: 0 8px;
  color: var(--jp-content-font-color3);
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Dialog {
  position: absolute;
  z-index: 10000;
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  top: 0;
  left: 0;
  margin: 0;
  padding: 0;
  width: 100%;
  height: 100%;
  background: var(--jp-dialog-background);
}

.jp-Dialog-content {
  display: flex;
  flex-direction: column;
  margin-left: auto;
  margin-right: auto;
  background: var(--jp-layout-color1);
  padding: 24px 24px 12px;
  min-width: 300px;
  min-height: 150px;
  max-width: 1000px;
  max-height: 500px;
  box-sizing: border-box;
  box-shadow: var(--jp-elevation-z20);
  word-wrap: break-word;
  border-radius: var(--jp-border-radius);

  /* This is needed so that all font sizing of children done in ems is
   * relative to this base size */
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color1);
  resize: both;
}

.jp-Dialog-content.jp-Dialog-content-small {
  max-width: 500px;
}

.jp-Dialog-button {
  overflow: visible;
}

button.jp-Dialog-button:focus {
  outline: 1px solid var(--jp-brand-color1);
  outline-offset: 4px;
  -moz-outline-radius: 0;
}

button.jp-Dialog-button:focus::-moz-focus-inner {
  border: 0;
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-accept:focus,
button.jp-Dialog-button.jp-mod-styled.jp-mod-warn:focus,
button.jp-Dialog-button.jp-mod-styled.jp-mod-reject:focus {
  outline-offset: 4px;
  -moz-outline-radius: 0;
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-accept:focus {
  outline: 1px solid var(--jp-accept-color-normal, var(--jp-brand-color1));
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-warn:focus {
  outline: 1px solid var(--jp-warn-color-normal, var(--jp-error-color1));
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-reject:focus {
  outline: 1px solid var(--jp-reject-color-normal, var(--md-grey-600));
}

button.jp-Dialog-close-button {
  padding: 0;
  height: 100%;
  min-width: unset;
  min-height: unset;
}

.jp-Dialog-header {
  display: flex;
  justify-content: space-between;
  flex: 0 0 auto;
  padding-bottom: 12px;
  font-size: var(--jp-ui-font-size3);
  font-weight: 400;
  color: var(--jp-ui-font-color1);
}

.jp-Dialog-body {
  display: flex;
  flex-direction: column;
  flex: 1 1 auto;
  font-size: var(--jp-ui-font-size1);
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  overflow: auto;
}

.jp-Dialog-footer {
  display: flex;
  flex-direction: row;
  justify-content: flex-end;
  align-items: center;
  flex: 0 0 auto;
  margin-left: -12px;
  margin-right: -12px;
  padding: 12px;
}

.jp-Dialog-checkbox {
  padding-right: 5px;
}

.jp-Dialog-checkbox > input:focus-visible {
  outline: 1px solid var(--jp-input-active-border-color);
  outline-offset: 1px;
}

.jp-Dialog-spacer {
  flex: 1 1 auto;
}

.jp-Dialog-title {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.jp-Dialog-body > .jp-select-wrapper {
  width: 100%;
}

.jp-Dialog-body > button {
  padding: 0 16px;
}

.jp-Dialog-body > label {
  line-height: 1.4;
  color: var(--jp-ui-font-color0);
}

.jp-Dialog-button.jp-mod-styled:not(:last-child) {
  margin-right: 12px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-Input-Boolean-Dialog {
  flex-direction: row-reverse;
  align-items: end;
  width: 100%;
}

.jp-Input-Boolean-Dialog > label {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-MainAreaWidget > :focus {
  outline: none;
}

.jp-MainAreaWidget .jp-MainAreaWidget-error {
  padding: 6px;
}

.jp-MainAreaWidget .jp-MainAreaWidget-error > pre {
  width: auto;
  padding: 10px;
  background: var(--jp-error-color3);
  border: var(--jp-border-width) solid var(--jp-error-color1);
  border-radius: var(--jp-border-radius);
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  white-space: pre-wrap;
  word-wrap: break-word;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/**
 * google-material-color v1.2.6
 * https://github.com/danlevan/google-material-color
 */
:root {
  --md-red-50: #ffebee;
  --md-red-100: #ffcdd2;
  --md-red-200: #ef9a9a;
  --md-red-300: #e57373;
  --md-red-400: #ef5350;
  --md-red-500: #f44336;
  --md-red-600: #e53935;
  --md-red-700: #d32f2f;
  --md-red-800: #c62828;
  --md-red-900: #b71c1c;
  --md-red-A100: #ff8a80;
  --md-red-A200: #ff5252;
  --md-red-A400: #ff1744;
  --md-red-A700: #d50000;
  --md-pink-50: #fce4ec;
  --md-pink-100: #f8bbd0;
  --md-pink-200: #f48fb1;
  --md-pink-300: #f06292;
  --md-pink-400: #ec407a;
  --md-pink-500: #e91e63;
  --md-pink-600: #d81b60;
  --md-pink-700: #c2185b;
  --md-pink-800: #ad1457;
  --md-pink-900: #880e4f;
  --md-pink-A100: #ff80ab;
  --md-pink-A200: #ff4081;
  --md-pink-A400: #f50057;
  --md-pink-A700: #c51162;
  --md-purple-50: #f3e5f5;
  --md-purple-100: #e1bee7;
  --md-purple-200: #ce93d8;
  --md-purple-300: #ba68c8;
  --md-purple-400: #ab47bc;
  --md-purple-500: #9c27b0;
  --md-purple-600: #8e24aa;
  --md-purple-700: #7b1fa2;
  --md-purple-800: #6a1b9a;
  --md-purple-900: #4a148c;
  --md-purple-A100: #ea80fc;
  --md-purple-A200: #e040fb;
  --md-purple-A400: #d500f9;
  --md-purple-A700: #a0f;
  --md-deep-purple-50: #ede7f6;
  --md-deep-purple-100: #d1c4e9;
  --md-deep-purple-200: #b39ddb;
  --md-deep-purple-300: #9575cd;
  --md-deep-purple-400: #7e57c2;
  --md-deep-purple-500: #673ab7;
  --md-deep-purple-600: #5e35b1;
  --md-deep-purple-700: #512da8;
  --md-deep-purple-800: #4527a0;
  --md-deep-purple-900: #311b92;
  --md-deep-purple-A100: #b388ff;
  --md-deep-purple-A200: #7c4dff;
  --md-deep-purple-A400: #651fff;
  --md-deep-purple-A700: #6200ea;
  --md-indigo-50: #e8eaf6;
  --md-indigo-100: #c5cae9;
  --md-indigo-200: #9fa8da;
  --md-indigo-300: #7986cb;
  --md-indigo-400: #5c6bc0;
  --md-indigo-500: #3f51b5;
  --md-indigo-600: #3949ab;
  --md-indigo-700: #303f9f;
  --md-indigo-800: #283593;
  --md-indigo-900: #1a237e;
  --md-indigo-A100: #8c9eff;
  --md-indigo-A200: #536dfe;
  --md-indigo-A400: #3d5afe;
  --md-indigo-A700: #304ffe;
  --md-blue-50: #e3f2fd;
  --md-blue-100: #bbdefb;
  --md-blue-200: #90caf9;
  --md-blue-300: #64b5f6;
  --md-blue-400: #42a5f5;
  --md-blue-500: #2196f3;
  --md-blue-600: #1e88e5;
  --md-blue-700: #1976d2;
  --md-blue-800: #1565c0;
  --md-blue-900: #0d47a1;
  --md-blue-A100: #82b1ff;
  --md-blue-A200: #448aff;
  --md-blue-A400: #2979ff;
  --md-blue-A700: #2962ff;
  --md-light-blue-50: #e1f5fe;
  --md-light-blue-100: #b3e5fc;
  --md-light-blue-200: #81d4fa;
  --md-light-blue-300: #4fc3f7;
  --md-light-blue-400: #29b6f6;
  --md-light-blue-500: #03a9f4;
  --md-light-blue-600: #039be5;
  --md-light-blue-700: #0288d1;
  --md-light-blue-800: #0277bd;
  --md-light-blue-900: #01579b;
  --md-light-blue-A100: #80d8ff;
  --md-light-blue-A200: #40c4ff;
  --md-light-blue-A400: #00b0ff;
  --md-light-blue-A700: #0091ea;
  --md-cyan-50: #e0f7fa;
  --md-cyan-100: #b2ebf2;
  --md-cyan-200: #80deea;
  --md-cyan-300: #4dd0e1;
  --md-cyan-400: #26c6da;
  --md-cyan-500: #00bcd4;
  --md-cyan-600: #00acc1;
  --md-cyan-700: #0097a7;
  --md-cyan-800: #00838f;
  --md-cyan-900: #006064;
  --md-cyan-A100: #84ffff;
  --md-cyan-A200: #18ffff;
  --md-cyan-A400: #00e5ff;
  --md-cyan-A700: #00b8d4;
  --md-teal-50: #e0f2f1;
  --md-teal-100: #b2dfdb;
  --md-teal-200: #80cbc4;
  --md-teal-300: #4db6ac;
  --md-teal-400: #26a69a;
  --md-teal-500: #009688;
  --md-teal-600: #00897b;
  --md-teal-700: #00796b;
  --md-teal-800: #00695c;
  --md-teal-900: #004d40;
  --md-teal-A100: #a7ffeb;
  --md-teal-A200: #64ffda;
  --md-teal-A400: #1de9b6;
  --md-teal-A700: #00bfa5;
  --md-green-50: #e8f5e9;
  --md-green-100: #c8e6c9;
  --md-green-200: #a5d6a7;
  --md-green-300: #81c784;
  --md-green-400: #66bb6a;
  --md-green-500: #4caf50;
  --md-green-600: #43a047;
  --md-green-700: #388e3c;
  --md-green-800: #2e7d32;
  --md-green-900: #1b5e20;
  --md-green-A100: #b9f6ca;
  --md-green-A200: #69f0ae;
  --md-green-A400: #00e676;
  --md-green-A700: #00c853;
  --md-light-green-50: #f1f8e9;
  --md-light-green-100: #dcedc8;
  --md-light-green-200: #c5e1a5;
  --md-light-green-300: #aed581;
  --md-light-green-400: #9ccc65;
  --md-light-green-500: #8bc34a;
  --md-light-green-600: #7cb342;
  --md-light-green-700: #689f38;
  --md-light-green-800: #558b2f;
  --md-light-green-900: #33691e;
  --md-light-green-A100: #ccff90;
  --md-light-green-A200: #b2ff59;
  --md-light-green-A400: #76ff03;
  --md-light-green-A700: #64dd17;
  --md-lime-50: #f9fbe7;
  --md-lime-100: #f0f4c3;
  --md-lime-200: #e6ee9c;
  --md-lime-300: #dce775;
  --md-lime-400: #d4e157;
  --md-lime-500: #cddc39;
  --md-lime-600: #c0ca33;
  --md-lime-700: #afb42b;
  --md-lime-800: #9e9d24;
  --md-lime-900: #827717;
  --md-lime-A100: #f4ff81;
  --md-lime-A200: #eeff41;
  --md-lime-A400: #c6ff00;
  --md-lime-A700: #aeea00;
  --md-yellow-50: #fffde7;
  --md-yellow-100: #fff9c4;
  --md-yellow-200: #fff59d;
  --md-yellow-300: #fff176;
  --md-yellow-400: #ffee58;
  --md-yellow-500: #ffeb3b;
  --md-yellow-600: #fdd835;
  --md-yellow-700: #fbc02d;
  --md-yellow-800: #f9a825;
  --md-yellow-900: #f57f17;
  --md-yellow-A100: #ffff8d;
  --md-yellow-A200: #ff0;
  --md-yellow-A400: #ffea00;
  --md-yellow-A700: #ffd600;
  --md-amber-50: #fff8e1;
  --md-amber-100: #ffecb3;
  --md-amber-200: #ffe082;
  --md-amber-300: #ffd54f;
  --md-amber-400: #ffca28;
  --md-amber-500: #ffc107;
  --md-amber-600: #ffb300;
  --md-amber-700: #ffa000;
  --md-amber-800: #ff8f00;
  --md-amber-900: #ff6f00;
  --md-amber-A100: #ffe57f;
  --md-amber-A200: #ffd740;
  --md-amber-A400: #ffc400;
  --md-amber-A700: #ffab00;
  --md-orange-50: #fff3e0;
  --md-orange-100: #ffe0b2;
  --md-orange-200: #ffcc80;
  --md-orange-300: #ffb74d;
  --md-orange-400: #ffa726;
  --md-orange-500: #ff9800;
  --md-orange-600: #fb8c00;
  --md-orange-700: #f57c00;
  --md-orange-800: #ef6c00;
  --md-orange-900: #e65100;
  --md-orange-A100: #ffd180;
  --md-orange-A200: #ffab40;
  --md-orange-A400: #ff9100;
  --md-orange-A700: #ff6d00;
  --md-deep-orange-50: #fbe9e7;
  --md-deep-orange-100: #ffccbc;
  --md-deep-orange-200: #ffab91;
  --md-deep-orange-300: #ff8a65;
  --md-deep-orange-400: #ff7043;
  --md-deep-orange-500: #ff5722;
  --md-deep-orange-600: #f4511e;
  --md-deep-orange-700: #e64a19;
  --md-deep-orange-800: #d84315;
  --md-deep-orange-900: #bf360c;
  --md-deep-orange-A100: #ff9e80;
  --md-deep-orange-A200: #ff6e40;
  --md-deep-orange-A400: #ff3d00;
  --md-deep-orange-A700: #dd2c00;
  --md-brown-50: #efebe9;
  --md-brown-100: #d7ccc8;
  --md-brown-200: #bcaaa4;
  --md-brown-300: #a1887f;
  --md-brown-400: #8d6e63;
  --md-brown-500: #795548;
  --md-brown-600: #6d4c41;
  --md-brown-700: #5d4037;
  --md-brown-800: #4e342e;
  --md-brown-900: #3e2723;
  --md-grey-50: #fafafa;
  --md-grey-100: #f5f5f5;
  --md-grey-200: #eee;
  --md-grey-300: #e0e0e0;
  --md-grey-400: #bdbdbd;
  --md-grey-500: #9e9e9e;
  --md-grey-600: #757575;
  --md-grey-700: #616161;
  --md-grey-800: #424242;
  --md-grey-900: #212121;
  --md-blue-grey-50: #eceff1;
  --md-blue-grey-100: #cfd8dc;
  --md-blue-grey-200: #b0bec5;
  --md-blue-grey-300: #90a4ae;
  --md-blue-grey-400: #78909c;
  --md-blue-grey-500: #607d8b;
  --md-blue-grey-600: #546e7a;
  --md-blue-grey-700: #455a64;
  --md-blue-grey-800: #37474f;
  --md-blue-grey-900: #263238;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| RenderedText
|----------------------------------------------------------------------------*/

:root {
  /* This is the padding value to fill the gaps between lines containing spans with background color. */
  --jp-private-code-span-padding: calc(
    (var(--jp-code-line-height) - 1) * var(--jp-code-font-size) / 2
  );
}

.jp-RenderedText {
  text-align: left;
  padding-left: var(--jp-code-padding);
  line-height: var(--jp-code-line-height);
  font-family: var(--jp-code-font-family);
}

.jp-RenderedText pre,
.jp-RenderedJavaScript pre,
.jp-RenderedHTMLCommon pre {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-code-font-size);
  border: none;
  margin: 0;
  padding: 0;
}

.jp-RenderedText pre a:link {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

.jp-RenderedText pre a:hover {
  text-decoration: underline;
  color: var(--jp-content-link-color);
}

.jp-RenderedText pre a:visited {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

/* console foregrounds and backgrounds */
.jp-RenderedText pre .ansi-black-fg {
  color: #3e424d;
}

.jp-RenderedText pre .ansi-red-fg {
  color: #e75c58;
}

.jp-RenderedText pre .ansi-green-fg {
  color: #00a250;
}

.jp-RenderedText pre .ansi-yellow-fg {
  color: #ddb62b;
}

.jp-RenderedText pre .ansi-blue-fg {
  color: #208ffb;
}

.jp-RenderedText pre .ansi-magenta-fg {
  color: #d160c4;
}

.jp-RenderedText pre .ansi-cyan-fg {
  color: #60c6c8;
}

.jp-RenderedText pre .ansi-white-fg {
  color: #c5c1b4;
}

.jp-RenderedText pre .ansi-black-bg {
  background-color: #3e424d;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-red-bg {
  background-color: #e75c58;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-green-bg {
  background-color: #00a250;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-yellow-bg {
  background-color: #ddb62b;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-blue-bg {
  background-color: #208ffb;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-magenta-bg {
  background-color: #d160c4;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-cyan-bg {
  background-color: #60c6c8;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-white-bg {
  background-color: #c5c1b4;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-black-intense-fg {
  color: #282c36;
}

.jp-RenderedText pre .ansi-red-intense-fg {
  color: #b22b31;
}

.jp-RenderedText pre .ansi-green-intense-fg {
  color: #007427;
}

.jp-RenderedText pre .ansi-yellow-intense-fg {
  color: #b27d12;
}

.jp-RenderedText pre .ansi-blue-intense-fg {
  color: #0065ca;
}

.jp-RenderedText pre .ansi-magenta-intense-fg {
  color: #a03196;
}

.jp-RenderedText pre .ansi-cyan-intense-fg {
  color: #258f8f;
}

.jp-RenderedText pre .ansi-white-intense-fg {
  color: #a1a6b2;
}

.jp-RenderedText pre .ansi-black-intense-bg {
  background-color: #282c36;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-red-intense-bg {
  background-color: #b22b31;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-green-intense-bg {
  background-color: #007427;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-yellow-intense-bg {
  background-color: #b27d12;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-blue-intense-bg {
  background-color: #0065ca;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-magenta-intense-bg {
  background-color: #a03196;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-cyan-intense-bg {
  background-color: #258f8f;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-white-intense-bg {
  background-color: #a1a6b2;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-default-inverse-fg {
  color: var(--jp-ui-inverse-font-color0);
}

.jp-RenderedText pre .ansi-default-inverse-bg {
  background-color: var(--jp-inverse-layout-color0);
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-bold {
  font-weight: bold;
}

.jp-RenderedText pre .ansi-underline {
  text-decoration: underline;
}

.jp-RenderedText[data-mime-type='application/vnd.jupyter.stderr'] {
  background: var(--jp-rendermime-error-background);
  padding-top: var(--jp-code-padding);
}

/*-----------------------------------------------------------------------------
| RenderedLatex
|----------------------------------------------------------------------------*/

.jp-RenderedLatex {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-content-font-size1);
  line-height: var(--jp-content-line-height);
}

/* Left-justify outputs.*/
.jp-OutputArea-output.jp-RenderedLatex {
  padding: var(--jp-code-padding);
  text-align: left;
}

/*-----------------------------------------------------------------------------
| RenderedHTML
|----------------------------------------------------------------------------*/

.jp-RenderedHTMLCommon {
  color: var(--jp-content-font-color1);
  font-family: var(--jp-content-font-family);
  font-size: var(--jp-content-font-size1);
  line-height: var(--jp-content-line-height);

  /* Give a bit more R padding on Markdown text to keep line lengths reasonable */
  padding-right: 20px;
}

.jp-RenderedHTMLCommon em {
  font-style: italic;
}

.jp-RenderedHTMLCommon strong {
  font-weight: bold;
}

.jp-RenderedHTMLCommon u {
  text-decoration: underline;
}

.jp-RenderedHTMLCommon a:link {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

.jp-RenderedHTMLCommon a:hover {
  text-decoration: underline;
  color: var(--jp-content-link-color);
}

.jp-RenderedHTMLCommon a:visited {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

/* Headings */

.jp-RenderedHTMLCommon h1,
.jp-RenderedHTMLCommon h2,
.jp-RenderedHTMLCommon h3,
.jp-RenderedHTMLCommon h4,
.jp-RenderedHTMLCommon h5,
.jp-RenderedHTMLCommon h6 {
  line-height: var(--jp-content-heading-line-height);
  font-weight: var(--jp-content-heading-font-weight);
  font-style: normal;
  margin: var(--jp-content-heading-margin-top) 0
    var(--jp-content-heading-margin-bottom) 0;
}

.jp-RenderedHTMLCommon h1:first-child,
.jp-RenderedHTMLCommon h2:first-child,
.jp-RenderedHTMLCommon h3:first-child,
.jp-RenderedHTMLCommon h4:first-child,
.jp-RenderedHTMLCommon h5:first-child,
.jp-RenderedHTMLCommon h6:first-child {
  margin-top: calc(0.5 * var(--jp-content-heading-margin-top));
}

.jp-RenderedHTMLCommon h1:last-child,
.jp-RenderedHTMLCommon h2:last-child,
.jp-RenderedHTMLCommon h3:last-child,
.jp-RenderedHTMLCommon h4:last-child,
.jp-RenderedHTMLCommon h5:last-child,
.jp-RenderedHTMLCommon h6:last-child {
  margin-bottom: calc(0.5 * var(--jp-content-heading-margin-bottom));
}

.jp-RenderedHTMLCommon h1 {
  font-size: var(--jp-content-font-size5);
}

.jp-RenderedHTMLCommon h2 {
  font-size: var(--jp-content-font-size4);
}

.jp-RenderedHTMLCommon h3 {
  font-size: var(--jp-content-font-size3);
}

.jp-RenderedHTMLCommon h4 {
  font-size: var(--jp-content-font-size2);
}

.jp-RenderedHTMLCommon h5 {
  font-size: var(--jp-content-font-size1);
}

.jp-RenderedHTMLCommon h6 {
  font-size: var(--jp-content-font-size0);
}

/* Lists */

/* stylelint-disable selector-max-type, selector-max-compound-selectors */

.jp-RenderedHTMLCommon ul:not(.list-inline),
.jp-RenderedHTMLCommon ol:not(.list-inline) {
  padding-left: 2em;
}

.jp-RenderedHTMLCommon ul {
  list-style: disc;
}

.jp-RenderedHTMLCommon ul ul {
  list-style: square;
}

.jp-RenderedHTMLCommon ul ul ul {
  list-style: circle;
}

.jp-RenderedHTMLCommon ol {
  list-style: decimal;
}

.jp-RenderedHTMLCommon ol ol {
  list-style: upper-alpha;
}

.jp-RenderedHTMLCommon ol ol ol {
  list-style: lower-alpha;
}

.jp-RenderedHTMLCommon ol ol ol ol {
  list-style: lower-roman;
}

.jp-RenderedHTMLCommon ol ol ol ol ol {
  list-style: decimal;
}

.jp-RenderedHTMLCommon ol,
.jp-RenderedHTMLCommon ul {
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon ul ul,
.jp-RenderedHTMLCommon ul ol,
.jp-RenderedHTMLCommon ol ul,
.jp-RenderedHTMLCommon ol ol {
  margin-bottom: 0;
}

/* stylelint-enable selector-max-type, selector-max-compound-selectors */

.jp-RenderedHTMLCommon hr {
  color: var(--jp-border-color2);
  background-color: var(--jp-border-color1);
  margin-top: 1em;
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon > pre {
  margin: 1.5em 2em;
}

.jp-RenderedHTMLCommon pre,
.jp-RenderedHTMLCommon code {
  border: 0;
  background-color: var(--jp-layout-color0);
  color: var(--jp-content-font-color1);
  font-family: var(--jp-code-font-family);
  font-size: inherit;
  line-height: var(--jp-code-line-height);
  padding: 0;
  white-space: pre-wrap;
}

.jp-RenderedHTMLCommon :not(pre) > code {
  background-color: var(--jp-layout-color2);
  padding: 1px 5px;
}

/* Tables */

.jp-RenderedHTMLCommon table {
  border-collapse: collapse;
  border-spacing: 0;
  border: none;
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  table-layout: fixed;
  margin-left: auto;
  margin-bottom: 1em;
  margin-right: auto;
}

.jp-RenderedHTMLCommon thead {
  border-bottom: var(--jp-border-width) solid var(--jp-border-color1);
  vertical-align: bottom;
}

.jp-RenderedHTMLCommon td,
.jp-RenderedHTMLCommon th,
.jp-RenderedHTMLCommon tr {
  vertical-align: middle;
  padding: 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}

.jp-RenderedMarkdown.jp-RenderedHTMLCommon td,
.jp-RenderedMarkdown.jp-RenderedHTMLCommon th {
  max-width: none;
}

:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon td,
:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon th,
:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon tr {
  text-align: right;
}

.jp-RenderedHTMLCommon th {
  font-weight: bold;
}

.jp-RenderedHTMLCommon tbody tr:nth-child(odd) {
  background: var(--jp-layout-color0);
}

.jp-RenderedHTMLCommon tbody tr:nth-child(even) {
  background: var(--jp-rendermime-table-row-background);
}

.jp-RenderedHTMLCommon tbody tr:hover {
  background: var(--jp-rendermime-table-row-hover-background);
}

.jp-RenderedHTMLCommon p {
  text-align: left;
  margin: 0;
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon img {
  -moz-force-broken-image-icon: 1;
}

/* Restrict to direct children as other images could be nested in other content. */
.jp-RenderedHTMLCommon > img {
  display: block;
  margin-left: 0;
  margin-right: 0;
  margin-bottom: 1em;
}

/* Change color behind transparent images if they need it... */
[data-jp-theme-light='false'] .jp-RenderedImage img.jp-needs-light-background {
  background-color: var(--jp-inverse-layout-color1);
}

[data-jp-theme-light='true'] .jp-RenderedImage img.jp-needs-dark-background {
  background-color: var(--jp-inverse-layout-color1);
}

.jp-RenderedHTMLCommon img,
.jp-RenderedImage img,
.jp-RenderedHTMLCommon svg,
.jp-RenderedSVG svg {
  max-width: 100%;
  height: auto;
}

.jp-RenderedHTMLCommon img.jp-mod-unconfined,
.jp-RenderedImage img.jp-mod-unconfined,
.jp-RenderedHTMLCommon svg.jp-mod-unconfined,
.jp-RenderedSVG svg.jp-mod-unconfined {
  max-width: none;
}

.jp-RenderedHTMLCommon .alert {
  padding: var(--jp-notebook-padding);
  border: var(--jp-border-width) solid transparent;
  border-radius: var(--jp-border-radius);
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon .alert-info {
  color: var(--jp-info-color0);
  background-color: var(--jp-info-color3);
  border-color: var(--jp-info-color2);
}

.jp-RenderedHTMLCommon .alert-info hr {
  border-color: var(--jp-info-color3);
}

.jp-RenderedHTMLCommon .alert-info > p:last-child,
.jp-RenderedHTMLCommon .alert-info > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-warning {
  color: var(--jp-warn-color0);
  background-color: var(--jp-warn-color3);
  border-color: var(--jp-warn-color2);
}

.jp-RenderedHTMLCommon .alert-warning hr {
  border-color: var(--jp-warn-color3);
}

.jp-RenderedHTMLCommon .alert-warning > p:last-child,
.jp-RenderedHTMLCommon .alert-warning > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-success {
  color: var(--jp-success-color0);
  background-color: var(--jp-success-color3);
  border-color: var(--jp-success-color2);
}

.jp-RenderedHTMLCommon .alert-success hr {
  border-color: var(--jp-success-color3);
}

.jp-RenderedHTMLCommon .alert-success > p:last-child,
.jp-RenderedHTMLCommon .alert-success > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-danger {
  color: var(--jp-error-color0);
  background-color: var(--jp-error-color3);
  border-color: var(--jp-error-color2);
}

.jp-RenderedHTMLCommon .alert-danger hr {
  border-color: var(--jp-error-color3);
}

.jp-RenderedHTMLCommon .alert-danger > p:last-child,
.jp-RenderedHTMLCommon .alert-danger > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon blockquote {
  margin: 1em 2em;
  padding: 0 1em;
  border-left: 5px solid var(--jp-border-color2);
}

a.jp-InternalAnchorLink {
  visibility: hidden;
  margin-left: 8px;
  color: var(--md-blue-800);
}

h1:hover .jp-InternalAnchorLink,
h2:hover .jp-InternalAnchorLink,
h3:hover .jp-InternalAnchorLink,
h4:hover .jp-InternalAnchorLink,
h5:hover .jp-InternalAnchorLink,
h6:hover .jp-InternalAnchorLink {
  visibility: visible;
}

.jp-RenderedHTMLCommon kbd {
  background-color: var(--jp-rendermime-table-row-background);
  border: 1px solid var(--jp-border-color0);
  border-bottom-color: var(--jp-border-color2);
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.25);
  display: inline-block;
  font-size: var(--jp-ui-font-size0);
  line-height: 1em;
  padding: 0.2em 0.5em;
}

/* Most direct children of .jp-RenderedHTMLCommon have a margin-bottom of 1.0.
 * At the bottom of cells this is a bit too much as there is also spacing
 * between cells. Going all the way to 0 gets too tight between markdown and
 * code cells.
 */
.jp-RenderedHTMLCommon > *:last-child {
  margin-bottom: 0.5em;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-cursor-backdrop {
  position: fixed;
  width: 200px;
  height: 200px;
  margin-top: -100px;
  margin-left: -100px;
  will-change: transform;
  z-index: 100;
}

.lm-mod-drag-image {
  will-change: transform;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-lineFormSearch {
  padding: 4px 12px;
  background-color: var(--jp-layout-color2);
  box-shadow: var(--jp-toolbar-box-shadow);
  z-index: 2;
  font-size: var(--jp-ui-font-size1);
}

.jp-lineFormCaption {
  font-size: var(--jp-ui-font-size0);
  line-height: var(--jp-ui-font-size1);
  margin-top: 4px;
  color: var(--jp-ui-font-color0);
}

.jp-baseLineForm {
  border: none;
  border-radius: 0;
  position: absolute;
  background-size: 16px;
  background-repeat: no-repeat;
  background-position: center;
  outline: none;
}

.jp-lineFormButtonContainer {
  top: 4px;
  right: 8px;
  height: 24px;
  padding: 0 12px;
  width: 12px;
}

.jp-lineFormButtonIcon {
  top: 0;
  right: 0;
  background-color: var(--jp-brand-color1);
  height: 100%;
  width: 100%;
  box-sizing: border-box;
  padding: 4px 6px;
}

.jp-lineFormButton {
  top: 0;
  right: 0;
  background-color: transparent;
  height: 100%;
  width: 100%;
  box-sizing: border-box;
}

.jp-lineFormWrapper {
  overflow: hidden;
  padding: 0 8px;
  border: 1px solid var(--jp-border-color0);
  background-color: var(--jp-input-active-background);
  height: 22px;
}

.jp-lineFormWrapperFocusWithin {
  border: var(--jp-border-width) solid var(--md-blue-500);
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-lineFormInput {
  background: transparent;
  width: 200px;
  height: 100%;
  border: none;
  outline: none;
  color: var(--jp-ui-font-color0);
  line-height: 28px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-JSONEditor {
  display: flex;
  flex-direction: column;
  width: 100%;
}

.jp-JSONEditor-host {
  flex: 1 1 auto;
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  border-radius: 0;
  background: var(--jp-layout-color0);
  min-height: 50px;
  padding: 1px;
}

.jp-JSONEditor.jp-mod-error .jp-JSONEditor-host {
  border-color: red;
  outline-color: red;
}

.jp-JSONEditor-header {
  display: flex;
  flex: 1 0 auto;
  padding: 0 0 0 12px;
}

.jp-JSONEditor-header label {
  flex: 0 0 auto;
}

.jp-JSONEditor-commitButton {
  height: 16px;
  width: 16px;
  background-size: 18px;
  background-repeat: no-repeat;
  background-position: center;
}

.jp-JSONEditor-host.jp-mod-focused {
  background-color: var(--jp-input-active-background);
  border: 1px solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
}

.jp-Editor.jp-mod-dropTarget {
  border: var(--jp-border-width) solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/
.jp-DocumentSearch-input {
  border: none;
  outline: none;
  color: var(--jp-ui-font-color0);
  font-size: var(--jp-ui-font-size1);
  background-color: var(--jp-layout-color0);
  font-family: var(--jp-ui-font-family);
  padding: 2px 1px;
  resize: none;
}

.jp-DocumentSearch-overlay {
  position: absolute;
  background-color: var(--jp-toolbar-background);
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  border-left: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  top: 0;
  right: 0;
  z-index: 7;
  min-width: 405px;
  padding: 2px;
  font-size: var(--jp-ui-font-size1);

  --jp-private-document-search-button-height: 20px;
}

.jp-DocumentSearch-overlay button {
  background-color: var(--jp-toolbar-background);
  outline: 0;
}

.jp-DocumentSearch-overlay button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-overlay button:active {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-overlay-row {
  display: flex;
  align-items: center;
  margin-bottom: 2px;
}

.jp-DocumentSearch-button-content {
  display: inline-block;
  cursor: pointer;
  box-sizing: border-box;
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-button-content svg {
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-input-wrapper {
  border: var(--jp-border-width) solid var(--jp-border-color0);
  display: flex;
  background-color: var(--jp-layout-color0);
  margin: 2px;
}

.jp-DocumentSearch-input-wrapper:focus-within {
  border-color: var(--jp-cell-editor-active-border-color);
}

.jp-DocumentSearch-toggle-wrapper,
.jp-DocumentSearch-button-wrapper {
  all: initial;
  overflow: hidden;
  display: inline-block;
  border: none;
  box-sizing: border-box;
}

.jp-DocumentSearch-toggle-wrapper {
  width: 14px;
  height: 14px;
}

.jp-DocumentSearch-button-wrapper {
  width: var(--jp-private-document-search-button-height);
  height: var(--jp-private-document-search-button-height);
}

.jp-DocumentSearch-toggle-wrapper:focus,
.jp-DocumentSearch-button-wrapper:focus {
  outline: var(--jp-border-width) solid
    var(--jp-cell-editor-active-border-color);
  outline-offset: -1px;
}

.jp-DocumentSearch-toggle-wrapper,
.jp-DocumentSearch-button-wrapper,
.jp-DocumentSearch-button-content:focus {
  outline: none;
}

.jp-DocumentSearch-toggle-placeholder {
  width: 5px;
}

.jp-DocumentSearch-input-button::before {
  display: block;
  padding-top: 100%;
}

.jp-DocumentSearch-input-button-off {
  opacity: var(--jp-search-toggle-off-opacity);
}

.jp-DocumentSearch-input-button-off:hover {
  opacity: var(--jp-search-toggle-hover-opacity);
}

.jp-DocumentSearch-input-button-on {
  opacity: var(--jp-search-toggle-on-opacity);
}

.jp-DocumentSearch-index-counter {
  padding-left: 10px;
  padding-right: 10px;
  user-select: none;
  min-width: 35px;
  display: inline-block;
}

.jp-DocumentSearch-up-down-wrapper {
  display: inline-block;
  padding-right: 2px;
  margin-left: auto;
  white-space: nowrap;
}

.jp-DocumentSearch-spacer {
  margin-left: auto;
}

.jp-DocumentSearch-up-down-wrapper button {
  outline: 0;
  border: none;
  width: var(--jp-private-document-search-button-height);
  height: var(--jp-private-document-search-button-height);
  vertical-align: middle;
  margin: 1px 5px 2px;
}

.jp-DocumentSearch-up-down-button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-up-down-button:active {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-filter-button {
  border-radius: var(--jp-border-radius);
}

.jp-DocumentSearch-filter-button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-filter-button-enabled {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-filter-button-enabled:hover {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-search-options {
  padding: 0 8px;
  margin-left: 3px;
  width: 100%;
  display: grid;
  justify-content: start;
  grid-template-columns: 1fr 1fr;
  align-items: center;
  justify-items: stretch;
}

.jp-DocumentSearch-search-filter-disabled {
  color: var(--jp-ui-font-color2);
}

.jp-DocumentSearch-search-filter {
  display: flex;
  align-items: center;
  user-select: none;
}

.jp-DocumentSearch-regex-error {
  color: var(--jp-error-color0);
}

.jp-DocumentSearch-replace-button-wrapper {
  overflow: hidden;
  display: inline-block;
  box-sizing: border-box;
  border: var(--jp-border-width) solid var(--jp-border-color0);
  margin: auto 2px;
  padding: 1px 4px;
  height: calc(var(--jp-private-document-search-button-height) + 2px);
}

.jp-DocumentSearch-replace-button-wrapper:focus {
  border: var(--jp-border-width) solid var(--jp-cell-editor-active-border-color);
}

.jp-DocumentSearch-replace-button {
  display: inline-block;
  text-align: center;
  cursor: pointer;
  box-sizing: border-box;
  color: var(--jp-ui-font-color1);

  /* height - 2 * (padding of wrapper) */
  line-height: calc(var(--jp-private-document-search-button-height) - 2px);
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-replace-button:focus {
  outline: none;
}

.jp-DocumentSearch-replace-wrapper-class {
  margin-left: 14px;
  display: flex;
}

.jp-DocumentSearch-replace-toggle {
  border: none;
  background-color: var(--jp-toolbar-background);
  border-radius: var(--jp-border-radius);
}

.jp-DocumentSearch-replace-toggle:hover {
  background-color: var(--jp-layout-color2);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.cm-editor {
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  font-family: var(--jp-code-font-family);
  border: 0;
  border-radius: 0;
  height: auto;

  /* Changed to auto to autogrow */
}

.cm-editor pre {
  padding: 0 var(--jp-code-padding);
}

.jp-CodeMirrorEditor[data-type='inline'] .cm-dialog {
  background-color: var(--jp-layout-color0);
  color: var(--jp-content-font-color1);
}

.jp-CodeMirrorEditor {
  cursor: text;
}

/* When zoomed out 67% and 33% on a screen of 1440 width x 900 height */
@media screen and (min-width: 2138px) and (max-width: 4319px) {
  .jp-CodeMirrorEditor[data-type='inline'] .cm-cursor {
    border-left: var(--jp-code-cursor-width1) solid
      var(--jp-editor-cursor-color);
  }
}

/* When zoomed out less than 33% */
@media screen and (min-width: 4320px) {
  .jp-CodeMirrorEditor[data-type='inline'] .cm-cursor {
    border-left: var(--jp-code-cursor-width2) solid
      var(--jp-editor-cursor-color);
  }
}

.cm-editor.jp-mod-readOnly .cm-cursor {
  display: none;
}

.jp-CollaboratorCursor {
  border-left: 5px solid transparent;
  border-right: 5px solid transparent;
  border-top: none;
  border-bottom: 3px solid;
  background-clip: content-box;
  margin-left: -5px;
  margin-right: -5px;
}

.cm-searching,
.cm-searching span {
  /* `.cm-searching span`: we need to override syntax highlighting */
  background-color: var(--jp-search-unselected-match-background-color);
  color: var(--jp-search-unselected-match-color);
}

.cm-searching::selection,
.cm-searching span::selection {
  background-color: var(--jp-search-unselected-match-background-color);
  color: var(--jp-search-unselected-match-color);
}

.jp-current-match > .cm-searching,
.jp-current-match > .cm-searching span,
.cm-searching > .jp-current-match,
.cm-searching > .jp-current-match span {
  background-color: var(--jp-search-selected-match-background-color);
  color: var(--jp-search-selected-match-color);
}

.jp-current-match > .cm-searching::selection,
.cm-searching > .jp-current-match::selection,
.jp-current-match > .cm-searching span::selection {
  background-color: var(--jp-search-selected-match-background-color);
  color: var(--jp-search-selected-match-color);
}

.cm-trailingspace {
  background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAYAAAB4ka1VAAAAsElEQVQIHQGlAFr/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA7+r3zKmT0/+pk9P/7+r3zAAAAAAAAAAABAAAAAAAAAAA6OPzM+/q9wAAAAAA6OPzMwAAAAAAAAAAAgAAAAAAAAAAGR8NiRQaCgAZIA0AGR8NiQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQyoYJ/SY80UAAAAASUVORK5CYII=);
  background-position: center left;
  background-repeat: repeat-x;
}

.jp-CollaboratorCursor-hover {
  position: absolute;
  z-index: 1;
  transform: translateX(-50%);
  color: white;
  border-radius: 3px;
  padding-left: 4px;
  padding-right: 4px;
  padding-top: 1px;
  padding-bottom: 1px;
  text-align: center;
  font-size: var(--jp-ui-font-size1);
  white-space: nowrap;
}

.jp-CodeMirror-ruler {
  border-left: 1px dashed var(--jp-border-color2);
}

/* Styles for shared cursors (remote cursor locations and selected ranges) */
.jp-CodeMirrorEditor .cm-ySelectionCaret {
  position: relative;
  border-left: 1px solid black;
  margin-left: -1px;
  margin-right: -1px;
  box-sizing: border-box;
}

.jp-CodeMirrorEditor .cm-ySelectionCaret > .cm-ySelectionInfo {
  white-space: nowrap;
  position: absolute;
  top: -1.15em;
  padding-bottom: 0.05em;
  left: -1px;
  font-size: 0.95em;
  font-family: var(--jp-ui-font-family);
  font-weight: bold;
  line-height: normal;
  user-select: none;
  color: white;
  padding-left: 2px;
  padding-right: 2px;
  z-index: 101;
  transition: opacity 0.3s ease-in-out;
}

.jp-CodeMirrorEditor .cm-ySelectionInfo {
  transition-delay: 0.7s;
  opacity: 0;
}

.jp-CodeMirrorEditor .cm-ySelectionCaret:hover > .cm-ySelectionInfo {
  opacity: 1;
  transition-delay: 0s;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-MimeDocument {
  outline: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-filebrowser-button-height: 28px;
  --jp-private-filebrowser-button-width: 48px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-FileBrowser .jp-SidePanel-content {
  display: flex;
  flex-direction: column;
}

.jp-FileBrowser-toolbar.jp-Toolbar {
  flex-wrap: wrap;
  row-gap: 12px;
  border-bottom: none;
  height: auto;
  margin: 8px 12px 0;
  box-shadow: none;
  padding: 0;
  justify-content: flex-start;
}

.jp-FileBrowser-Panel {
  flex: 1 1 auto;
  display: flex;
  flex-direction: column;
}

.jp-BreadCrumbs {
  flex: 0 0 auto;
  margin: 8px 12px;
}

.jp-BreadCrumbs-item {
  margin: 0 2px;
  padding: 0 2px;
  border-radius: var(--jp-border-radius);
  cursor: pointer;
}

.jp-BreadCrumbs-item:hover {
  background-color: var(--jp-layout-color2);
}

.jp-BreadCrumbs-item:first-child {
  margin-left: 0;
}

.jp-BreadCrumbs-item.jp-mod-dropTarget {
  background-color: var(--jp-brand-color2);
  opacity: 0.7;
}

/*-----------------------------------------------------------------------------
| Buttons
|----------------------------------------------------------------------------*/

.jp-FileBrowser-toolbar > .jp-Toolbar-item {
  flex: 0 0 auto;
  padding-left: 0;
  padding-right: 2px;
  align-items: center;
  height: unset;
}

.jp-FileBrowser-toolbar > .jp-Toolbar-item .jp-ToolbarButtonComponent {
  width: 40px;
}

/*-----------------------------------------------------------------------------
| Other styles
|----------------------------------------------------------------------------*/

.jp-FileDialog.jp-mod-conflict input {
  color: var(--jp-error-color1);
}

.jp-FileDialog .jp-new-name-title {
  margin-top: 12px;
}

.jp-LastModified-hidden {
  display: none;
}

.jp-FileSize-hidden {
  display: none;
}

.jp-FileBrowser .lm-AccordionPanel > h3:first-child {
  display: none;
}

/*-----------------------------------------------------------------------------
| DirListing
|----------------------------------------------------------------------------*/

.jp-DirListing {
  flex: 1 1 auto;
  display: flex;
  flex-direction: column;
  outline: 0;
}

.jp-DirListing-header {
  flex: 0 0 auto;
  display: flex;
  flex-direction: row;
  align-items: center;
  overflow: hidden;
  border-top: var(--jp-border-width) solid var(--jp-border-color2);
  border-bottom: var(--jp-border-width) solid var(--jp-border-color1);
  box-shadow: var(--jp-toolbar-box-shadow);
  z-index: 2;
}

.jp-DirListing-headerItem {
  padding: 4px 12px 2px;
  font-weight: 500;
}

.jp-DirListing-headerItem:hover {
  background: var(--jp-layout-color2);
}

.jp-DirListing-headerItem.jp-id-name {
  flex: 1 0 84px;
}

.jp-DirListing-headerItem.jp-id-modified {
  flex: 0 0 112px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
}

.jp-DirListing-headerItem.jp-id-filesize {
  flex: 0 0 75px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
}

.jp-id-narrow {
  display: none;
  flex: 0 0 5px;
  padding: 4px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
  color: var(--jp-border-color2);
}

.jp-DirListing-narrow .jp-id-narrow {
  display: block;
}

.jp-DirListing-narrow .jp-id-modified,
.jp-DirListing-narrow .jp-DirListing-itemModified {
  display: none;
}

.jp-DirListing-headerItem.jp-mod-selected {
  font-weight: 600;
}

/* increase specificity to override bundled default */
.jp-DirListing-content {
  flex: 1 1 auto;
  margin: 0;
  padding: 0;
  list-style-type: none;
  overflow: auto;
  background-color: var(--jp-layout-color1);
}

.jp-DirListing-content mark {
  color: var(--jp-ui-font-color0);
  background-color: transparent;
  font-weight: bold;
}

.jp-DirListing-content .jp-DirListing-item.jp-mod-selected mark {
  color: var(--jp-ui-inverse-font-color0);
}

/* Style the directory listing content when a user drops a file to upload */
.jp-DirListing.jp-mod-native-drop .jp-DirListing-content {
  outline: 5px dashed rgba(128, 128, 128, 0.5);
  outline-offset: -10px;
  cursor: copy;
}

.jp-DirListing-item {
  display: flex;
  flex-direction: row;
  align-items: center;
  padding: 4px 12px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-DirListing-checkboxWrapper {
  /* Increases hit area of checkbox. */
  padding: 4px;
}

.jp-DirListing-header
  .jp-DirListing-checkboxWrapper
  + .jp-DirListing-headerItem {
  padding-left: 4px;
}

.jp-DirListing-content .jp-DirListing-checkboxWrapper {
  position: relative;
  left: -4px;
  margin: -4px 0 -4px -8px;
}

.jp-DirListing-checkboxWrapper.jp-mod-visible {
  visibility: visible;
}

/* For devices that support hovering, hide checkboxes until hovered, selected...
*/
@media (hover: hover) {
  .jp-DirListing-checkboxWrapper {
    visibility: hidden;
  }

  .jp-DirListing-item:hover .jp-DirListing-checkboxWrapper,
  .jp-DirListing-item.jp-mod-selected .jp-DirListing-checkboxWrapper {
    visibility: visible;
  }
}

.jp-DirListing-item[data-is-dot] {
  opacity: 75%;
}

.jp-DirListing-item.jp-mod-selected {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.jp-DirListing-item.jp-mod-dropTarget {
  background: var(--jp-brand-color3);
}

.jp-DirListing-item:hover:not(.jp-mod-selected) {
  background: var(--jp-layout-color2);
}

.jp-DirListing-itemIcon {
  flex: 0 0 20px;
  margin-right: 4px;
}

.jp-DirListing-itemText {
  flex: 1 0 64px;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
  user-select: none;
}

.jp-DirListing-itemText:focus {
  outline-width: 2px;
  outline-color: var(--jp-inverse-layout-color1);
  outline-style: solid;
  outline-offset: 1px;
}

.jp-DirListing-item.jp-mod-selected .jp-DirListing-itemText:focus {
  outline-color: var(--jp-layout-color1);
}

.jp-DirListing-itemModified {
  flex: 0 0 125px;
  text-align: right;
}

.jp-DirListing-itemFileSize {
  flex: 0 0 90px;
  text-align: right;
}

.jp-DirListing-editor {
  flex: 1 0 64px;
  outline: none;
  border: none;
  color: var(--jp-ui-font-color1);
  background-color: var(--jp-layout-color1);
}

.jp-DirListing-item.jp-mod-running .jp-DirListing-itemIcon::before {
  color: var(--jp-success-color1);
  content: '\25CF';
  font-size: 8px;
  position: absolute;
  left: -8px;
}

.jp-DirListing-item.jp-mod-running.jp-mod-selected
  .jp-DirListing-itemIcon::before {
  color: var(--jp-ui-inverse-font-color1);
}

.jp-DirListing-item.lm-mod-drag-image,
.jp-DirListing-item.jp-mod-selected.lm-mod-drag-image {
  font-size: var(--jp-ui-font-size1);
  padding-left: 4px;
  margin-left: 4px;
  width: 160px;
  background-color: var(--jp-ui-inverse-font-color2);
  box-shadow: var(--jp-elevation-z2);
  border-radius: 0;
  color: var(--jp-ui-font-color1);
  transform: translateX(-40%) translateY(-58%);
}

.jp-Document {
  min-width: 120px;
  min-height: 120px;
  outline: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Main OutputArea
| OutputArea has a list of Outputs
|----------------------------------------------------------------------------*/

.jp-OutputArea {
  overflow-y: auto;
}

.jp-OutputArea-child {
  display: table;
  table-layout: fixed;
  width: 100%;
  overflow: hidden;
}

.jp-OutputPrompt {
  width: var(--jp-cell-prompt-width);
  color: var(--jp-cell-outprompt-font-color);
  font-family: var(--jp-cell-prompt-font-family);
  padding: var(--jp-code-padding);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;
  opacity: var(--jp-cell-prompt-opacity);

  /* Right align prompt text, don't wrap to handle large prompt numbers */
  text-align: right;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;

  /* Disable text selection */
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-OutputArea-prompt {
  display: table-cell;
  vertical-align: top;
}

.jp-OutputArea-output {
  display: table-cell;
  width: 100%;
  height: auto;
  overflow: auto;
  user-select: text;
  -moz-user-select: text;
  -webkit-user-select: text;
  -ms-user-select: text;
}

.jp-OutputArea .jp-RenderedText {
  padding-left: 1ch;
}

/**
 * Prompt overlay.
 */

.jp-OutputArea-promptOverlay {
  position: absolute;
  top: 0;
  width: var(--jp-cell-prompt-width);
  height: 100%;
  opacity: 0.5;
}

.jp-OutputArea-promptOverlay:hover {
  background: var(--jp-layout-color2);
  box-shadow: inset 0 0 1px var(--jp-inverse-layout-color0);
  cursor: zoom-out;
}

.jp-mod-outputsScrolled .jp-OutputArea-promptOverlay:hover {
  cursor: zoom-in;
}

/**
 * Isolated output.
 */
.jp-OutputArea-output.jp-mod-isolated {
  width: 100%;
  display: block;
}

/*
When drag events occur, `lm-mod-override-cursor` is added to the body.
Because iframes steal all cursor events, the following two rules are necessary
to suppress pointer events while resize drags are occurring. There may be a
better solution to this problem.
*/
body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated {
  position: relative;
}

body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: transparent;
}

/* pre */

.jp-OutputArea-output pre {
  border: none;
  margin: 0;
  padding: 0;
  overflow-x: auto;
  overflow-y: auto;
  word-break: break-all;
  word-wrap: break-word;
  white-space: pre-wrap;
}

/* tables */

.jp-OutputArea-output.jp-RenderedHTMLCommon table {
  margin-left: 0;
  margin-right: 0;
}

/* description lists */

.jp-OutputArea-output dl,
.jp-OutputArea-output dt,
.jp-OutputArea-output dd {
  display: block;
}

.jp-OutputArea-output dl {
  width: 100%;
  overflow: hidden;
  padding: 0;
  margin: 0;
}

.jp-OutputArea-output dt {
  font-weight: bold;
  float: left;
  width: 20%;
  padding: 0;
  margin: 0;
}

.jp-OutputArea-output dd {
  float: left;
  width: 80%;
  padding: 0;
  margin: 0;
}

.jp-TrimmedOutputs pre {
  background: var(--jp-layout-color3);
  font-size: calc(var(--jp-code-font-size) * 1.4);
  text-align: center;
  text-transform: uppercase;
}

/* Hide the gutter in case of
 *  - nested output areas (e.g. in the case of output widgets)
 *  - mirrored output areas
 */
.jp-OutputArea .jp-OutputArea .jp-OutputArea-prompt {
  display: none;
}

/* Hide empty lines in the output area, for instance due to cleared widgets */
.jp-OutputArea-prompt:empty {
  padding: 0;
  border: 0;
}

/*-----------------------------------------------------------------------------
| executeResult is added to any Output-result for the display of the object
| returned by a cell
|----------------------------------------------------------------------------*/

.jp-OutputArea-output.jp-OutputArea-executeResult {
  margin-left: 0;
  width: 100%;
}

/* Text output with the Out[] prompt needs a top padding to match the
 * alignment of the Out[] prompt itself.
 */
.jp-OutputArea-executeResult .jp-RenderedText.jp-OutputArea-output {
  padding-top: var(--jp-code-padding);
  border-top: var(--jp-border-width) solid transparent;
}

/*-----------------------------------------------------------------------------
| The Stdin output
|----------------------------------------------------------------------------*/

.jp-Stdin-prompt {
  color: var(--jp-content-font-color0);
  padding-right: var(--jp-code-padding);
  vertical-align: baseline;
  flex: 0 0 auto;
}

.jp-Stdin-input {
  font-family: var(--jp-code-font-family);
  font-size: inherit;
  color: inherit;
  background-color: inherit;
  width: 42%;
  min-width: 200px;

  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;

  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0 0.25em;
  margin: 0 0.25em;
  flex: 0 0 70%;
}

.jp-Stdin-input::placeholder {
  opacity: 0;
}

.jp-Stdin-input:focus {
  box-shadow: none;
}

.jp-Stdin-input:focus::placeholder {
  opacity: 1;
}

/*-----------------------------------------------------------------------------
| Output Area View
|----------------------------------------------------------------------------*/

.jp-LinkedOutputView .jp-OutputArea {
  height: 100%;
  display: block;
}

.jp-LinkedOutputView .jp-OutputArea-output:only-child {
  height: 100%;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

@media print {
  .jp-OutputArea-child {
    break-inside: avoid-page;
  }
}

/*-----------------------------------------------------------------------------
| Mobile
|----------------------------------------------------------------------------*/
@media only screen and (max-width: 760px) {
  .jp-OutputPrompt {
    display: table-row;
    text-align: left;
  }

  .jp-OutputArea-child .jp-OutputArea-output {
    display: table-row;
    margin-left: var(--jp-notebook-padding);
  }
}

/* Trimmed outputs warning */
.jp-TrimmedOutputs > a {
  margin: 10px;
  text-decoration: none;
  cursor: pointer;
}

.jp-TrimmedOutputs > a:hover {
  text-decoration: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Table of Contents
|----------------------------------------------------------------------------*/

:root {
  --jp-private-toc-active-width: 4px;
}

.jp-TableOfContents {
  display: flex;
  flex-direction: column;
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  height: 100%;
}

.jp-TableOfContents-placeholder {
  text-align: center;
}

.jp-TableOfContents-placeholderContent {
  color: var(--jp-content-font-color2);
  padding: 8px;
}

.jp-TableOfContents-placeholderContent > h3 {
  margin-bottom: var(--jp-content-heading-margin-bottom);
}

.jp-TableOfContents .jp-SidePanel-content {
  overflow-y: auto;
}

.jp-TableOfContents-tree {
  margin: 4px;
}

.jp-TableOfContents ol {
  list-style-type: none;
}

/* stylelint-disable-next-line selector-max-type */
.jp-TableOfContents li > ol {
  /* Align left border with triangle icon center */
  padding-left: 11px;
}

.jp-TableOfContents-content {
  /* left margin for the active heading indicator */
  margin: 0 0 0 var(--jp-private-toc-active-width);
  padding: 0;
  background-color: var(--jp-layout-color1);
}

.jp-tocItem {
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-tocItem-heading {
  display: flex;
  cursor: pointer;
}

.jp-tocItem-heading:hover {
  background-color: var(--jp-layout-color2);
}

.jp-tocItem-content {
  display: block;
  padding: 4px 0;
  white-space: nowrap;
  text-overflow: ellipsis;
  overflow-x: hidden;
}

.jp-tocItem-collapser {
  height: 20px;
  margin: 2px 2px 0;
  padding: 0;
  background: none;
  border: none;
  cursor: pointer;
}

.jp-tocItem-collapser:hover {
  background-color: var(--jp-layout-color3);
}

/* Active heading indicator */

.jp-tocItem-heading::before {
  content: ' ';
  background: transparent;
  width: var(--jp-private-toc-active-width);
  height: 24px;
  position: absolute;
  left: 0;
  border-radius: var(--jp-border-radius);
}

.jp-tocItem-heading.jp-tocItem-active::before {
  background-color: var(--jp-brand-color1);
}

.jp-tocItem-heading:hover.jp-tocItem-active::before {
  background: var(--jp-brand-color0);
  opacity: 1;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Collapser {
  flex: 0 0 var(--jp-cell-collapser-width);
  padding: 0;
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
  border-radius: var(--jp-border-radius);
  opacity: 1;
}

.jp-Collapser-child {
  display: block;
  width: 100%;
  box-sizing: border-box;

  /* height: 100% doesn't work because the height of its parent is computed from content */
  position: absolute;
  top: 0;
  bottom: 0;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

/*
Hiding collapsers in print mode.

Note: input and output wrappers have "display: block" propery in print mode.
*/

@media print {
  .jp-Collapser {
    display: none;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Header/Footer
|----------------------------------------------------------------------------*/

/* Hidden by zero height by default */
.jp-CellHeader,
.jp-CellFooter {
  height: 0;
  width: 100%;
  padding: 0;
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Input
|----------------------------------------------------------------------------*/

/* All input areas */
.jp-InputArea {
  display: table;
  table-layout: fixed;
  width: 100%;
  overflow: hidden;
}

.jp-InputArea-editor {
  display: table-cell;
  overflow: hidden;
  vertical-align: top;

  /* This is the non-active, default styling */
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  border-radius: 0;
  background: var(--jp-cell-editor-background);
}

.jp-InputPrompt {
  display: table-cell;
  vertical-align: top;
  width: var(--jp-cell-prompt-width);
  color: var(--jp-cell-inprompt-font-color);
  font-family: var(--jp-cell-prompt-font-family);
  padding: var(--jp-code-padding);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  opacity: var(--jp-cell-prompt-opacity);
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;

  /* Right align prompt text, don't wrap to handle large prompt numbers */
  text-align: right;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;

  /* Disable text selection */
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

/*-----------------------------------------------------------------------------
| Mobile
|----------------------------------------------------------------------------*/
@media only screen and (max-width: 760px) {
  .jp-InputArea-editor {
    display: table-row;
    margin-left: var(--jp-notebook-padding);
  }

  .jp-InputPrompt {
    display: table-row;
    text-align: left;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Placeholder
|----------------------------------------------------------------------------*/

.jp-Placeholder {
  display: table;
  table-layout: fixed;
  width: 100%;
}

.jp-Placeholder-prompt {
  display: table-cell;
  box-sizing: border-box;
}

.jp-Placeholder-content {
  display: table-cell;
  padding: 4px 6px;
  border: 1px solid transparent;
  border-radius: 0;
  background: none;
  box-sizing: border-box;
  cursor: pointer;
}

.jp-Placeholder-contentContainer {
  display: flex;
}

.jp-Placeholder-content:hover,
.jp-InputPlaceholder > .jp-Placeholder-content:hover {
  border-color: var(--jp-layout-color3);
}

.jp-Placeholder-content .jp-MoreHorizIcon {
  width: 32px;
  height: 16px;
  border: 1px solid transparent;
  border-radius: var(--jp-border-radius);
}

.jp-Placeholder-content .jp-MoreHorizIcon:hover {
  border: 1px solid var(--jp-border-color1);
  box-shadow: 0 0 2px 0 rgba(0, 0, 0, 0.25);
  background-color: var(--jp-layout-color0);
}

.jp-PlaceholderText {
  white-space: nowrap;
  overflow-x: hidden;
  color: var(--jp-inverse-layout-color3);
  font-family: var(--jp-code-font-family);
}

.jp-InputPlaceholder > .jp-Placeholder-content {
  border-color: var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Private CSS variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-cell-scrolling-output-offset: 5px;
}

/*-----------------------------------------------------------------------------
| Cell
|----------------------------------------------------------------------------*/

.jp-Cell {
  padding: var(--jp-cell-padding);
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Common input/output
|----------------------------------------------------------------------------*/

.jp-Cell-inputWrapper,
.jp-Cell-outputWrapper {
  display: flex;
  flex-direction: row;
  padding: 0;
  margin: 0;

  /* Added to reveal the box-shadow on the input and output collapsers. */
  overflow: visible;
}

/* Only input/output areas inside cells */
.jp-Cell-inputArea,
.jp-Cell-outputArea {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Collapser
|----------------------------------------------------------------------------*/

/* Make the output collapser disappear when there is not output, but do so
 * in a manner that leaves it in the layout and preserves its width.
 */
.jp-Cell.jp-mod-noOutputs .jp-Cell-outputCollapser {
  border: none !important;
  background: transparent !important;
}

.jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputCollapser {
  min-height: var(--jp-cell-collapser-min-height);
}

/*-----------------------------------------------------------------------------
| Output
|----------------------------------------------------------------------------*/

/* Put a space between input and output when there IS output */
.jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputWrapper {
  margin-top: 5px;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea {
  overflow-y: auto;
  max-height: 24em;
  margin-left: var(--jp-private-cell-scrolling-output-offset);
  resize: vertical;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea[style*='height'] {
  max-height: unset;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea::after {
  content: ' ';
  box-shadow: inset 0 0 6px 2px rgb(0 0 0 / 30%);
  width: 100%;
  height: 100%;
  position: sticky;
  bottom: 0;
  top: 0;
  margin-top: -50%;
  float: left;
  display: block;
  pointer-events: none;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-child {
  padding-top: 6px;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-prompt {
  width: calc(
    var(--jp-cell-prompt-width) - var(--jp-private-cell-scrolling-output-offset)
  );
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-promptOverlay {
  left: calc(-1 * var(--jp-private-cell-scrolling-output-offset));
}

/*-----------------------------------------------------------------------------
| CodeCell
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| MarkdownCell
|----------------------------------------------------------------------------*/

.jp-MarkdownOutput {
  display: table-cell;
  width: 100%;
  margin-top: 0;
  margin-bottom: 0;
  padding-left: var(--jp-code-padding);
}

.jp-MarkdownOutput.jp-RenderedHTMLCommon {
  overflow: auto;
}

/* collapseHeadingButton (show always if hiddenCellsButton is _not_ shown) */
.jp-collapseHeadingButton {
  display: flex;
  min-height: var(--jp-cell-collapser-min-height);
  font-size: var(--jp-code-font-size);
  position: absolute;
  background-color: transparent;
  background-size: 25px;
  background-repeat: no-repeat;
  background-position-x: center;
  background-position-y: top;
  background-image: var(--jp-icon-caret-down);
  right: 0;
  top: 0;
  bottom: 0;
}

.jp-collapseHeadingButton.jp-mod-collapsed {
  background-image: var(--jp-icon-caret-right);
}

/*
 set the container font size to match that of content
 so that the nested collapse buttons have the right size
*/
.jp-MarkdownCell .jp-InputPrompt {
  font-size: var(--jp-content-font-size1);
}

/*
  Align collapseHeadingButton with cell top header
  The font sizes are identical to the ones in packages/rendermime/style/base.css
*/
.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='1'] {
  font-size: var(--jp-content-font-size5);
  background-position-y: calc(0.3 * var(--jp-content-font-size5));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='2'] {
  font-size: var(--jp-content-font-size4);
  background-position-y: calc(0.3 * var(--jp-content-font-size4));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='3'] {
  font-size: var(--jp-content-font-size3);
  background-position-y: calc(0.3 * var(--jp-content-font-size3));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='4'] {
  font-size: var(--jp-content-font-size2);
  background-position-y: calc(0.3 * var(--jp-content-font-size2));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='5'] {
  font-size: var(--jp-content-font-size1);
  background-position-y: top;
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='6'] {
  font-size: var(--jp-content-font-size0);
  background-position-y: top;
}

/* collapseHeadingButton (show only on (hover,active) if hiddenCellsButton is shown) */
.jp-Notebook.jp-mod-showHiddenCellsButton .jp-collapseHeadingButton {
  display: none;
}

.jp-Notebook.jp-mod-showHiddenCellsButton
  :is(.jp-MarkdownCell:hover, .jp-mod-active)
  .jp-collapseHeadingButton {
  display: flex;
}

/* showHiddenCellsButton (only show if jp-mod-showHiddenCellsButton is set, which
is a consequence of the showHiddenCellsButton option in Notebook Settings)*/
.jp-Notebook.jp-mod-showHiddenCellsButton .jp-showHiddenCellsButton {
  margin-left: calc(var(--jp-cell-prompt-width) + 2 * var(--jp-code-padding));
  margin-top: var(--jp-code-padding);
  border: 1px solid var(--jp-border-color2);
  background-color: var(--jp-border-color3) !important;
  color: var(--jp-content-font-color0) !important;
  display: flex;
}

.jp-Notebook.jp-mod-showHiddenCellsButton .jp-showHiddenCellsButton:hover {
  background-color: var(--jp-border-color2) !important;
}

.jp-showHiddenCellsButton {
  display: none;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

/*
Using block instead of flex to allow the use of the break-inside CSS property for
cell outputs.
*/

@media print {
  .jp-Cell-inputWrapper,
  .jp-Cell-outputWrapper {
    display: block;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-notebook-toolbar-padding: 2px 5px 2px 2px;
}

/*-----------------------------------------------------------------------------

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-NotebookPanel-toolbar {
  padding: var(--jp-notebook-toolbar-padding);

  /* disable paint containment from lumino 2.0 default strict CSS containment */
  contain: style size !important;
}

.jp-Toolbar-item.jp-Notebook-toolbarCellType .jp-select-wrapper.jp-mod-focused {
  border: none;
  box-shadow: none;
}

.jp-Notebook-toolbarCellTypeDropdown select {
  height: 24px;
  font-size: var(--jp-ui-font-size1);
  line-height: 14px;
  border-radius: 0;
  display: block;
}

.jp-Notebook-toolbarCellTypeDropdown span {
  top: 5px !important;
}

.jp-Toolbar-responsive-popup {
  position: absolute;
  height: fit-content;
  display: flex;
  flex-direction: row;
  flex-wrap: wrap;
  justify-content: flex-end;
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  background: var(--jp-toolbar-background);
  min-height: var(--jp-toolbar-micro-height);
  padding: var(--jp-notebook-toolbar-padding);
  z-index: 1;
  right: 0;
  top: 0;
}

.jp-Toolbar > .jp-Toolbar-responsive-opener {
  margin-left: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-Notebook-ExecutionIndicator {
  position: relative;
  display: inline-block;
  height: 100%;
  z-index: 9997;
}

.jp-Notebook-ExecutionIndicator-tooltip {
  visibility: hidden;
  height: auto;
  width: max-content;
  width: -moz-max-content;
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color1);
  text-align: justify;
  border-radius: 6px;
  padding: 0 5px;
  position: fixed;
  display: table;
}

.jp-Notebook-ExecutionIndicator-tooltip.up {
  transform: translateX(-50%) translateY(-100%) translateY(-32px);
}

.jp-Notebook-ExecutionIndicator-tooltip.down {
  transform: translateX(calc(-100% + 16px)) translateY(5px);
}

.jp-Notebook-ExecutionIndicator-tooltip.hidden {
  display: none;
}

.jp-Notebook-ExecutionIndicator:hover .jp-Notebook-ExecutionIndicator-tooltip {
  visibility: visible;
}

.jp-Notebook-ExecutionIndicator span {
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  color: var(--jp-ui-font-color1);
  line-height: 24px;
  display: block;
}

.jp-Notebook-ExecutionIndicator-progress-bar {
  display: flex;
  justify-content: center;
  height: 100%;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*
 * Execution indicator
 */
.jp-tocItem-content::after {
  content: '';

  /* Must be identical to form a circle */
  width: 12px;
  height: 12px;
  background: none;
  border: none;
  position: absolute;
  right: 0;
}

.jp-tocItem-content[data-running='0']::after {
  border-radius: 50%;
  border: var(--jp-border-width) solid var(--jp-inverse-layout-color3);
  background: none;
}

.jp-tocItem-content[data-running='1']::after {
  border-radius: 50%;
  border: var(--jp-border-width) solid var(--jp-inverse-layout-color3);
  background-color: var(--jp-inverse-layout-color3);
}

.jp-tocItem-content[data-running='0'],
.jp-tocItem-content[data-running='1'] {
  margin-right: 12px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-Notebook-footer {
  height: 27px;
  margin-left: calc(
    var(--jp-cell-prompt-width) + var(--jp-cell-collapser-width) +
      var(--jp-cell-padding)
  );
  width: calc(
    100% -
      (
        var(--jp-cell-prompt-width) + var(--jp-cell-collapser-width) +
          var(--jp-cell-padding) + var(--jp-cell-padding)
      )
  );
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  color: var(--jp-ui-font-color3);
  margin-top: 6px;
  background: none;
  cursor: pointer;
}

.jp-Notebook-footer:focus {
  border-color: var(--jp-cell-editor-active-border-color);
}

/* For devices that support hovering, hide footer until hover */
@media (hover: hover) {
  .jp-Notebook-footer {
    opacity: 0;
  }

  .jp-Notebook-footer:focus,
  .jp-Notebook-footer:hover {
    opacity: 1;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Imports
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| CSS variables
|----------------------------------------------------------------------------*/

:root {
  --jp-side-by-side-output-size: 1fr;
  --jp-side-by-side-resized-cell: var(--jp-side-by-side-output-size);
  --jp-private-notebook-dragImage-width: 304px;
  --jp-private-notebook-dragImage-height: 36px;
  --jp-private-notebook-selected-color: var(--md-blue-400);
  --jp-private-notebook-active-color: var(--md-green-400);
}

/*-----------------------------------------------------------------------------
| Notebook
|----------------------------------------------------------------------------*/

/* stylelint-disable selector-max-class */

.jp-NotebookPanel {
  display: block;
  height: 100%;
}

.jp-NotebookPanel.jp-Document {
  min-width: 240px;
  min-height: 120px;
}

.jp-Notebook {
  padding: var(--jp-notebook-padding);
  outline: none;
  overflow: auto;
  background: var(--jp-layout-color0);
}

.jp-Notebook.jp-mod-scrollPastEnd::after {
  display: block;
  content: '';
  min-height: var(--jp-notebook-scroll-padding);
}

.jp-MainAreaWidget-ContainStrict .jp-Notebook * {
  contain: strict;
}

.jp-Notebook .jp-Cell {
  overflow: visible;
}

.jp-Notebook .jp-Cell .jp-InputPrompt {
  cursor: move;
}

/*-----------------------------------------------------------------------------
| Notebook state related styling
|
| The notebook and cells each have states, here are the possibilities:
|
| - Notebook
|   - Command
|   - Edit
| - Cell
|   - None
|   - Active (only one can be active)
|   - Selected (the cells actions are applied to)
|   - Multiselected (when multiple selected, the cursor)
|   - No outputs
|----------------------------------------------------------------------------*/

/* Command or edit modes */

.jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-InputPrompt {
  opacity: var(--jp-cell-prompt-not-active-opacity);
  color: var(--jp-cell-prompt-not-active-font-color);
}

.jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-OutputPrompt {
  opacity: var(--jp-cell-prompt-not-active-opacity);
  color: var(--jp-cell-prompt-not-active-font-color);
}

/* cell is active */
.jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser {
  background: var(--jp-brand-color1);
}

/* cell is dirty */
.jp-Notebook .jp-Cell.jp-mod-dirty .jp-InputPrompt {
  color: var(--jp-warn-color1);
}

.jp-Notebook .jp-Cell.jp-mod-dirty .jp-InputPrompt::before {
  color: var(--jp-warn-color1);
  content: '';
}

.jp-Notebook .jp-Cell.jp-mod-active.jp-mod-dirty .jp-Collapser {
  background: var(--jp-warn-color1);
}

/* collapser is hovered */
.jp-Notebook .jp-Cell .jp-Collapser:hover {
  box-shadow: var(--jp-elevation-z2);
  background: var(--jp-brand-color1);
  opacity: var(--jp-cell-collapser-not-active-hover-opacity);
}

/* cell is active and collapser is hovered */
.jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser:hover {
  background: var(--jp-brand-color0);
  opacity: 1;
}

/* Command mode */

.jp-Notebook.jp-mod-commandMode .jp-Cell.jp-mod-selected {
  background: var(--jp-notebook-multiselected-color);
}

.jp-Notebook.jp-mod-commandMode
  .jp-Cell.jp-mod-active.jp-mod-selected:not(.jp-mod-multiSelected) {
  background: transparent;
}

/* Edit mode */

.jp-Notebook.jp-mod-editMode .jp-Cell.jp-mod-active .jp-InputArea-editor {
  border: var(--jp-border-width) solid var(--jp-cell-editor-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
  background-color: var(--jp-cell-editor-active-background);
}

/*-----------------------------------------------------------------------------
| Notebook drag and drop
|----------------------------------------------------------------------------*/

.jp-Notebook-cell.jp-mod-dropSource {
  opacity: 0.5;
}

.jp-Notebook-cell.jp-mod-dropTarget,
.jp-Notebook.jp-mod-commandMode
  .jp-Notebook-cell.jp-mod-active.jp-mod-selected.jp-mod-dropTarget {
  border-top-color: var(--jp-private-notebook-selected-color);
  border-top-style: solid;
  border-top-width: 2px;
}

.jp-dragImage {
  display: block;
  flex-direction: row;
  width: var(--jp-private-notebook-dragImage-width);
  height: var(--jp-private-notebook-dragImage-height);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background);
  overflow: visible;
}

.jp-dragImage-singlePrompt {
  box-shadow: 2px 2px 4px 0 rgba(0, 0, 0, 0.12);
}

.jp-dragImage .jp-dragImage-content {
  flex: 1 1 auto;
  z-index: 2;
  font-size: var(--jp-code-font-size);
  font-family: var(--jp-code-font-family);
  line-height: var(--jp-code-line-height);
  padding: var(--jp-code-padding);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background-color);
  color: var(--jp-content-font-color3);
  text-align: left;
  margin: 4px 4px 4px 0;
}

.jp-dragImage .jp-dragImage-prompt {
  flex: 0 0 auto;
  min-width: 36px;
  color: var(--jp-cell-inprompt-font-color);
  padding: var(--jp-code-padding);
  padding-left: 12px;
  font-family: var(--jp-cell-prompt-font-family);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  line-height: 1.9;
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;
}

.jp-dragImage-multipleBack {
  z-index: -1;
  position: absolute;
  height: 32px;
  width: 300px;
  top: 8px;
  left: 8px;
  background: var(--jp-layout-color2);
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  box-shadow: 2px 2px 4px 0 rgba(0, 0, 0, 0.12);
}

/*-----------------------------------------------------------------------------
| Cell toolbar
|----------------------------------------------------------------------------*/

.jp-NotebookTools {
  display: block;
  min-width: var(--jp-sidebar-min-width);
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);

  /* This is needed so that all font sizing of children done in ems is
    * relative to this base size */
  font-size: var(--jp-ui-font-size1);
  overflow: auto;
}

.jp-ActiveCellTool {
  padding: 12px 0;
  display: flex;
}

.jp-ActiveCellTool-Content {
  flex: 1 1 auto;
}

.jp-ActiveCellTool .jp-ActiveCellTool-CellContent {
  background: var(--jp-cell-editor-background);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  border-radius: 0;
  min-height: 29px;
}

.jp-ActiveCellTool .jp-InputPrompt {
  min-width: calc(var(--jp-cell-prompt-width) * 0.75);
}

.jp-ActiveCellTool-CellContent > pre {
  padding: 5px 4px;
  margin: 0;
  white-space: normal;
}

.jp-MetadataEditorTool {
  flex-direction: column;
  padding: 12px 0;
}

.jp-RankedPanel > :not(:first-child) {
  margin-top: 12px;
}

.jp-KeySelector select.jp-mod-styled {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  border: var(--jp-border-width) solid var(--jp-border-color1);
}

.jp-KeySelector label,
.jp-MetadataEditorTool label,
.jp-NumberSetter label {
  line-height: 1.4;
}

.jp-NotebookTools .jp-select-wrapper {
  margin-top: 4px;
  margin-bottom: 0;
}

.jp-NumberSetter input {
  width: 100%;
  margin-top: 4px;
}

.jp-NotebookTools .jp-Collapse {
  margin-top: 16px;
}

/*-----------------------------------------------------------------------------
| Presentation Mode (.jp-mod-presentationMode)
|----------------------------------------------------------------------------*/

.jp-mod-presentationMode .jp-Notebook {
  --jp-content-font-size1: var(--jp-content-presentation-font-size1);
  --jp-code-font-size: var(--jp-code-presentation-font-size);
}

.jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-InputPrompt,
.jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-OutputPrompt {
  flex: 0 0 110px;
}

/*-----------------------------------------------------------------------------
| Side-by-side Mode (.jp-mod-sideBySide)
|----------------------------------------------------------------------------*/
.jp-mod-sideBySide.jp-Notebook .jp-Notebook-cell {
  margin-top: 3em;
  margin-bottom: 3em;
  margin-left: 5%;
  margin-right: 5%;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell {
  display: grid;
  grid-template-columns: minmax(0, 1fr) min-content minmax(
      0,
      var(--jp-side-by-side-output-size)
    );
  grid-template-rows: auto minmax(0, 1fr) auto;
  grid-template-areas:
    'header header header'
    'input handle output'
    'footer footer footer';
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell.jp-mod-resizedCell {
  grid-template-columns: minmax(0, 1fr) min-content minmax(
      0,
      var(--jp-side-by-side-resized-cell)
    );
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellHeader {
  grid-area: header;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-Cell-inputWrapper {
  grid-area: input;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-Cell-outputWrapper {
  /* overwrite the default margin (no vertical separation needed in side by side move */
  margin-top: 0;
  grid-area: output;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellFooter {
  grid-area: footer;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellResizeHandle {
  grid-area: handle;
  user-select: none;
  display: block;
  height: 100%;
  cursor: ew-resize;
  padding: 0 var(--jp-cell-padding);
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellResizeHandle::after {
  content: '';
  display: block;
  background: var(--jp-border-color2);
  height: 100%;
  width: 5px;
}

.jp-mod-sideBySide.jp-Notebook
  .jp-CodeCell.jp-mod-resizedCell
  .jp-CellResizeHandle::after {
  background: var(--jp-border-color0);
}

.jp-CellResizeHandle {
  display: none;
}

/*-----------------------------------------------------------------------------
| Placeholder
|----------------------------------------------------------------------------*/

.jp-Cell-Placeholder {
  padding-left: 55px;
}

.jp-Cell-Placeholder-wrapper {
  background: #fff;
  border: 1px solid;
  border-color: #e5e6e9 #dfe0e4 #d0d1d5;
  border-radius: 4px;
  -webkit-border-radius: 4px;
  margin: 10px 15px;
}

.jp-Cell-Placeholder-wrapper-inner {
  padding: 15px;
  position: relative;
}

.jp-Cell-Placeholder-wrapper-body {
  background-repeat: repeat;
  background-size: 50% auto;
}

.jp-Cell-Placeholder-wrapper-body div {
  background: #f6f7f8;
  background-image: -webkit-linear-gradient(
    left,
    #f6f7f8 0%,
    #edeef1 20%,
    #f6f7f8 40%,
    #f6f7f8 100%
  );
  background-repeat: no-repeat;
  background-size: 800px 104px;
  height: 104px;
  position: absolute;
  right: 15px;
  left: 15px;
  top: 15px;
}

div.jp-Cell-Placeholder-h1 {
  top: 20px;
  height: 20px;
  left: 15px;
  width: 150px;
}

div.jp-Cell-Placeholder-h2 {
  left: 15px;
  top: 50px;
  height: 10px;
  width: 100px;
}

div.jp-Cell-Placeholder-content-1,
div.jp-Cell-Placeholder-content-2,
div.jp-Cell-Placeholder-content-3 {
  left: 15px;
  right: 15px;
  height: 10px;
}

div.jp-Cell-Placeholder-content-1 {
  top: 100px;
}

div.jp-Cell-Placeholder-content-2 {
  top: 120px;
}

div.jp-Cell-Placeholder-content-3 {
  top: 140px;
}

</style>
<style type="text/css">
/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*
The following CSS variables define the main, public API for styling JupyterLab.
These variables should be used by all plugins wherever possible. In other
words, plugins should not define custom colors, sizes, etc unless absolutely
necessary. This enables users to change the visual theme of JupyterLab
by changing these variables.

Many variables appear in an ordered sequence (0,1,2,3). These sequences
are designed to work well together, so for example, `--jp-border-color1` should
be used with `--jp-layout-color1`. The numbers have the following meanings:

* 0: super-primary, reserved for special emphasis
* 1: primary, most important under normal situations
* 2: secondary, next most important under normal situations
* 3: tertiary, next most important under normal situations

Throughout JupyterLab, we are mostly following principles from Google's
Material Design when selecting colors. We are not, however, following
all of MD as it is not optimized for dense, information rich UIs.
*/

:root {
  /* Elevation
   *
   * We style box-shadows using Material Design's idea of elevation. These particular numbers are taken from here:
   *
   * https://github.com/material-components/material-components-web
   * https://material-components-web.appspot.com/elevation.html
   */

  --jp-shadow-base-lightness: 0;
  --jp-shadow-umbra-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.2
  );
  --jp-shadow-penumbra-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.14
  );
  --jp-shadow-ambient-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.12
  );
  --jp-elevation-z0: none;
  --jp-elevation-z1: 0 2px 1px -1px var(--jp-shadow-umbra-color),
    0 1px 1px 0 var(--jp-shadow-penumbra-color),
    0 1px 3px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z2: 0 3px 1px -2px var(--jp-shadow-umbra-color),
    0 2px 2px 0 var(--jp-shadow-penumbra-color),
    0 1px 5px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z4: 0 2px 4px -1px var(--jp-shadow-umbra-color),
    0 4px 5px 0 var(--jp-shadow-penumbra-color),
    0 1px 10px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z6: 0 3px 5px -1px var(--jp-shadow-umbra-color),
    0 6px 10px 0 var(--jp-shadow-penumbra-color),
    0 1px 18px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z8: 0 5px 5px -3px var(--jp-shadow-umbra-color),
    0 8px 10px 1px var(--jp-shadow-penumbra-color),
    0 3px 14px 2px var(--jp-shadow-ambient-color);
  --jp-elevation-z12: 0 7px 8px -4px var(--jp-shadow-umbra-color),
    0 12px 17px 2px var(--jp-shadow-penumbra-color),
    0 5px 22px 4px var(--jp-shadow-ambient-color);
  --jp-elevation-z16: 0 8px 10px -5px var(--jp-shadow-umbra-color),
    0 16px 24px 2px var(--jp-shadow-penumbra-color),
    0 6px 30px 5px var(--jp-shadow-ambient-color);
  --jp-elevation-z20: 0 10px 13px -6px var(--jp-shadow-umbra-color),
    0 20px 31px 3px var(--jp-shadow-penumbra-color),
    0 8px 38px 7px var(--jp-shadow-ambient-color);
  --jp-elevation-z24: 0 11px 15px -7px var(--jp-shadow-umbra-color),
    0 24px 38px 3px var(--jp-shadow-penumbra-color),
    0 9px 46px 8px var(--jp-shadow-ambient-color);

  /* Borders
   *
   * The following variables, specify the visual styling of borders in JupyterLab.
   */

  --jp-border-width: 1px;
  --jp-border-color0: var(--md-grey-400);
  --jp-border-color1: var(--md-grey-400);
  --jp-border-color2: var(--md-grey-300);
  --jp-border-color3: var(--md-grey-200);
  --jp-inverse-border-color: var(--md-grey-600);
  --jp-border-radius: 2px;

  /* UI Fonts
   *
   * The UI font CSS variables are used for the typography all of the JupyterLab
   * user interface elements that are not directly user generated content.
   *
   * The font sizing here is done assuming that the body font size of --jp-ui-font-size1
   * is applied to a parent element. When children elements, such as headings, are sized
   * in em all things will be computed relative to that body size.
   */

  --jp-ui-font-scale-factor: 1.2;
  --jp-ui-font-size0: 0.83333em;
  --jp-ui-font-size1: 13px; /* Base font size */
  --jp-ui-font-size2: 1.2em;
  --jp-ui-font-size3: 1.44em;
  --jp-ui-font-family: system-ui, -apple-system, blinkmacsystemfont, 'Segoe UI',
    helvetica, arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji',
    'Segoe UI Symbol';

  /*
   * Use these font colors against the corresponding main layout colors.
   * In a light theme, these go from dark to light.
   */

  /* Defaults use Material Design specification */
  --jp-ui-font-color0: rgba(0, 0, 0, 1);
  --jp-ui-font-color1: rgba(0, 0, 0, 0.87);
  --jp-ui-font-color2: rgba(0, 0, 0, 0.54);
  --jp-ui-font-color3: rgba(0, 0, 0, 0.38);

  /*
   * Use these against the brand/accent/warn/error colors.
   * These will typically go from light to darker, in both a dark and light theme.
   */

  --jp-ui-inverse-font-color0: rgba(255, 255, 255, 1);
  --jp-ui-inverse-font-color1: rgba(255, 255, 255, 1);
  --jp-ui-inverse-font-color2: rgba(255, 255, 255, 0.7);
  --jp-ui-inverse-font-color3: rgba(255, 255, 255, 0.5);

  /* Content Fonts
   *
   * Content font variables are used for typography of user generated content.
   *
   * The font sizing here is done assuming that the body font size of --jp-content-font-size1
   * is applied to a parent element. When children elements, such as headings, are sized
   * in em all things will be computed relative to that body size.
   */

  --jp-content-line-height: 1.6;
  --jp-content-font-scale-factor: 1.2;
  --jp-content-font-size0: 0.83333em;
  --jp-content-font-size1: 14px; /* Base font size */
  --jp-content-font-size2: 1.2em;
  --jp-content-font-size3: 1.44em;
  --jp-content-font-size4: 1.728em;
  --jp-content-font-size5: 2.0736em;

  /* This gives a magnification of about 125% in presentation mode over normal. */
  --jp-content-presentation-font-size1: 17px;
  --jp-content-heading-line-height: 1;
  --jp-content-heading-margin-top: 1.2em;
  --jp-content-heading-margin-bottom: 0.8em;
  --jp-content-heading-font-weight: 500;

  /* Defaults use Material Design specification */
  --jp-content-font-color0: rgba(0, 0, 0, 1);
  --jp-content-font-color1: rgba(0, 0, 0, 0.87);
  --jp-content-font-color2: rgba(0, 0, 0, 0.54);
  --jp-content-font-color3: rgba(0, 0, 0, 0.38);
  --jp-content-link-color: var(--md-blue-900);
  --jp-content-font-family: system-ui, -apple-system, blinkmacsystemfont,
    'Segoe UI', helvetica, arial, sans-serif, 'Apple Color Emoji',
    'Segoe UI Emoji', 'Segoe UI Symbol';

  /*
   * Code Fonts
   *
   * Code font variables are used for typography of code and other monospaces content.
   */

  --jp-code-font-size: 13px;
  --jp-code-line-height: 1.3077; /* 17px for 13px base */
  --jp-code-padding: 5px; /* 5px for 13px base, codemirror highlighting needs integer px value */
  --jp-code-font-family-default: menlo, consolas, 'DejaVu Sans Mono', monospace;
  --jp-code-font-family: var(--jp-code-font-family-default);

  /* This gives a magnification of about 125% in presentation mode over normal. */
  --jp-code-presentation-font-size: 16px;

  /* may need to tweak cursor width if you change font size */
  --jp-code-cursor-width0: 1.4px;
  --jp-code-cursor-width1: 2px;
  --jp-code-cursor-width2: 4px;

  /* Layout
   *
   * The following are the main layout colors use in JupyterLab. In a light
   * theme these would go from light to dark.
   */

  --jp-layout-color0: white;
  --jp-layout-color1: white;
  --jp-layout-color2: var(--md-grey-200);
  --jp-layout-color3: var(--md-grey-400);
  --jp-layout-color4: var(--md-grey-600);

  /* Inverse Layout
   *
   * The following are the inverse layout colors use in JupyterLab. In a light
   * theme these would go from dark to light.
   */

  --jp-inverse-layout-color0: #111;
  --jp-inverse-layout-color1: var(--md-grey-900);
  --jp-inverse-layout-color2: var(--md-grey-800);
  --jp-inverse-layout-color3: var(--md-grey-700);
  --jp-inverse-layout-color4: var(--md-grey-600);

  /* Brand/accent */

  --jp-brand-color0: var(--md-blue-900);
  --jp-brand-color1: var(--md-blue-700);
  --jp-brand-color2: var(--md-blue-300);
  --jp-brand-color3: var(--md-blue-100);
  --jp-brand-color4: var(--md-blue-50);
  --jp-accent-color0: var(--md-green-900);
  --jp-accent-color1: var(--md-green-700);
  --jp-accent-color2: var(--md-green-300);
  --jp-accent-color3: var(--md-green-100);

  /* State colors (warn, error, success, info) */

  --jp-warn-color0: var(--md-orange-900);
  --jp-warn-color1: var(--md-orange-700);
  --jp-warn-color2: var(--md-orange-300);
  --jp-warn-color3: var(--md-orange-100);
  --jp-error-color0: var(--md-red-900);
  --jp-error-color1: var(--md-red-700);
  --jp-error-color2: var(--md-red-300);
  --jp-error-color3: var(--md-red-100);
  --jp-success-color0: var(--md-green-900);
  --jp-success-color1: var(--md-green-700);
  --jp-success-color2: var(--md-green-300);
  --jp-success-color3: var(--md-green-100);
  --jp-info-color0: var(--md-cyan-900);
  --jp-info-color1: var(--md-cyan-700);
  --jp-info-color2: var(--md-cyan-300);
  --jp-info-color3: var(--md-cyan-100);

  /* Cell specific styles */

  --jp-cell-padding: 5px;
  --jp-cell-collapser-width: 8px;
  --jp-cell-collapser-min-height: 20px;
  --jp-cell-collapser-not-active-hover-opacity: 0.6;
  --jp-cell-editor-background: var(--md-grey-100);
  --jp-cell-editor-border-color: var(--md-grey-300);
  --jp-cell-editor-box-shadow: inset 0 0 2px var(--md-blue-300);
  --jp-cell-editor-active-background: var(--jp-layout-color0);
  --jp-cell-editor-active-border-color: var(--jp-brand-color1);
  --jp-cell-prompt-width: 64px;
  --jp-cell-prompt-font-family: var(--jp-code-font-family-default);
  --jp-cell-prompt-letter-spacing: 0;
  --jp-cell-prompt-opacity: 1;
  --jp-cell-prompt-not-active-opacity: 0.5;
  --jp-cell-prompt-not-active-font-color: var(--md-grey-700);

  /* A custom blend of MD grey and blue 600
   * See https://meyerweb.com/eric/tools/color-blend/#546E7A:1E88E5:5:hex */
  --jp-cell-inprompt-font-color: #307fc1;

  /* A custom blend of MD grey and orange 600
   * https://meyerweb.com/eric/tools/color-blend/#546E7A:F4511E:5:hex */
  --jp-cell-outprompt-font-color: #bf5b3d;

  /* Notebook specific styles */

  --jp-notebook-padding: 10px;
  --jp-notebook-select-background: var(--jp-layout-color1);
  --jp-notebook-multiselected-color: var(--md-blue-50);

  /* The scroll padding is calculated to fill enough space at the bottom of the
  notebook to show one single-line cell (with appropriate padding) at the top
  when the notebook is scrolled all the way to the bottom. We also subtract one
  pixel so that no scrollbar appears if we have just one single-line cell in the
  notebook. This padding is to enable a 'scroll past end' feature in a notebook.
  */
  --jp-notebook-scroll-padding: calc(
    100% - var(--jp-code-font-size) * var(--jp-code-line-height) -
      var(--jp-code-padding) - var(--jp-cell-padding) - 1px
  );

  /* Rendermime styles */

  --jp-rendermime-error-background: #fdd;
  --jp-rendermime-table-row-background: var(--md-grey-100);
  --jp-rendermime-table-row-hover-background: var(--md-light-blue-50);

  /* Dialog specific styles */

  --jp-dialog-background: rgba(0, 0, 0, 0.25);

  /* Console specific styles */

  --jp-console-padding: 10px;

  /* Toolbar specific styles */

  --jp-toolbar-border-color: var(--jp-border-color1);
  --jp-toolbar-micro-height: 8px;
  --jp-toolbar-background: var(--jp-layout-color1);
  --jp-toolbar-box-shadow: 0 0 2px 0 rgba(0, 0, 0, 0.24);
  --jp-toolbar-header-margin: 4px 4px 0 4px;
  --jp-toolbar-active-background: var(--md-grey-300);

  /* Statusbar specific styles */

  --jp-statusbar-height: 24px;

  /* Input field styles */

  --jp-input-box-shadow: inset 0 0 2px var(--md-blue-300);
  --jp-input-active-background: var(--jp-layout-color1);
  --jp-input-hover-background: var(--jp-layout-color1);
  --jp-input-background: var(--md-grey-100);
  --jp-input-border-color: var(--jp-inverse-border-color);
  --jp-input-active-border-color: var(--jp-brand-color1);
  --jp-input-active-box-shadow-color: rgba(19, 124, 189, 0.3);

  /* General editor styles */

  --jp-editor-selected-background: #d9d9d9;
  --jp-editor-selected-focused-background: #d7d4f0;
  --jp-editor-cursor-color: var(--jp-ui-font-color0);

  /* Code mirror specific styles */

  --jp-mirror-editor-keyword-color: #008000;
  --jp-mirror-editor-atom-color: #88f;
  --jp-mirror-editor-number-color: #080;
  --jp-mirror-editor-def-color: #00f;
  --jp-mirror-editor-variable-color: var(--md-grey-900);
  --jp-mirror-editor-variable-2-color: rgb(0, 54, 109);
  --jp-mirror-editor-variable-3-color: #085;
  --jp-mirror-editor-punctuation-color: #05a;
  --jp-mirror-editor-property-color: #05a;
  --jp-mirror-editor-operator-color: #a2f;
  --jp-mirror-editor-comment-color: #408080;
  --jp-mirror-editor-string-color: #ba2121;
  --jp-mirror-editor-string-2-color: #708;
  --jp-mirror-editor-meta-color: #a2f;
  --jp-mirror-editor-qualifier-color: #555;
  --jp-mirror-editor-builtin-color: #008000;
  --jp-mirror-editor-bracket-color: #997;
  --jp-mirror-editor-tag-color: #170;
  --jp-mirror-editor-attribute-color: #00c;
  --jp-mirror-editor-header-color: blue;
  --jp-mirror-editor-quote-color: #090;
  --jp-mirror-editor-link-color: #00c;
  --jp-mirror-editor-error-color: #f00;
  --jp-mirror-editor-hr-color: #999;

  /*
    RTC user specific colors.
    These colors are used for the cursor, username in the editor,
    and the icon of the user.
  */

  --jp-collaborator-color1: #ffad8e;
  --jp-collaborator-color2: #dac83d;
  --jp-collaborator-color3: #72dd76;
  --jp-collaborator-color4: #00e4d0;
  --jp-collaborator-color5: #45d4ff;
  --jp-collaborator-color6: #e2b1ff;
  --jp-collaborator-color7: #ff9de6;

  /* Vega extension styles */

  --jp-vega-background: white;

  /* Sidebar-related styles */

  --jp-sidebar-min-width: 250px;

  /* Search-related styles */

  --jp-search-toggle-off-opacity: 0.5;
  --jp-search-toggle-hover-opacity: 0.8;
  --jp-search-toggle-on-opacity: 1;
  --jp-search-selected-match-background-color: rgb(245, 200, 0);
  --jp-search-selected-match-color: black;
  --jp-search-unselected-match-background-color: var(
    --jp-inverse-layout-color0
  );
  --jp-search-unselected-match-color: var(--jp-ui-inverse-font-color0);

  /* Icon colors that work well with light or dark backgrounds */
  --jp-icon-contrast-color0: var(--md-purple-600);
  --jp-icon-contrast-color1: var(--md-green-600);
  --jp-icon-contrast-color2: var(--md-pink-600);
  --jp-icon-contrast-color3: var(--md-blue-600);

  /* Button colors */
  --jp-accept-color-normal: var(--md-blue-700);
  --jp-accept-color-hover: var(--md-blue-800);
  --jp-accept-color-active: var(--md-blue-900);
  --jp-warn-color-normal: var(--md-red-700);
  --jp-warn-color-hover: var(--md-red-800);
  --jp-warn-color-active: var(--md-red-900);
  --jp-reject-color-normal: var(--md-grey-600);
  --jp-reject-color-hover: var(--md-grey-700);
  --jp-reject-color-active: var(--md-grey-800);

  /* File or activity icons and switch semantic variables */
  --jp-jupyter-icon-color: #f37626;
  --jp-notebook-icon-color: #f37626;
  --jp-json-icon-color: var(--md-orange-700);
  --jp-console-icon-background-color: var(--md-blue-700);
  --jp-console-icon-color: white;
  --jp-terminal-icon-background-color: var(--md-grey-800);
  --jp-terminal-icon-color: var(--md-grey-200);
  --jp-text-editor-icon-color: var(--md-grey-700);
  --jp-inspector-icon-color: var(--md-grey-700);
  --jp-switch-color: var(--md-grey-400);
  --jp-switch-true-position-color: var(--md-orange-900);
}
</style>
<style type="text/css">
/* Force rendering true colors when outputing to pdf */
* {
  -webkit-print-color-adjust: exact;
}

/* Misc */
a.anchor-link {
  display: none;
}

/* Input area styling */
.jp-InputArea {
  overflow: hidden;
}

.jp-InputArea-editor {
  overflow: hidden;
}

.cm-editor.cm-s-jupyter .highlight pre {
/* weird, but --jp-code-padding defined to be 5px but 4px horizontal padding is hardcoded for pre.cm-line */
  padding: var(--jp-code-padding) 4px;
  margin: 0;

  font-family: inherit;
  font-size: inherit;
  line-height: inherit;
  color: inherit;

}

.jp-OutputArea-output pre {
  line-height: inherit;
  font-family: inherit;
}

.jp-RenderedText pre {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-code-font-size);
}

/* Hiding the collapser by default */
.jp-Collapser {
  display: none;
}

@page {
    margin: 0.5in; /* Margin for each printed piece of paper */
}

@media print {
  .jp-Cell-inputWrapper,
  .jp-Cell-outputWrapper {
    display: block;
  }
}
</style>
<!-- Load mathjax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML-full,Safe"> </script>
<!-- MathJax configuration -->
<script type="text/x-mathjax-config">
    init_mathjax = function() {
        if (window.MathJax) {
        // MathJax loaded
            MathJax.Hub.Config({
                TeX: {
                    equationNumbers: {
                    autoNumber: "AMS",
                    useLabelIds: true
                    }
                },
                tex2jax: {
                    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                    processEscapes: true,
                    processEnvironments: true
                },
                displayAlign: 'center',
                messageStyle: 'none',
                CommonHTML: {
                    linebreaks: {
                    automatic: true
                    }
                }
            });

            MathJax.Hub.Queue(["Typeset", MathJax.Hub]);
        }
    }
    init_mathjax();
    </script>
<!-- End of mathjax configuration --><script type="module">
  document.addEventListener("DOMContentLoaded", async () => {
    const diagrams = document.querySelectorAll(".jp-Mermaid > pre.mermaid");
    // do not load mermaidjs if not needed
    if (!diagrams.length) {
      return;
    }
    const mermaid = (await import("https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.7.0/mermaid.esm.min.mjs")).default;
    const parser = new DOMParser();

    mermaid.initialize({
      maxTextSize: 100000,
      maxEdges: 100000,
      startOnLoad: false,
      fontFamily: window
        .getComputedStyle(document.body)
        .getPropertyValue("--jp-ui-font-family"),
      theme: document.querySelector("body[data-jp-theme-light='true']")
        ? "default"
        : "dark",
    });

    let _nextMermaidId = 0;

    function makeMermaidImage(svg) {
      const img = document.createElement("img");
      const doc = parser.parseFromString(svg, "image/svg+xml");
      const svgEl = doc.querySelector("svg");
      const { maxWidth } = svgEl?.style || {};
      const firstTitle = doc.querySelector("title");
      const firstDesc = doc.querySelector("desc");

      img.setAttribute("src", `data:image/svg+xml,${encodeURIComponent(svg)}`);
      if (maxWidth) {
        img.width = parseInt(maxWidth);
      }
      if (firstTitle) {
        img.setAttribute("alt", firstTitle.textContent);
      }
      if (firstDesc) {
        const caption = document.createElement("figcaption");
        caption.className = "sr-only";
        caption.textContent = firstDesc.textContent;
        return [img, caption];
      }
      return [img];
    }

    async function makeMermaidError(text) {
      let errorMessage = "";
      try {
        await mermaid.parse(text);
      } catch (err) {
        errorMessage = `${err}`;
      }

      const result = document.createElement("details");
      result.className = 'jp-RenderedMermaid-Details';
      const summary = document.createElement("summary");
      summary.className = 'jp-RenderedMermaid-Summary';
      const pre = document.createElement("pre");
      const code = document.createElement("code");
      code.innerText = text;
      pre.appendChild(code);
      summary.appendChild(pre);
      result.appendChild(summary);

      const warning = document.createElement("pre");
      warning.innerText = errorMessage;
      result.appendChild(warning);
      return [result];
    }

    async function renderOneMarmaid(src) {
      const id = `jp-mermaid-${_nextMermaidId++}`;
      const parent = src.parentNode;
      let raw = src.textContent.trim();
      const el = document.createElement("div");
      el.style.visibility = "hidden";
      document.body.appendChild(el);
      let results = null;
      let output = null;
      try {
        let { svg } = await mermaid.render(id, raw, el);
        svg = cleanMermaidSvg(svg);
        results = makeMermaidImage(svg);
        output = document.createElement("figure");
        results.map(output.appendChild, output);
      } catch (err) {
        parent.classList.add("jp-mod-warning");
        results = await makeMermaidError(raw);
        output = results[0];
      } finally {
        el.remove();
      }
      parent.classList.add("jp-RenderedMermaid");
      parent.appendChild(output);
    }


    /**
     * Post-process to ensure mermaid diagrams contain only valid SVG and XHTML.
     */
    function cleanMermaidSvg(svg) {
      return svg.replace(RE_VOID_ELEMENT, replaceVoidElement);
    }


    /**
     * A regular expression for all void elements, which may include attributes and
     * a slash.
     *
     * @see https://developer.mozilla.org/en-US/docs/Glossary/Void_element
     *
     * Of these, only `<br>` is generated by Mermaid in place of `\n`,
     * but _any_ "malformed" tag will break the SVG rendering entirely.
     */
    const RE_VOID_ELEMENT =
      /<\s*(area|base|br|col|embed|hr|img|input|link|meta|param|source|track|wbr)\s*([^>]*?)\s*>/gi;

    /**
     * Ensure a void element is closed with a slash, preserving any attributes.
     */
    function replaceVoidElement(match, tag, rest) {
      rest = rest.trim();
      if (!rest.endsWith('/')) {
        rest = `${rest} /`;
      }
      return `<${tag} ${rest}>`;
    }

    void Promise.all([...diagrams].map(renderOneMarmaid));
  });
</script>
<style>
  .jp-Mermaid:not(.jp-RenderedMermaid) {
    display: none;
  }

  .jp-RenderedMermaid {
    overflow: auto;
    display: flex;
  }

  .jp-RenderedMermaid.jp-mod-warning {
    width: auto;
    padding: 0.5em;
    margin-top: 0.5em;
    border: var(--jp-border-width) solid var(--jp-warn-color2);
    border-radius: var(--jp-border-radius);
    color: var(--jp-ui-font-color1);
    font-size: var(--jp-ui-font-size1);
    white-space: pre-wrap;
    word-wrap: break-word;
  }

  .jp-RenderedMermaid figure {
    margin: 0;
    overflow: auto;
    max-width: 100%;
  }

  .jp-RenderedMermaid img {
    max-width: 100%;
  }

  .jp-RenderedMermaid-Details > pre {
    margin-top: 1em;
  }

  .jp-RenderedMermaid-Summary {
    color: var(--jp-warn-color2);
  }

  .jp-RenderedMermaid:not(.jp-mod-warning) pre {
    display: none;
  }

  .jp-RenderedMermaid-Summary > pre {
    display: inline-block;
    white-space: normal;
  }
</style>
<!-- End of mermaid configuration --></head>
<body class="jp-Notebook" data-jp-theme-light="true" data-jp-theme-name="JupyterLab Light">
<main>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=09220305-7373-4de1-bca2-96696f041e27">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h1 id="Agenda---Data-Analytics-Life-Cycle">Agenda - Data Analytics Life Cycle<a class="anchor-link" href="#Agenda---Data-Analytics-Life-Cycle"></a></h1><p>There are five relevant phases of the data analytics life cycle that are pertinent to this performance assessment.</p>
<ol>
<li>Discovery Phase</li>
<li>Data Acquisition</li>
<li>Data Cleaning &amp; Exploratory Data Analysis</li>
<li>Data Mining / Machine Learning</li>
<li>Reporting</li>
</ol>
<p>Each of these phases will be talked about in detail in this document.</p>
<p>Please note that <em><strong>bolded and italicized letters</strong></em> correspond to the WGU grading rubric.</p>
<h1 id="Discovery-Phase">Discovery Phase<a class="anchor-link" href="#Discovery-Phase"></a></h1><h3 id="Research-Question---A">Research Question - <em><strong>A</strong></em><a class="anchor-link" href="#Research-Question---A"></a></h3><p>Using historical market stock prices and technical indicators (RSI, MACD, MFI), how accurately can a neural network model, specifically an LSTM-based Recurrent Neural Network, predict stock price movements after the occurrence of a bullish candlestick pattern (e.g., 1 day, 3 days, 5 days, 10 days, and 15 days afterwards)? Additionally, how does the performance differ when using binary classification versus regression for price prediction?</p>
<p>Note: I will only be identifying <strong>bullish candlestick</strong> patterns for this project, as these patterns are typically used to signal potential upward price movements. I am not identifying bearish candlestick patterns to save resources as well as I am personally more interested in upward price movement. These candlestick patterns that I will be identifying are shown in the "Data Definitions" section of this project.</p>
<h3 id="Short-Summary:-Benefits-to-Stock-Traders">Short Summary: Benefits to Stock Traders<a class="anchor-link" href="#Short-Summary:-Benefits-to-Stock-Traders"></a></h3><p>Using a neural network model like LSTM to predict stock price movements can benefit traders by providing more accurate forecasts based on historical stock price data and technical indicators like RSI, MACD, and MFI. By combining bullish candlestick patterns with these additional indicators, traders can make better-informed decisions instead of relying solely on pattern analysis. LSTM models excel at capturing long-term trends, making them ideal for stock price predictions. By using an LSTM, this method should enhance prediction reliability, improving investor confidence in anticipating price movements and make more profitable trades. From our data analysis we should also be able to recognize which specific candlestick patterns yield more accurate stock price predictions.</p>
<h3 id="Long-Summary:-Benefits-to-Stock-Traders">Long Summary: Benefits to Stock Traders<a class="anchor-link" href="#Long-Summary:-Benefits-to-Stock-Traders"></a></h3><p>My research question involves using a neural network model, specifically an LSTM-based Recurrent Neural Network (RNN), to predict stock price movements after a bullish candlestick pattern by analyzing historical market data and technical indicators like the Relative Strength Index (RSI), Moving Average Convergence Divergence (MACD), and Money Flow Index (MFI). This question benefits from data analysis because financial markets are complex and influenced by many factors that interact in unpredictable ways. Stock prices are impacted by a mix of past price movements, investor behavior, market trends, and technical indicators, making it difficult to predict future movements with simple models. By using data analysis and machine learning, we can uncover patterns in large amounts of historical data and understand how these different factors interact with one another, potentially leading to more accurate predictions.</p>
<p>Candlestick patterns are widely used by traders to identify trends and predict future price movements. However, analyzing candlestick patterns in isolation may not provide the most reliable predictions. By incorporating additional data, such as historical stock prices and technical indicators like RSI, MACD, and MFI, we can potentially strengthen the accuracy of these predictions. Data analysis allows us to test how well these combined factorscandlestick patterns along with RSI, MACD, and MFIhelp forecast future price changes. This data-driven approach goes beyond subjective interpretation, enabling the model to detect patterns and relationships in the data that may not be obvious to human analysts. By isolating specific candlestick patterns and training an LSTM on each of my chosen candlestick patterns, we can determine which of these candlestick patterns yield the best stock price predictions.</p>
<p>Stock price prediction is a time-series problem, and LSTMs are well-suited for handling such data because they are designed to track long-term trends over time. In my case, I have a specific time-series problem that can be considered as a pattern-based prediction problem where Im identifying specific events (candlestick patterns) and then predicting the immediate future based on these events. Data analysis helps fine-tune the LSTM model to improve its performance, ensuring it can make accurate predictions on new, unseen data. Additionally, comparing different approachessuch as binary classification (predicting whether the price will go up or down) and regression (predicting the exact price change)requires rigorous analysis. By evaluating model performance through metrics like accuracy for classification or mean squared error for regression, data analysis enables an objective comparison of which approach provides more reliable predictions.</p>
<p>In summary, data analysis is essential in this research because it allows us to better understand how candlestick patterns, when combined with historical stock prices and technical indicators like RSI, MACD, and MFI, can help predict stock price movements. By leveraging machine learning models like LSTMs, we can uncover complex patterns in the data, leading to more informed and reliable predictions. This data-driven approach provides a more objective and accurate way to forecast price movements than relying on intuition or subjective judgment.</p>
<h3 id="Hypothesis">Hypothesis<a class="anchor-link" href="#Hypothesis"></a></h3><p>After performing our data analysis, we should be able to conclude either the null or alternative hypothesis. The null hypothesis is the default assumption, stating that our independent variables (past stock price [low, high, open, close], RSI, MACD, MFI) have no effect, no relationship, or cause no change, to our dependent variable (future stock price).</p>
<p><strong>Null Hypothesis</strong>: The LSTM-based Recurrent Neural Network model does not significantly predict stock price movements (up or down) better than random chance when using historical market stock prices and technical indicators (RSI, MACD, MFI) for prediction.</p>
<p><strong>Alternative Hypothesis</strong>: The LSTM-based Recurrent Neural Network model significantly predicts stock price movements (up or down) better than random chance when using historical market stock prices and technical indicators (RSI, MACD, MFI) for prediction.</p>
<h3 id="Data-Definitions">Data Definitions<a class="anchor-link" href="#Data-Definitions"></a></h3><p><strong>1) Candlestick Pattern</strong></p>
<p>Charting technique used in technical analysis which helps traders identify bullish (belief that the price of a stock will rise) or bearish (belief that the price of a stock will decrease) patterns. Candlestick patterns are known to show the favored direction of a stock's price, but it is not guaranteed.</p>
<p>In the case of this project, it represents price movements of a stock over a specific time period. For example, in this project we will use daily candlesticks, this means each candlestick will provide the following information:</p>
<ul>
<li>Low: The lowest stock price reached during this day</li>
<li>High: The highest stock price reached during this day</li>
<li>Open: The stock price at the beginning of the day</li>
<li>Close: The stock price at the close of the day</li>
</ul>
<p>Below is an example of a bearish candle (left) and bullish candle (right) and how to interpet them:</p>
<div style="text-align: center;">
<img alt="No description has been provided for this image" src="C:/Users/james/Documents/WGU/Course-D214-MSDA Capstone/Screenshots/candle_example.png"/>
</div>
<p>There are many types of candlestick patterns, however as mentioned earlier in my project I will only identify bullish candlestick patterns. Specifically, I will identify five of the most popular candlestick patterns to save resources, specifically time. The following are the candlestick patterns that I will use for my data analysis and exactly how I will identify each of them:</p>
<ol>
<li>Hammer<ul>
<li>Description: A single candlestick with a small body at the top and a long lower shadow. It appears at the bottom of a downtrend.</li>
<li>Significance: Shows that despite strong selling pressure, buyers stepped in, potentially signaling a reversal.</li>
<li>How I'm identifying this pattern: Looking for a bullish or bearish candlestick with a small body (body is less than 30% of the total candlestick length; candlestick length is the distance between low and high), with the lower shadow (the distance between the low and the open for a bullish candle, or the low and the close for a bearish candle) being at least two times the length of the body. There also must be little to no upper shadow (upper shadow will be less than 10% of the total candle length). This pattern must occur during a downtrend; to confirm a downtrend, I will fit a regression line (line of best fit) through the closing prices for the previous five candles-if the slope is negative or equal to zero, this means that a downtrend is present. It doesn't matter if the single candlestick is a bearish or bullish candle, they both can display the hammer pattern. The images below are visual representations of exactly how I am going to identify the hammer pattern as highlighted with blue dots; it doesn't matter if the single candlestick is a bearish or bullish candle, they both display the hammer pattern. In the images below, although it shows that there is a decently sized upper shadow for the hammer pattern, this is incorrect as it should be less than 10% of the candle's total length.</li>
</ul>
</li>
</ol>
<div style="text-align: center;">
<img alt="No description has been provided for this image" src="C:/Users/james/Documents/WGU/Course-D214-MSDA Capstone/Screenshots/Hammer_wtih_bearish_candle.png"/>
</div>
<div style="text-align: center;">
<img alt="No description has been provided for this image" src="C:/Users/james/Documents/WGU/Course-D214-MSDA Capstone/Screenshots/Hammer_wtih_bullish_candle.png"/>
</div>
<ol start="2">
<li>Inverted Hammer<ul>
<li>Description: A single candlestick with a small body near the bottom, a long upper shadow, and little to no lower shadow. This pattern appears at the bottom of a downtrend.</li>
<li>Significance: Shows that bulls may be gaining control, though confirmation from the next candle is needed.</li>
<li>How I'm identifying this pattern: Looking for a candlestick (bullish or bearish) with a small body (less than 30% of the total candlestick length; candlestick length is the distance between low and high) at the bottom, a long upper shadow (at least twice the length of the body), and little to no lower shadow (lower shadow will be less than 10% of the total candle length). This pattern must occur during a downtrend; to confirm a downtrend, I will fit a regression line (line of best fit) through the closing prices for the previous five candles-if the slope is negative or equal to zero, this means that a downtrend is present. The images below are visual representations of exactly how I am going to identify the inverted hammer pattern as highlighted with blue dots; it doesn't matter if the single candlestick is a bearish or bullish candle, they both display the inverted hammer pattern.</li>
</ul>
</li>
</ol>
<div style="text-align: center;">
<img alt="No description has been provided for this image" src="C:/Users/james/Documents/WGU/Course-D214-MSDA Capstone/Screenshots/Inverted_Hammer_wtih_bearish_candle.png"/>
</div>
<div style="text-align: center;">
<img alt="No description has been provided for this image" src="C:/Users/james/Documents/WGU/Course-D214-MSDA Capstone/Screenshots/Inverted_Hammer_wtih_bullish_candle.png"/>
</div>
<ol start="3">
<li>Bullish Engulfing Pattern<ul>
<li>Description: A two-candle pattern where a small bearish candle is followed by a larger bullish candle that completely engulfs the previous one.</li>
<li>Significance: A strong signal of a shift from bearish to bullish sentiment, often indicating a reversal.</li>
<li>How I'm identifying this pattern: Looking for a small bearish candle followed by a large bullish candle that completely engulfs the range (high and low prices) of the first candle (two-candle pattern). The second candle's body (open and close price) must be larger by at least 2 times the body of the first candle and fully engulf the body of the first candle. This pattern must occur during a downtrend; to confirm a downtrend, I will fit a regression line (line of best fit) through the closing prices for the previous five candles-if the slope is negative or equal to zero, this means that a downtrend is present. Note that the first candle (the bearish candle) in the identified pattern will be treated as the fifth candle in the slope calculation for confirming the downtrend. The image below is a visual representation of exactly how I am going to identify the bullish engulfing pattern as highlighted with blue dots.</li>
</ul>
</li>
</ol>
<div style="text-align: center;">
<img alt="No description has been provided for this image" src="C:/Users/james/Documents/WGU/Course-D214-MSDA Capstone/Screenshots/Bullish_engulfing.png"/>
</div>
<ol start="4">
<li>Bullish Harami<ul>
<li>Description: A two-candle pattern where a small bullish candle is contained entirely within the range of the previous large bearish candle.</li>
<li>Significance: A sign of potential reversal, suggesting that selling pressure is weakening and buyers are starting to take control.</li>
<li>How I'm identifying this pattern: Looking for a large bearish candle followed by a small bullish candle that is entirely within the range of the first candle; this means that the small bullish candle should be fully contained within the previous candle's high and low. The large bearish candle will be at least twice the entire length of the following bullish candle. The body of the large bearish candle will also completely engulf the body of the small bullish candle. This pattern must occur during a downtrend; to confirm a downtrend, I will fit a regression line (line of best fit) through the closing prices for the previous five candles-if the slope is negative or equal to zero, this means that a downtrend is present. Note that the first candle (the bearish candle) in the identified pattern will be treated as the fifth candle in the slope calculation for confirming the downtrend. The image below is a visual representation of exactly how I am going to identify the bullish harami pattern as highlighted with blue dots.</li>
</ul>
</li>
</ol>
<div style="text-align: center;">
<img alt="No description has been provided for this image" src="C:/Users/james/Documents/WGU/Course-D214-MSDA Capstone/Screenshots/Bullish_harami.png"/>
</div>
<ol start="5">
<li>Three White Soldiers<ul>
<li>Description: A three-candle pattern consisting of three consecutive long bullish candles that close progressively higher.</li>
<li>Significance: A strong bullish signal, indicating a powerful upward trend and a continuation of the previous bullish move.</li>
<li>How I'm identifying this pattern: Looking for three consecutive bullish candles with each one closing higher than the previous candle. The candles should show a steady upward movement without large wicks. The upper and lower wicks should each be no more than 20% of the total candle length. Unlike the other patterns, this pattern does not need to occur during a downtrend. The visual below is a representation of how I am going to identify the three white soliders pattern as highlighted with blue dots.</li>
</ul>
</li>
</ol>
<div style="text-align: center;">
<img alt="No description has been provided for this image" src="C:/Users/james/Documents/WGU/Course-D214-MSDA Capstone/Screenshots/Three_white_soldiers.png"/>
</div>
<p>(Note: The way I am identifying these candlestick patterns is highly specific. This approach ensures consistency and a systematic method for recognizing patterns, which leads to more reliable and repeatable results when analyzing trends. While the specifications may differ in certain details, they still encompass the core definition of each candlestick pattern. Some examples of these adjustments include:</p>
<ul>
<li>The requirement for identifying a downtrend, using the slope of the 5 previous candles' closing prices. If the slope is negative, it confirms a downtrend.</li>
<li>For example, just looking at the hammer pattern, the lower shadow (the distance between the low and the open for a bullish candle, or the low and the close for a bearish candle) must be at least twice the length of the body.</li>
<li>Various other specific calculations for each pattern, ensuring that all conditions for pattern identification are met with precision.</li>
</ul>
<p>These modifications are designed to provide a consistent, clear, and quantifiable framework for candlestick pattern identification, which enhances the accuracy of trend analysis.)</p>
<p><strong>2) Stock Price</strong></p>
<p>Stock price is the current market value or price of a single share of a company's stock. Each stock has it's own ticker symbol, for example, Microsoft's ticker symbol is "MSFT".</p>
<p>I will be using four stock price associated independent variables for my analysis. My independent variables for stock price are daily variables, meaning they are recorded once daily. These variables are as follows:</p>
<ol>
<li>Low: The lowest stock price reached during this day</li>
<li>High: The highest stock price reached during this day</li>
<li>Open: The stock price at the beginning of the day</li>
<li>Close: The stock price at the close of the day</li>
</ol>
<p>My sole dependent variable for this analysis is also a stock price variable. It can be explained two different ways depending on the problem I am trying to solve:</p>
<ol>
<li>Binary Classification: Predicting if the closing price will go 'up' or 'down' for multiple future time periods (1 day, 3 days, 5 days, 10 days, and 15 days), compared to the closing price from the last candle of the identified candlestick pattern.</li>
<li>Regression: Predicting the exact closing price (a continuous value) for multiple future time periods (1 day, 3 days, 5 days, 10 days, and 15 days), after the identified candlestick pattern.</li>
</ol>
<p><strong>3) Relative Strength Index (RSI)</strong></p>
<p>RSI will be another of my independent variables for this analysis. RSI is a momentum oscillator; a momentum oscillator is a type of technical indicator that reflects the rate at which a stock's price is moving. It helps traders determine whether a price movement is strengthening or weakening.</p>
<p>RSI is calculated on a scale of 0 to 100 and helps identify potential overbought or oversold conditions in the market. When the RSI rises above 70, it indicates that an asset may be overbought, suggesting a potential reversal or pullback. Conversely, when the RSI falls below 30, it suggests the asset may be oversold and due for a rebound.</p>
<div style="text-align: center;">
<img alt="No description has been provided for this image" src="C:/Users/james/Documents/WGU/Course-D214-MSDA Capstone/Screenshots/Fidelity_RSI_70_30.png"/><p style="text-align: right;">(Fidelity, n.d., RSI: Relative Strength Index)</p>
</div>
<p>RSI is valuable not only for indicating overbought or oversold conditions but also for gauging the strength of a trend. A reading above 50 generally signals bullish momentum, while a reading below 50 suggests bearish momentum.</p>
<p>Divergences between price and RSI can also provide important insights; for example, if stock price is making new highs but RSI is not, it could indicate weakening momentum and a potential reversal. Traders often use RSI to confirm entry and exit points, helping them make more informed decisions based on current market conditions and trends.</p>
<div style="text-align: center;">
<img alt="No description has been provided for this image" src="C:/Users/james/Documents/WGU/Course-D214-MSDA Capstone/Screenshots/Fidelity_RSI_divergence.png"/><p style="text-align: right;">(Fidelity, n.d., RSI: Relative Strength Index)</p>
</div>
<p>Here is how RSI will be calculated in my analysis:</p>
<p>I will use a 14-day lookback period as it is the standard for calculating RSI, as originally proposed by J. Welles Wilder (he developed RSI) in his 1978 book <em>New Concepts in Technical Trading Systems</em>. This 14-day period is widely accepted in technical analysis and provides a balanced approach to evaluating price momentum over a short timeframe. It helps traders capture short- to medium-term trends, providing signals about the asset's momentum while still maintaining a reasonable level of reliability.</p>
<p>The following table is just an example illustration for 14 days, to help visualize how RSI is calculated.</p>
<table>
<thead>
<tr>
<th>Day</th>
<th>Closing Price</th>
<th>Change</th>
<th>Gain</th>
<th>Loss</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>100</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>2</td>
<td>102</td>
<td>+2</td>
<td>2</td>
<td>0</td>
</tr>
<tr>
<td>3</td>
<td>101</td>
<td>-1</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>4</td>
<td>103</td>
<td>+2</td>
<td>2</td>
<td>0</td>
</tr>
<tr>
<td>5</td>
<td>105</td>
<td>+2</td>
<td>2</td>
<td>0</td>
</tr>
<tr>
<td>6</td>
<td>104</td>
<td>-1</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>7</td>
<td>107</td>
<td>+3</td>
<td>3</td>
<td>0</td>
</tr>
<tr>
<td>8</td>
<td>110</td>
<td>+3</td>
<td>3</td>
<td>0</td>
</tr>
<tr>
<td>9</td>
<td>109</td>
<td>-1</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>10</td>
<td>112</td>
<td>+3</td>
<td>3</td>
<td>0</td>
</tr>
<tr>
<td>11</td>
<td>113</td>
<td>+1</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>12</td>
<td>111</td>
<td>-2</td>
<td>0</td>
<td>2</td>
</tr>
<tr>
<td>13</td>
<td>115</td>
<td>+4</td>
<td>4</td>
<td>0</td>
</tr>
<tr>
<td>14</td>
<td>116</td>
<td>+1</td>
<td>1</td>
<td>0</td>
</tr>
</tbody>
</table>
<p><strong>Step 1)</strong> Calculate average gain and average loss</p>
<ul>
<li><p>Sum the gains for the first 14 days = 2 + 0 + 2 + 2 + 0 + 3 + 3 + 0 + 3 + 1 + 0 + 4 + 1 = 21</p>
</li>
<li><p>Sum the losses for the first 14 days = 0 + 1 + 0 + 0 + 1 + 0 + 0 + 1 + 0 + 0 + 2 + 0 + 0 = 5</p>
</li>
<li><p>Average gain = 21 / 14 = 1.5</p>
</li>
<li><p>Average loss = 5 / 14 = 0.36</p>
</li>
</ul>
<br/>
<p><strong>Step 2)</strong> Calculate Relative Strength (RS)</p>
<ul>
<li>RS = Average gain / Average loss = 1.5 / 0.36 = 4.17</li>
</ul>
<br/>
<p><strong>Step 3)</strong> Calculate RSI</p>
<ul>
<li>RSI = 100 - 100 / (1 + RS) = 100 - 100 / 5.17 = 80.65</li>
</ul>
<p>Note: This (80.65) is the RSI value on the 14th day as it uses price data from day 1 to day 14. If I wanted the RSI value for the 15th day, it will be based on price data from day 2 to day 15.
<br/></p>
<p><strong>4) Money Flow Index (MFI)</strong></p>
<p>In addition to the previously mentioned technical indicators, I will also incorporate the Money Flow Index (MFI) as another important independent variable in my analysis.</p>
<p>MFI is a momentum indicator that measures the flow of money into and out of an asset over a specific period of time, typically 14 days. The MFI combines both price and volume, which helps identify overbought or oversold conditions in the market. It is calculated by comparing the typical price (average of high, low, and close) of each period to the volume of trades during that period. When the MFI is above 80, it indicates that the asset is overbought, and when it is below 20, it indicates that the asset is oversold. Traders use the MFI to confirm price trends or spot potential reversals, as extreme values of MFI can signal that an asset is about to experience a trend change.</p>
<div style="text-align: center;">
<img alt="No description has been provided for this image" src="C:/Users/james/Documents/WGU/Course-D214-MSDA Capstone/Screenshots/Fidelity_MFI.png"/><p style="text-align: right;">(Fidelity, n.d., Money Flow Index)</p>
</div>
<p>For the purposes of my analysis, I will be calculating MFI using a 14-day period, which is the standard lookback period. MFI will help me capture the relationship between price movements and volume flow, similar to Money Flow Index (MFI), but with the added complexity of considering both price and volume for identifying buying or selling pressure.</p>
<p>Below is an example of how MFI might be calculated, starting with determining the typical price for each day (High + Low + Close) / 3, then multiplying by volume, and finally applying the formula to get the MFI over a 14-day period:</p>
<table>
<thead>
<tr>
<th>Day</th>
<th>High</th>
<th>Low</th>
<th>Close</th>
<th>Volume</th>
<th>Typical Price</th>
<th>Money Flow</th>
<th>MFI Calculation</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>105</td>
<td>98</td>
<td>102</td>
<td>2000</td>
<td>(105 + 98 + 102) / 3 = 101.67</td>
<td>Positive</td>
<td>N/A (First day)</td>
</tr>
<tr>
<td>2</td>
<td>107</td>
<td>100</td>
<td>104</td>
<td>2500</td>
<td>(107 + 100 + 104) / 3 = 103.67</td>
<td>Positive</td>
<td>N/A (First 14 days)</td>
</tr>
<tr>
<td>3</td>
<td>106</td>
<td>99</td>
<td>103</td>
<td>2200</td>
<td>(106 + 99 + 103) / 3 = 102.67</td>
<td>Negative</td>
<td>N/A (First 14 days)</td>
</tr>
<tr>
<td>4</td>
<td>108</td>
<td>101</td>
<td>106</td>
<td>2400</td>
<td>(108 + 101 + 106) / 3 = 105.00</td>
<td>Positive</td>
<td>N/A (First 14 days)</td>
</tr>
<tr>
<td>5</td>
<td>110</td>
<td>104</td>
<td>107</td>
<td>2300</td>
<td>(110 + 104 + 107) / 3 = 107.00</td>
<td>Positive</td>
<td>N/A (First 14 days)</td>
</tr>
<tr>
<td>6</td>
<td>111</td>
<td>105</td>
<td>109</td>
<td>2500</td>
<td>(111 + 105 + 109) / 3 = 108.33</td>
<td>Positive</td>
<td>N/A (First 14 days)</td>
</tr>
<tr>
<td>7</td>
<td>113</td>
<td>106</td>
<td>110</td>
<td>2600</td>
<td>(113 + 106 + 110) / 3 = 109.67</td>
<td>Positive</td>
<td>N/A (First 14 days)</td>
</tr>
<tr>
<td>8</td>
<td>115</td>
<td>108</td>
<td>112</td>
<td>2700</td>
<td>(115 + 108 + 112) / 3 = 111.67</td>
<td>Positive</td>
<td>N/A (First 14 days)</td>
</tr>
<tr>
<td>9</td>
<td>116</td>
<td>109</td>
<td>113</td>
<td>2800</td>
<td>(116 + 109 + 113) / 3 = 112.67</td>
<td>Positive</td>
<td>N/A (First 14 days)</td>
</tr>
<tr>
<td>10</td>
<td>118</td>
<td>110</td>
<td>114</td>
<td>2900</td>
<td>(118 + 110 + 114) / 3 = 114.00</td>
<td>Positive</td>
<td>N/A (First 14 days)</td>
</tr>
<tr>
<td>11</td>
<td>120</td>
<td>112</td>
<td>116</td>
<td>3000</td>
<td>(120 + 112 + 116) / 3 = 116.00</td>
<td>Positive</td>
<td>N/A (First 14 days)</td>
</tr>
<tr>
<td>12</td>
<td>122</td>
<td>113</td>
<td>118</td>
<td>3100</td>
<td>(122 + 113 + 118) / 3 = 117.67</td>
<td>Positive</td>
<td>N/A (First 14 days)</td>
</tr>
<tr>
<td>13</td>
<td>124</td>
<td>115</td>
<td>119</td>
<td>3200</td>
<td>(124 + 115 + 119) / 3 = 119.33</td>
<td>Positive</td>
<td>N/A (First 14 days)</td>
</tr>
<tr>
<td>14</td>
<td>126</td>
<td>116</td>
<td>121</td>
<td>3300</td>
<td>(126 + 116 + 121) / 3 = 121.00</td>
<td>Positive</td>
<td>N/A (First 14 days)</td>
</tr>
<tr>
<td>15</td>
<td>125</td>
<td>118</td>
<td>120</td>
<td>3400</td>
<td>(125 + 118 + 120) / 3 = 121.00</td>
<td>Negative</td>
<td>MFI = (Positive Money Flow / Negative Money Flow) * 100</td>
</tr>
</tbody>
</table>
<p><strong>5) Moving Average Convergence Divergence (MACD)</strong></p>
<p>MACD will be associated with two more independent variables for my analysis, MACD line and signal line.</p>
<p>MACD is a trend-following momentum indicator that shows the relationship between two moving averages of a stocks price. First, we need to understand what moving averages are in relationship to stock price. In simple terms, a moving average calculation computes the average of a stock's prices over a specific period. This helps to smooth out short-term price fluctuations, making it easier to identify long-term trends.</p>
<p>We have different types of moving averages:</p>
<ol>
<li><p>Simple Moving Average (SMA): This is the average of a stock's price over a specific period of time. For example, a 10-day SMA is the average of the last 10 days of closing prices. It gives equal weight to each price point in the calculation.</p>
</li>
<li><p>Exponential Moving Average (EMA): The EMA is a variation of the moving average that gives more weight to recent prices, making it more sensitive to new information. This is important for capturing recent price movements and trends more quickly than the SMA, which treats all price data equally.</p>
</li>
</ol>
<p>Now that we understand moving averages, MACD is calculated by subtracting the 26-day exponential moving average (EMA) from the 12-day EMA. The resulting value is the MACD line.
The 26-day EMA reflects long term price trends which smooths out price data over a longer period while the 12-day EMA captures short term price trends. The subtraction between them resulting in the MACD line is important as it represents the difference between short and long term price trends.</p>
<p>Along with the MACD line, a signal line is also calculated. This is a 9-day EMA of the MACD line, and it helps traders identify buy and sell signals. Specifically, the signal line is calculated by taking the MACD line values for the past 9 days and applying the standard EMA formula to smooth out fluctuations in the MACD line over this period.</p>
<p>When the MACD line crosses above the signal line, it is generally considered a bullish signal, suggesting a potential upward movement in the stock price. Conversely, when the MACD line crosses below the signal line, it is viewed as a bearish signal, indicating a potential downward price movement.</p>
<div style="text-align: center;">
<img alt="No description has been provided for this image" src="C:/Users/james/Documents/WGU/Course-D214-MSDA Capstone/Screenshots/Fidelity_MACD.png"/><p style="text-align: right;">(Fidelity, n.d., MACD)</p>
</div>
<p>The MACD line also reports bullish signals when it turns up from below zero. Conversely, while it crosses below zero, it is considered bearish.</p>
<div style="text-align: center;">
<img alt="No description has been provided for this image" src="C:/Users/james/Documents/WGU/Course-D214-MSDA Capstone/Screenshots/Fidelity_MACD_above_below_zero.png"/><p style="text-align: right;">(Fidelity, n.d., MACD)</p>
</div>
<p>Here is how MACD will be calculated in my analysis, starting with the following formula to calculate EMA:</p>
<div style="text-align: center;">
<img alt="No description has been provided for this image" src="C:/Users/james/Documents/WGU/Course-D214-MSDA Capstone/Screenshots/MACD_EMA_formula.png"/>
</div>
<p>To calculate the 12-day EMA, we need to start out by calculating the 12-day SMA (simple moving average).</p>
<table>
<thead>
<tr>
<th>Day</th>
<th>Price</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>100</td>
</tr>
<tr>
<td>2</td>
<td>102</td>
</tr>
<tr>
<td>3</td>
<td>105</td>
</tr>
<tr>
<td>4</td>
<td>107</td>
</tr>
<tr>
<td>5</td>
<td>110</td>
</tr>
<tr>
<td>6</td>
<td>108</td>
</tr>
<tr>
<td>7</td>
<td>111</td>
</tr>
<tr>
<td>8</td>
<td>113</td>
</tr>
<tr>
<td>9</td>
<td>112</td>
</tr>
<tr>
<td>10</td>
<td>115</td>
</tr>
<tr>
<td>11</td>
<td>118</td>
</tr>
<tr>
<td>12</td>
<td>120</td>
</tr>
</tbody>
</table>
<p><strong>Step 1)</strong> Calculate SMA</p>
<p>12-day SMA = sum of stock prices / 12 days = (100 + 102 + 105 + 107 + 110 + 108 + 111 + 113 + 112 + 115 + 118 + 120) / 12 = 100.08</p>
<p>Although 100.08 is the SMA of those 12 days, it is also called EMA(1) or EMA1.</p>
<br/>
<p><strong>Step 2)</strong> Calculate the EMA for subsequent days using the formula</p>
<ul>
<li>For Day 2</li>
</ul>
<p>EMA(2) = (2 / (12 + 1))  (102100.08) + 100.08 = 0.1538  1.92 + 100.08 = 100.3753</p>
<ul>
<li>For Day 3</li>
</ul>
<p>EMA(3) = (2 / (12 + 1))  (105100.3753) + 100.3753 = 0.1538  4.6247 + 100.3753 = 101.0866</p>
<ul>
<li>For Day 4</li>
</ul>
<p>EMA(4) = (2 / (12 + 1))  (107101.0866) + 101.0866 = 0.1538  5.9134 + 101.0866 = 101.9961</p>
<p>Keep repeating until you get to day 12 or EMA(12); the value on this day is the 12-day EMA. When calculating 26-day EMA, the calculation is the same except the calculation will use a 26 day period.</p>
<p>Then as mentioned previously, to get the MACD value (or MACD line), just subtract the 26-day EMA from the 12-day EMA.
<br/>
<br/>
<br/></p>
<p>Now, here is how the signal line will be calculated in my analysis, the calculation for the signal line is similar as it uses a similar EMA formula. To calculate the signal line, we have to use the following formula:</p>
<div style="text-align: center;">
<img alt="No description has been provided for this image" src="C:/Users/james/Documents/WGU/Course-D214-MSDA Capstone/Screenshots/MACD_EMA_formula_signal_line.png"/>
</div>
<p>(Example data)</p>
<table>
<thead>
<tr>
<th>Day</th>
<th>26-Day EMA</th>
<th>12-Day EMA</th>
<th>MACD (12-Day EMA - 26-Day EMA)</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>0.5</td>
<td>0.5</td>
<td>0</td>
</tr>
<tr>
<td>2</td>
<td>0.515</td>
<td>0.531</td>
<td>0.016</td>
</tr>
<tr>
<td>3</td>
<td>0.520</td>
<td>0.537</td>
<td>0.017</td>
</tr>
<tr>
<td>4</td>
<td>0.531</td>
<td>0.555</td>
<td>0.024</td>
</tr>
<tr>
<td>5</td>
<td>0.541</td>
<td>0.580</td>
<td>0.039</td>
</tr>
<tr>
<td>6</td>
<td>0.563</td>
<td>0.601</td>
<td>0.038</td>
</tr>
<tr>
<td>7</td>
<td>0.584</td>
<td>0.620</td>
<td>0.036</td>
</tr>
<tr>
<td>8</td>
<td>0.602</td>
<td>0.634</td>
<td>0.032</td>
</tr>
<tr>
<td>9</td>
<td>0.620</td>
<td>0.646</td>
<td>0.026</td>
</tr>
</tbody>
</table>
<p><strong>Step 1)</strong> Calculate the 9-day SMA of MACD</p>
<p>9-day SMA = 0 + 0.016 + 0.017 + 0.024 + 0.039 + 0.038 + 0.036 + 0.032 + 0.026 / 9 = 0.0253</p>
<p>Although 0.0253 is the SMA of those 9 days, it is also called EMA(1) or EMA1 of the signal line.</p>
<br/>
<p><strong>Step 2)</strong> Apply EMA formula until we reach Day 9 which gives us the value of the 9-day EMA of the MACD Line</p>
<ul>
<li>For Day 2</li>
</ul>
<p>EMA(2) = (2 / (9 + 1))  (0.0160.0253) + 0.0253 = 0.2  (-0.0093) + 0.0253 = 0.02344</p>
<ul>
<li>For Day 3</li>
</ul>
<p>EMA(3) = (2 / (9 + 1))  (0.0170.02344) + 0.02344 = 0.2  (-0.00644
) + 0.02344 = 0.0222</p>
<ul>
<li>Repeat until Day 9 to get EMA(9) or EMA 9; this is the 9-day EMA of the MACD line, or in other words the value of the signal line.</li>
</ul>
<br/>
To summarize, MACD (or MACD Line) will be calculated using a 12-day and 26-day lookback period in my analysis by subtracting the 26-day EMA from the 12-day EMA. The signal line will be calculated as the 9-day EMA of the MACD Line. These periods are standard and widely accepted for calculating the MACD and signal line.
<br/>
<br/>
<p><strong>6) RNN (Recurrent Neural Network)</strong></p>
<p>An RNN (Recurrent Neural Network) is a type of neural network designed for processing sequences of data, such as time series, speech, or text. Unlike traditional feedforward neural networks, RNNs have connections that loop back on themselves, allowing information to persist across steps in the sequence.</p>
<p>The key feature of an RNN is its ability to maintain a hidden state that acts as memory, enabling the network to remember past information and use it to influence predictions for future inputs. This makes RNNs suitable for tasks where the order and context of data are important.</p>
<p>For example, in addition to an RNN's ability to retain and process long-term dependencies from past information, it is also capable of understanding the context and meaning of a sentence based on the order of words. This ability allows the network to distinguish between different sentence structures that might use the same words but convey entirely different meanings, such as "the mouse ate the cat" versus "the cat ate the mouse."</p>
<p>While both sentences consist of the same words, their meaning changes dramatically depending on the order of the words. An RNN processes each word sequentially (processes each word, one at a time, from left to right) and updates it hidden state to reflect which is the word that performs the action (the eating) and which is the word that is the object being eaten.</p>
<p>In my project, I am deploying a time series model, where I believe the sequence of variables plays a crucial role in predicting future prices making an RNN a robust choice for my model selection. For instance, yesterdays price action should have a stronger influence on predicting todays stock prices than price action from ten days ago.</p>
<p>Since I am using 30 days of historical data to predict future stock prices, it is important that I use a model such as an RNN which can recognize the order in which events occur significantly influences future outcomes. For instance, the price action over the last few days is more relevant for predicting today's stock price than events that happened weeks ago.</p>
<p>However, vanilla RNNs have limitations, such as difficulty learning long-term dependencies due to issues like vanishing gradients. To address this, more advanced versions of RNNs like LSTMs (Long Short-Term Memory) were developed and is better at capturing long-range dependencies in sequences.
<br/></p>
<p><strong>7) Long Short-Term Memory (LSTM) neural network</strong></p>
<p>Specifically, to answer my research question I will be using a variant of the RNN, which is called a Long Short-Term Memory (LSTM) neural network.</p>
<p>LSTMs (Long Short-Term Memory) are advanced types of RNNs that address common RNN issues like vanishing gradients. They are particularly effective for long text sequences because they retain information over longer dependencies.</p>
<p>To understand the issue of a vanishing gradient, it is important to understand what a gradient is. The gradient is the slope or the direction of deepest descent in relation to minimizing the error (cost) function. For a simplified understanding let us take the equation for univariate linear regression: y = mx + b. By getting the best fit line through our set of data points we now have 'm' (slope) and 'b' (intercept). But the values for 'm' and 'b' did not come from just anywhere - it came from minimizing the the linear regression error function. In essence the error function minimizes SSR which is the sum of the square root of the residuals (observed vs. predicted values) to get the best values for 'm' and 'b'.</p>
<p>Let us pretend the error function for linear regression was f(x) = x^2 (which it is not, but pretend it is). We can see that the error function graphs a parabola. Let us for example plug in the value where x = 4, this returns a value of 16 as 4^2 = 16. So we get the coordinates on our parabola of (4,16), which are simple (x,y) coordinates, where the value of 16 is our total error when x = 4. If we take the partial derivative of the error function, f(x) = x^2 in respect to 'x', we get the following equation: d/dx = 2x. Here the 'd/dx' just refers to the partial derivative in respect to 'x'. So now, using the same x = 4 value to plug into our partial derivative formula we get: 2 * 4 = 8. Here, the value 8 represents the slope tangent to the coordinates (4,16); or we can say that at coordinates (4,16) the value 8 represents the direction of deepest descent in relation to minimizing the error function. Specifically, the value of 8 is our gradient when x = 4. We mentioned previously that for our error function, when x = 4, y = 16, the y-value of '16' represents the total error. Now, that we have our gradient we can subtract our current 'x' value of 4 by the gradient of 8, which gives us a new value, x = -4. However, when we do that we run into a problem where the error function will never be minimized as when we plug x = -4 back into our error function it returns the same total error value of 16. This is why it is recommended to multiply the gradient by a learning rate before subtracting it from the previous 'x' value. If our learning rate = 0.5, we can multiply our gradient: 0.25 * 8 = 2. So now we can take our 'x' value of 4 and subtract it with our gradient with the learning rate applied: 4 - (0.25 * 8) = 4 - 2 = 2. The value '2' will now be plugged into our error function f(x) = x^2 -&gt; 2^2 = 4. Now we can see our total error is at '4', which is much lower than it was at '16' previously. We repeat this process iteratively, adjusting x each time, until the error is as small as possible. This in a nutshell is gradient descent. However just to clarify, in the case of linear regression, the actual error function requires computing the partial derivatives of the error function with respect to both 'm' (slope) and 'b' (intercept). These gradients are then used to minimize the error by updating both 'm' and 'b' iteratively.</p>
<p>Now that we understand what gradient descent is, neural networks rely on a process called backpropagation to adjust their weights (think of this as the slope and intercept in the previous example) based on the gradient, which indicates the direction of deepest descent in relation to minimizing error. However, in very deep networks or sequential models like Recurrent Neural Networks (RNNs), this process can lead to the vanishing gradient problem. As the gradients are passed backward through multiple layers or time steps, they are repeatedly multiplied, and if they are small values (fractions), this causes them to shrink exponentially. Eventually, the gradients become nearly zero, meaning earlier layers or time steps receive insignificant updates during training. As a result, the model struggles to learn long-term dependencies or patterns, making it difficult to capture important relationships in the data.</p>
<p>LSTM networks address this issue as their architecture consists of "cells", rather than traditional "nodes" found in other neural networks. These LSTM cells are the fundamental building blocks of LSTM networks which help address the vanishing gradient problem.</p>
<p>Each LSTM cell has a memory cell, a hidden state, and three main gates: the forget gate, the input gate, and the output gate. The memory cell stores important information over time. The forget gate decides what information in the memory cell should be discarded, helping the network forget irrelevant or outdated data. The input gate controls which new information should be added to the memory cell, allowing the LSTM to update its knowledge with new data as it comes in. The output gate determines which part of the information in the memory cell should be passed on to the hidden state, which is then passed on the the next LSTM cell.</p>
<p>By controlling the flow of information in and out of the memory cell, the LSTM can remember long-term patterns in the data while preventing the vanishing gradient problem, which helps the model learn more effectively over longer sequences.</p>
<p>I am using a sequence of 30 days of stock data (one observation), then the LSTM network will process each day of the sequence as a separate time step. For each of these time steps (daily data), there will be an LSTM cell that processes the data.</p>
<p>In my case:</p>
<ul>
<li>One obervation consists of 30 days of data (including candlestick data like open, close, high, low prices, as well as technical indicators like MFI, MACD, and RSI)</li>
<li>There are 30 time steps because each day is treated as one time step</li>
<li>There are 30 LSTM cells because one LSTM cell processes only one time step (one day)</li>
</ul>
<p>Thus, the LSTM network will process the full 30-day sequence, one day (or time step) at a time, with each LSTM cell handling a single time step in the sequence. After the LSTM processes the one time step, it passes information to the next cell (via the output gate to the hidden state, then to the next cell) in the sequence. This allows the network to capture the temporal dependencies and patterns over the 30 days of data, meaning that earlier time steps are influencing later time steps. This allows the network to learn patterns over time and remember information from previous time steps, which is crucial for time-series data like stock prices.</p>
<p>After training the model, the LSTM will make predictions about future stock price movements by leveraging the patterns it has learned from the 30 time steps (days) across all of my observations.</p>
<p>The combination of these gates allows the LSTM to maintain and manage long-term dependencies in the data, unlike traditional RNNs, which struggle with this due to the vanishing gradient problem. As the gates selectively control the flow of information, the LSTM network can "remember" important information over long periods, making it very effective for time-series data such as stock prices, where past data can influence future trends. This is particularly useful in my research on stock price prediction, where patterns and trends from previous days or months could significantly impact predictions for the next day.</p>
<p>So, that we fully understand, I am going to use a quick second example. If we have a dataset with sentences that are all 10 words long--each sentence would be an observation, the sequence is the entire sentence, so 10 words long--one time step corresponds to one word in the sequence--since I have 10 time steps (one per word), I will have 10 LSTM cells, one for each word in the sentence. So, for each sentence (observation) with 10 words, the LSTM will process the sentence word by word (one time step at a time), and each word will be processed by an individual LSTM cell, with information being passed from one cell to the next.</p>
<h3 id="Summary-of-Variables-and-Process-of-Analysis">Summary of Variables and Process of Analysis<a class="anchor-link" href="#Summary-of-Variables-and-Process-of-Analysis"></a></h3><ul>
<li><p>Independent variables (8 variables)</p>
<ul>
<li>Stock Price: Low</li>
<li>Stock Price: High</li>
<li>Stock Price: Open</li>
<li>Stock Price: Close</li>
<li>Relative Strength Index (RSI)</li>
<li>Money Flow Index (MFI)</li>
<li>Moving Average Convergence Divergence (MACD): MACD line</li>
<li>Moving Average Convergence Divergence (MACD): Signal line
<br/><br/></li>
</ul>
</li>
<li><p>Dependent variable</p>
<ul>
<li>Binary Classification: Predicting if the closing price will go 'up' or 'down' for multiple future time periods (1 day, 3 days, 5 days, 10 days, and 15 days), compared to the closing price from the last candle of the identified candlestick pattern.</li>
<li>Regression: Predicting the exact closing price (a continuous value) for multiple future time periods (1 day, 3 days, 5 days, 10 days, and 15 days), after the identified candlestick pattern.</li>
</ul>
</li>
</ul>
<br/>
As a reminder, we are going to identify these five candlestick patterns and train an LSTM on their 30-day sequences:

<ol>
<li>Hammer</li>
<li>Inverted Hammer</li>
<li>Bullish Englufing Pattern</li>
<li>Bullish Harami</li>
<li>Three White Soldiers</li>
</ol>
<p>For example, with our data of daily stock prices we will identify all "hammer" candlestick patterns using the data definition I previously defined. I am then going to lookback 30 days from the end of my identified candlestick pattern to gather all the data for my independent variables; the last candle in the candlestick pattern is the 30th day in the sequence.</p>
<p>A specific example is that I identified the last candle in a "hammer" candlestick pattern to occur on January 30th. So, since my sequence is 30 days, I will take all the data from January 1 to January 30th. On each of those days, I will gather my 8 independent variables. Each time-step represents one day, meaning I have 8 independent variables per day, and across the entire 30-day sequence, I will have a dataset with 30 time-steps, each containing 8 features. This 30-day sequence will serve as the input to the LSTM model, which will learn to recognize patterns in the time series data and predict the closing price for future days, conditioned on the identified candlestick pattern.</p>
<p>Although, some of the identified patterns are of different lengths; the hammer pattern is identified using one days's worth of data, while the three white soldiers is identified using three day's worth of data--my sequence will always incorporate only 30 days worth of data.</p>
<p>Just something to note, since my independent variable data is a 30-day sequence, it is on the 30th day of that sequence which includes the data of the last candle for the identified candlestick pattern. On the 30th day of the sequence is where I retrieve my closing prices to compare against the closing price of a future day.</p>
<p>To conclude, at the end of my analysis I will have five different trained LSTMs each using a different set of 30-day sequence data which represents a specific candlestick pattern.</p>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=b7f768ef-1b80-46b7-baa2-ab7d021a9eb4">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h1 id="Data-Acquisition---B,-C">Data Acquisition - <em>B, C</em><a class="anchor-link" href="#Data-Acquisition---B,-C"></a></h1><h3 id="Load-necessary-packages-for-data-acquisition:">Load necessary packages for data acquisition:<a class="anchor-link" href="#Load-necessary-packages-for-data-acquisition:"></a></h3><ol>
<li>yfinance: Used to access Yahoo finance data via their API</li>
<li>pandas: Used to read the finance data into a data frame and inspect data</li>
</ol>
<p>An important thing to note about the 'yfinance' package is that it automatically adjusts the stock prices for stock splits when retrieving historical data. This is great, so that I do not have to manually adjust them.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=5764891e-0c2f-4bd9-8063-e64771d8b167">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[1]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1">#load packages</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">yfinance</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">yf</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=012bd614-58d3-4c63-9442-026891c5ee86">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Preview-Data">Preview Data<a class="anchor-link" href="#Preview-Data"></a></h3><p>I am going to select the stock ticker 'SPY' to perform my initial data cleaning. Once I create my data cleaning function, I will be able to apply it to other stock tickers. However, for this project, I am selecting 'SPY' because it encompasses many stocks and represents a broad market index, making it a reliable proxy for overall market performance.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=8a476d2d-d143-40ec-8047-a28620492e04">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[2]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">ticker</span> <span class="o">=</span> <span class="s2">"SPY"</span> <span class="c1">#The stock ticker for 'S&amp;P 500'</span>
<span class="n">ticker_symbol</span> <span class="o">=</span> <span class="n">ticker</span> <span class="c1">#used for the training step in a later section as need to save ticker data</span>
<span class="n">ticker</span> <span class="o">=</span> <span class="n">yf</span><span class="o">.</span><span class="n">Ticker</span><span class="p">(</span><span class="n">ticker</span><span class="p">)</span>

<span class="c1"># Define the custom date range (start and end dates in 'YYYY-MM-DD' format)</span>
<span class="n">start_date</span> <span class="o">=</span> <span class="s2">"2000-01-01"</span>
<span class="n">end_date</span> <span class="o">=</span> <span class="s2">"2025-2-14"</span>

<span class="c1"># Fetch the historical data for the defined date range</span>
<span class="n">finance_df</span> <span class="o">=</span> <span class="n">ticker</span><span class="o">.</span><span class="n">history</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="n">start_date</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="n">end_date</span><span class="p">)</span>
<span class="c1">#finance_df = ticker.history(period="10y") #Get 10 years worth of data, #10y, #max</span>

<span class="c1">#write out CSV</span>
<span class="n">finance_df</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="sa">f</span><span class="s1">'</span><span class="si">{</span><span class="n">ticker_symbol</span><span class="si">}</span><span class="s1">_financials_output.csv'</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># `index=False` avoids writing the index column</span>

<span class="c1">#preview data</span>
<span class="n">finance_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[2]:</div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html" tabindex="0">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th></th>
<th>Open</th>
<th>High</th>
<th>Low</th>
<th>Close</th>
<th>Volume</th>
<th>Dividends</th>
<th>Stock Splits</th>
<th>Capital Gains</th>
</tr>
<tr>
<th>Date</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<th>2000-01-03 00:00:00-05:00</th>
<td>94.485455</td>
<td>94.485455</td>
<td>91.697098</td>
<td>92.692940</td>
<td>8164300</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr>
<th>2000-01-04 00:00:00-05:00</th>
<td>91.478006</td>
<td>91.816592</td>
<td>88.998361</td>
<td>89.068069</td>
<td>8089800</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr>
<th>2000-01-05 00:00:00-05:00</th>
<td>89.187560</td>
<td>90.203319</td>
<td>87.474713</td>
<td>89.227394</td>
<td>12177900</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr>
<th>2000-01-06 00:00:00-05:00</th>
<td>88.988414</td>
<td>90.183424</td>
<td>87.793404</td>
<td>87.793404</td>
<td>6227200</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr>
<th>2000-01-07 00:00:00-05:00</th>
<td>89.426569</td>
<td>92.892097</td>
<td>89.267234</td>
<td>92.892097</td>
<td>8066500</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=c1de10ed-8891-44c5-9319-af86791617e8">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h1 id="Data-Cleaning-and-Exploratory-Data-Analysis">Data Cleaning and Exploratory Data Analysis<a class="anchor-link" href="#Data-Cleaning-and-Exploratory-Data-Analysis"></a></h1><h3 id="Load-necessary-packages-for-data-acquisition:">Load necessary packages for data acquisition:<a class="anchor-link" href="#Load-necessary-packages-for-data-acquisition:"></a></h3><ol>
<li>numpy: Used for numerical computations, in my case, computations with data frames</li>
</ol>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=b9453c19-a82b-4760-a7d0-a464e64bfa61">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[3]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1">#load package</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=fd0fe9e2-b19c-4560-9d73-64ed34ff44cb">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Data-Cleaning-Process">Data Cleaning Process<a class="anchor-link" href="#Data-Cleaning-Process"></a></h3><ol>
<li><p>Identify the 'hammer' pattern</p>
</li>
<li><p>Identify the 'inverted hammer' pattern</p>
</li>
<li><p>Identify the 'bullish engulfing' pattern</p>
</li>
<li><p>Identify the 'bullish harami' pattern</p>
</li>
<li><p>Identify the 'three white soldiers' pattern</p>
</li>
<li><p>Perform Further Cleaning on Variables / Calculate Variables (RSI, MACD, MFI)</p>
</li>
</ol>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=bb714ed0-40f9-4feb-9688-862072b66a7a">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Data-Cleaning-Outcomes">Data Cleaning Outcomes<a class="anchor-link" href="#Data-Cleaning-Outcomes"></a></h3><p>Each step in this 'data cleaning outcomes' section corresponds to the numeric step in the 'data cleaning process' section.</p>
<p><strong>1) Identify the Hammer Pattern</strong></p>
<p>Remember my following definition of the hammer candlestick pattern:</p>
<p><em>Looking for a bullish or bearish candlestick with a small body (body is less than 30% of the total candlestick length; candlestick length is the distance between low and high), with the lower shadow (the distance between the low and the open for a bullish candle, or the low and the close for a bearish candle) being at least two times the length of the body. There also must be little to no upper shadow (upper shadow will be less than 10% of the total candle length). This pattern must occur during a downtrend; to confirm a downtrend, I will fit a regression line (line of best fit) through the closing prices for the previous five candles-if the slope is negative or equal to zero, this means that a downtrend is present. It doesn't matter if the single candlestick is a bearish or bullish candle, they both can display the hammer pattern.</em></p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=b2883ae2-5991-4ee1-90cb-6f073c2452af">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[4]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">finance_df</span> <span class="o">=</span> <span class="n">finance_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">'Dividends'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1">#remove 'Dividends' column</span>
<span class="n">finance_df</span> <span class="o">=</span> <span class="n">finance_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">'Stock Splits'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1">#remove 'Dividends' column</span>
<span class="n">finance_df</span> <span class="o">=</span> <span class="n">finance_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">'Capital Gains'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1">#remove 'Dividends' column</span>
<span class="n">finance_df</span><span class="p">[</span><span class="s1">'Row_index'</span><span class="p">]</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">finance_df</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="c1">#creates index column</span>
<span class="n">finance_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[4]:</div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html" tabindex="0">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th></th>
<th>Open</th>
<th>High</th>
<th>Low</th>
<th>Close</th>
<th>Volume</th>
<th>Row_index</th>
</tr>
<tr>
<th>Date</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<th>2000-01-03 00:00:00-05:00</th>
<td>94.485455</td>
<td>94.485455</td>
<td>91.697098</td>
<td>92.692940</td>
<td>8164300</td>
<td>1</td>
</tr>
<tr>
<th>2000-01-04 00:00:00-05:00</th>
<td>91.478006</td>
<td>91.816592</td>
<td>88.998361</td>
<td>89.068069</td>
<td>8089800</td>
<td>2</td>
</tr>
<tr>
<th>2000-01-05 00:00:00-05:00</th>
<td>89.187560</td>
<td>90.203319</td>
<td>87.474713</td>
<td>89.227394</td>
<td>12177900</td>
<td>3</td>
</tr>
<tr>
<th>2000-01-06 00:00:00-05:00</th>
<td>88.988414</td>
<td>90.183424</td>
<td>87.793404</td>
<td>87.793404</td>
<td>6227200</td>
<td>4</td>
</tr>
<tr>
<th>2000-01-07 00:00:00-05:00</th>
<td>89.426569</td>
<td>92.892097</td>
<td>89.267234</td>
<td>92.892097</td>
<td>8066500</td>
<td>5</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=f418e0bd-48f8-4dde-8d05-262065d90843">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[5]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1">#Creates a conditional statement based on conditions, and applies the 'choices' label.</span>
<span class="c1">#This will result in a new column 'Candle', which describes if the daily candle was 'Bullish',</span>
<span class="c1">#Bearish, or Neutral</span>
<span class="n">conditions</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">finance_df</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">finance_df</span><span class="p">[</span><span class="s1">'Open'</span><span class="p">],</span>
    <span class="n">finance_df</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">]</span> <span class="o">==</span> <span class="n">finance_df</span><span class="p">[</span><span class="s1">'Open'</span><span class="p">],</span>
    <span class="n">finance_df</span><span class="p">[</span><span class="s1">'Open'</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">finance_df</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">]</span>
<span class="p">]</span>
<span class="n">choices</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'Bullish'</span><span class="p">,</span> <span class="s1">'Neutral'</span><span class="p">,</span> <span class="s1">'Bearish'</span><span class="p">]</span>
<span class="n">finance_df</span><span class="p">[</span><span class="s1">'Candle'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">conditions</span><span class="p">,</span> <span class="n">choices</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s1">'Unknown'</span><span class="p">)</span>
<span class="n">finance_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span> 
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[5]:</div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html" tabindex="0">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th></th>
<th>Open</th>
<th>High</th>
<th>Low</th>
<th>Close</th>
<th>Volume</th>
<th>Row_index</th>
<th>Candle</th>
</tr>
<tr>
<th>Date</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<th>2000-01-03 00:00:00-05:00</th>
<td>94.485455</td>
<td>94.485455</td>
<td>91.697098</td>
<td>92.692940</td>
<td>8164300</td>
<td>1</td>
<td>Bearish</td>
</tr>
<tr>
<th>2000-01-04 00:00:00-05:00</th>
<td>91.478006</td>
<td>91.816592</td>
<td>88.998361</td>
<td>89.068069</td>
<td>8089800</td>
<td>2</td>
<td>Bearish</td>
</tr>
<tr>
<th>2000-01-05 00:00:00-05:00</th>
<td>89.187560</td>
<td>90.203319</td>
<td>87.474713</td>
<td>89.227394</td>
<td>12177900</td>
<td>3</td>
<td>Bullish</td>
</tr>
<tr>
<th>2000-01-06 00:00:00-05:00</th>
<td>88.988414</td>
<td>90.183424</td>
<td>87.793404</td>
<td>87.793404</td>
<td>6227200</td>
<td>4</td>
<td>Bearish</td>
</tr>
<tr>
<th>2000-01-07 00:00:00-05:00</th>
<td>89.426569</td>
<td>92.892097</td>
<td>89.267234</td>
<td>92.892097</td>
<td>8066500</td>
<td>5</td>
<td>Bullish</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=cd63046b-0773-411c-8ad0-9bb676b9f209">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[6]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1">#Create column 'Body_length' which calculates the body length of the </span>
<span class="c1">#candle. </span>
<span class="n">finance_df</span><span class="p">[</span><span class="s1">'Body_length'</span><span class="p">]</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">finance_df</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">]</span> <span class="o">-</span> <span class="n">finance_df</span><span class="p">[</span><span class="s1">'Open'</span><span class="p">])</span> <span class="c1">#get the absolute number</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=850dd6f9-a8fb-4e47-85bd-97e15d127825">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[7]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1">#Create column 'Lower_shadow_length' which calculates the distance between </span>
<span class="c1">#the low and the open for a bullish candle, or the low and the close for a bearish candle.</span>
<span class="n">conditions</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">finance_df</span><span class="p">[</span><span class="s1">'Candle'</span><span class="p">]</span> <span class="o">==</span> <span class="s1">'Bullish'</span><span class="p">,</span>
    <span class="n">finance_df</span><span class="p">[</span><span class="s1">'Candle'</span><span class="p">]</span> <span class="o">==</span> <span class="s1">'Neutral'</span><span class="p">,</span>
    <span class="n">finance_df</span><span class="p">[</span><span class="s1">'Candle'</span><span class="p">]</span> <span class="o">==</span> <span class="s1">'Bearish'</span>
<span class="p">]</span>
<span class="n">choices</span> <span class="o">=</span> <span class="p">[</span><span class="n">finance_df</span><span class="p">[</span><span class="s1">'Open'</span><span class="p">]</span> <span class="o">-</span> <span class="n">finance_df</span><span class="p">[</span><span class="s1">'Low'</span><span class="p">],</span> 
           <span class="n">finance_df</span><span class="p">[</span><span class="s1">'Open'</span><span class="p">]</span> <span class="o">-</span> <span class="n">finance_df</span><span class="p">[</span><span class="s1">'Low'</span><span class="p">],</span> 
           <span class="n">finance_df</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">]</span> <span class="o">-</span> <span class="n">finance_df</span><span class="p">[</span><span class="s1">'Low'</span><span class="p">]]</span>
<span class="n">finance_df</span><span class="p">[</span><span class="s1">'Lower_shadow_length'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">conditions</span><span class="p">,</span> <span class="n">choices</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=5704156e-5701-44d0-8399-bfea944f8dca">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[8]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1">#Create column 'Upper_shadow_length' which calculates the distance between </span>
<span class="c1">#the high and the close for a bullish candle, or the high and the open for a bearish candle.</span>
<span class="n">conditions</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">finance_df</span><span class="p">[</span><span class="s1">'Candle'</span><span class="p">]</span> <span class="o">==</span> <span class="s1">'Bullish'</span><span class="p">,</span>
    <span class="n">finance_df</span><span class="p">[</span><span class="s1">'Candle'</span><span class="p">]</span> <span class="o">==</span> <span class="s1">'Neutral'</span><span class="p">,</span>
    <span class="n">finance_df</span><span class="p">[</span><span class="s1">'Candle'</span><span class="p">]</span> <span class="o">==</span> <span class="s1">'Bearish'</span>
<span class="p">]</span>
<span class="n">choices</span> <span class="o">=</span> <span class="p">[</span><span class="n">finance_df</span><span class="p">[</span><span class="s1">'High'</span><span class="p">]</span> <span class="o">-</span> <span class="n">finance_df</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">],</span> 
           <span class="n">finance_df</span><span class="p">[</span><span class="s1">'High'</span><span class="p">]</span> <span class="o">-</span> <span class="n">finance_df</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">],</span> 
           <span class="n">finance_df</span><span class="p">[</span><span class="s1">'High'</span><span class="p">]</span> <span class="o">-</span> <span class="n">finance_df</span><span class="p">[</span><span class="s1">'Open'</span><span class="p">]]</span>
<span class="n">finance_df</span><span class="p">[</span><span class="s1">'Upper_shadow_length'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">conditions</span><span class="p">,</span> <span class="n">choices</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=cacfe504-b3bf-489e-b769-3293156ae248">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[9]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1">#Create column 'Total_candle_length' which calculates the distance between low and high prices</span>
<span class="n">finance_df</span><span class="p">[</span><span class="s1">'Total_candle_length'</span><span class="p">]</span> <span class="o">=</span> <span class="n">finance_df</span><span class="p">[</span><span class="s1">'High'</span><span class="p">]</span> <span class="o">-</span> <span class="n">finance_df</span><span class="p">[</span><span class="s1">'Low'</span><span class="p">]</span>
<span class="n">finance_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[9]:</div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html" tabindex="0">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th></th>
<th>Open</th>
<th>High</th>
<th>Low</th>
<th>Close</th>
<th>Volume</th>
<th>Row_index</th>
<th>Candle</th>
<th>Body_length</th>
<th>Lower_shadow_length</th>
<th>Upper_shadow_length</th>
<th>Total_candle_length</th>
</tr>
<tr>
<th>Date</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<th>2000-01-03 00:00:00-05:00</th>
<td>94.485455</td>
<td>94.485455</td>
<td>91.697098</td>
<td>92.692940</td>
<td>8164300</td>
<td>1</td>
<td>Bearish</td>
<td>1.792515</td>
<td>0.995842</td>
<td>0.000000</td>
<td>2.788357</td>
</tr>
<tr>
<th>2000-01-04 00:00:00-05:00</th>
<td>91.478006</td>
<td>91.816592</td>
<td>88.998361</td>
<td>89.068069</td>
<td>8089800</td>
<td>2</td>
<td>Bearish</td>
<td>2.409937</td>
<td>0.069709</td>
<td>0.338586</td>
<td>2.818232</td>
</tr>
<tr>
<th>2000-01-05 00:00:00-05:00</th>
<td>89.187560</td>
<td>90.203319</td>
<td>87.474713</td>
<td>89.227394</td>
<td>12177900</td>
<td>3</td>
<td>Bullish</td>
<td>0.039834</td>
<td>1.712847</td>
<td>0.975925</td>
<td>2.728606</td>
</tr>
<tr>
<th>2000-01-06 00:00:00-05:00</th>
<td>88.988414</td>
<td>90.183424</td>
<td>87.793404</td>
<td>87.793404</td>
<td>6227200</td>
<td>4</td>
<td>Bearish</td>
<td>1.195010</td>
<td>0.000000</td>
<td>1.195010</td>
<td>2.390020</td>
</tr>
<tr>
<th>2000-01-07 00:00:00-05:00</th>
<td>89.426569</td>
<td>92.892097</td>
<td>89.267234</td>
<td>92.892097</td>
<td>8066500</td>
<td>5</td>
<td>Bullish</td>
<td>3.465529</td>
<td>0.159335</td>
<td>0.000000</td>
<td>3.624863</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=2ad52c79-3d88-4a5a-8693-51f174158c34">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[10]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">hammer</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1">#initialize empty list</span>
<span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">finance_df</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
    <span class="n">start_index</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Row_index'</span><span class="p">]</span> <span class="o">-</span> <span class="mi">5</span> <span class="c1">#Get the starting index of the fifth previous candle</span>
    <span class="n">end_index</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Row_index'</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span> <span class="c1">#subtacting 1 because only getting previous 5 candlestick closing prices, not current candle's close</span>
    <span class="c1">#for example, if row['Row_index'] = 6; start_index = 6 - 5 = 1; end_index = 6 - 1 = 5 --- getting rows 1 (start_index) through 5 (end_index) which </span>
    <span class="c1">#is the previous five candles data since our current candle is the sixth candle</span>
    
    
    <span class="k">if</span> <span class="p">(</span><span class="n">start_index</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">):</span> <span class="c1">#since we are subtracting to get starting index, it will be negative numbers at first; skip these</span>
        <span class="n">hammer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"No"</span><span class="p">)</span>
        <span class="k">continue</span>
        
    <span class="n">temp_df</span> <span class="o">=</span> <span class="n">finance_df</span><span class="p">[(</span><span class="n">finance_df</span><span class="p">[</span><span class="s1">'Row_index'</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">start_index</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">finance_df</span><span class="p">[</span><span class="s1">'Row_index'</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">end_index</span><span class="p">)]</span>
    <span class="n">closing_prices</span> <span class="o">=</span> <span class="n">temp_df</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>

    <span class="c1">#Fit a regression line (line of best fit) through the closing prices; negative slope represent current downtrend</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">closing_prices</span><span class="p">))</span>
    <span class="n">slope</span><span class="p">,</span> <span class="n">intercept</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">polyfit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">closing_prices</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> 

    <span class="c1">#if slope &lt;= 0, then in a current downtrend the previous 5 days in terms of closing prices</span>
    <span class="c1">#to clarify, although slope may be zero which means no change, I will still be counting this as a downtrend</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">slope</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">):</span>
        <span class="k">if</span> <span class="p">((</span><span class="n">row</span><span class="p">[</span><span class="s1">'Lower_shadow_length'</span><span class="p">])</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s1">'Body_length'</span><span class="p">]</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)):</span> <span class="c1">#lower shadow must be at least twice as long as body length</span>
            <span class="k">if</span> <span class="p">((</span><span class="n">row</span><span class="p">[</span><span class="s1">'Upper_shadow_length'</span><span class="p">])</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s1">'Total_candle_length'</span><span class="p">]</span> <span class="o">*</span> <span class="mf">0.10</span><span class="p">)):</span> <span class="c1">#Upper shadow is less than 10% of the total candle length</span>
                <span class="k">if</span> <span class="p">((</span><span class="n">row</span><span class="p">[</span><span class="s1">'Body_length'</span><span class="p">])</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s1">'Total_candle_length'</span><span class="p">]</span> <span class="o">*</span> <span class="mf">0.30</span><span class="p">)):</span> <span class="c1">#body is less than 30% of the total candlestick length</span>
                    <span class="n">hammer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"Yes"</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">hammer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"No"</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">hammer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"No"</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">hammer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"No"</span><span class="p">)</span>
            
    <span class="k">elif</span> <span class="p">(</span><span class="n">slope</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">):</span>
        <span class="n">hammer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"No"</span><span class="p">)</span>


<span class="c1">#Create new column</span>
<span class="n">finance_df</span><span class="p">[</span><span class="s1">'Hammer_pattern'</span><span class="p">]</span> <span class="o">=</span> <span class="n">hammer</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=a0947497-66c9-4185-811f-d8bba5e517cd">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[11]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1">#We can see that we now have a new column 'Hammer_pattern' -- 'Yes' = hammer pattern is identified</span>
<span class="n">finance_df</span><span class="p">[</span><span class="n">finance_df</span><span class="p">[</span><span class="s1">'Hammer_pattern'</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"Yes"</span><span class="p">]</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[11]:</div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html" tabindex="0">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th></th>
<th>Open</th>
<th>High</th>
<th>Low</th>
<th>Close</th>
<th>Volume</th>
<th>Row_index</th>
<th>Candle</th>
<th>Body_length</th>
<th>Lower_shadow_length</th>
<th>Upper_shadow_length</th>
<th>Total_candle_length</th>
<th>Hammer_pattern</th>
</tr>
<tr>
<th>Date</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<th>2000-02-14 00:00:00-05:00</th>
<td>89.087943</td>
<td>89.087943</td>
<td>88.151852</td>
<td>88.908691</td>
<td>8528800</td>
<td>30</td>
<td>Bearish</td>
<td>0.179251</td>
<td>0.756839</td>
<td>0.000000</td>
<td>0.936091</td>
<td>Yes</td>
</tr>
<tr>
<th>2000-05-22 00:00:00-04:00</th>
<td>90.252842</td>
<td>90.392615</td>
<td>87.537270</td>
<td>89.494080</td>
<td>10839400</td>
<td>98</td>
<td>Bearish</td>
<td>0.758763</td>
<td>1.956809</td>
<td>0.139772</td>
<td>2.855344</td>
<td>Yes</td>
</tr>
<tr>
<th>2000-09-14 00:00:00-04:00</th>
<td>95.989388</td>
<td>96.029417</td>
<td>94.888592</td>
<td>95.839279</td>
<td>3397100</td>
<td>178</td>
<td>Bearish</td>
<td>0.150109</td>
<td>0.950687</td>
<td>0.040029</td>
<td>1.140825</td>
<td>Yes</td>
</tr>
<tr>
<th>2001-02-14 00:00:00-05:00</th>
<td>85.432068</td>
<td>85.432068</td>
<td>84.150432</td>
<td>85.052086</td>
<td>8400100</td>
<td>283</td>
<td>Bearish</td>
<td>0.379982</td>
<td>0.901654</td>
<td>0.000000</td>
<td>1.281636</td>
<td>Yes</td>
</tr>
<tr>
<th>2001-02-22 00:00:00-05:00</th>
<td>81.374623</td>
<td>81.496993</td>
<td>79.229964</td>
<td>81.026840</td>
<td>21281600</td>
<td>288</td>
<td>Bearish</td>
<td>0.347783</td>
<td>1.796876</td>
<td>0.122369</td>
<td>2.267028</td>
<td>Yes</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=7fea86a8-c56a-49ef-a17e-5430d0074c00">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[12]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1">#We can see how many hammer candlestick patterns are present</span>
<span class="n">finance_df</span><span class="p">[</span><span class="s1">'Hammer_pattern'</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[12]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>Hammer_pattern
No     6248
Yes      70
Name: count, dtype: int64</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=e9ee88d4-940f-4f6c-ae5f-1d15951c39b6">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p><strong>2) Identify the Inverted Hammer Pattern</strong></p>
<p>Remember my following definition of the inverted hammer candlestick pattern:</p>
<p><em>Looking for a candlestick (bullish or bearish) with a small body (less than 30% of the total candlestick length; candlestick length is the distance between low and high) at the bottom, a long upper shadow (at least twice the length of the body), and little to no lower shadow (lower shadow will be less than 10% of the total candle length). This pattern must occur during a downtrend; to confirm a downtrend, I will fit a regression line (line of best fit) through the closing prices for the previous five candles-if the slope is negative or equal to zero, this means that a downtrend is present. It doesn't matter if the single candlestick is a bearish or bullish candle, they both can display the inverted hammer pattern.</em></p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=6b9ba738-c1bd-4c63-a059-26d07f7b9f19">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[13]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">inverted_hammer</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1">#initialize empty list</span>
<span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">finance_df</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
    <span class="n">start_index</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Row_index'</span><span class="p">]</span> <span class="o">-</span> <span class="mi">5</span> <span class="c1">#Get the starting index of the fifth previous candle</span>
    <span class="n">end_index</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Row_index'</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span> <span class="c1">#subtacting 1 because only getting previous 5 candlestick closing prices, not current candle's close</span>
    
    <span class="k">if</span> <span class="p">(</span><span class="n">start_index</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">):</span> <span class="c1">#since we are subtracting to get starting index, it will be a negative number; skip these</span>
        <span class="n">inverted_hammer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"No"</span><span class="p">)</span>
        <span class="k">continue</span>
        
    
    <span class="n">temp_df</span> <span class="o">=</span> <span class="n">finance_df</span><span class="p">[(</span><span class="n">finance_df</span><span class="p">[</span><span class="s1">'Row_index'</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">start_index</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">finance_df</span><span class="p">[</span><span class="s1">'Row_index'</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">end_index</span><span class="p">)]</span>
    <span class="n">closing_prices</span> <span class="o">=</span> <span class="n">temp_df</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>

    <span class="c1">#Fit a regression line (line of best fit) through the closing prices; negative slope represents current downtrend</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">closing_prices</span><span class="p">))</span>
    <span class="n">slope</span><span class="p">,</span> <span class="n">intercept</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">polyfit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">closing_prices</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> 

    <span class="c1">#if slope &lt;= 0, then in a current downtrend the previous 5 days in terms of closing prices</span>
    <span class="c1">#to clarify, although slope may be zero which means no change, I will still be counting this as a downtrend</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">slope</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">):</span>
        <span class="k">if</span> <span class="p">((</span><span class="n">row</span><span class="p">[</span><span class="s1">'Upper_shadow_length'</span><span class="p">])</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s1">'Body_length'</span><span class="p">]</span> <span class="o">*</span> <span class="mf">2.0</span><span class="p">)):</span> <span class="c1">#upper shadow is at least twice the length of the candle body</span>
            <span class="k">if</span> <span class="p">((</span><span class="n">row</span><span class="p">[</span><span class="s1">'Lower_shadow_length'</span><span class="p">])</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s1">'Total_candle_length'</span><span class="p">]</span> <span class="o">*</span> <span class="mf">0.10</span><span class="p">)):</span> <span class="c1">#lower shadow is less than 20% of the total candle length</span>
                <span class="k">if</span> <span class="p">((</span><span class="n">row</span><span class="p">[</span><span class="s1">'Body_length'</span><span class="p">])</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s1">'Total_candle_length'</span><span class="p">]</span> <span class="o">*</span> <span class="mf">0.30</span><span class="p">)):</span> <span class="c1">#body is less than 30% of the total candlestick length</span>
                    <span class="n">inverted_hammer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"Yes"</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">inverted_hammer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"No"</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">inverted_hammer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"No"</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">inverted_hammer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"No"</span><span class="p">)</span>
                
    <span class="k">elif</span> <span class="p">(</span><span class="n">slope</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">):</span>
        <span class="n">inverted_hammer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"No"</span><span class="p">)</span>

<span class="c1">#Create new column</span>
<span class="n">finance_df</span><span class="p">[</span><span class="s1">'InvertedHammer_pattern'</span><span class="p">]</span> <span class="o">=</span> <span class="n">inverted_hammer</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=18532f39-5498-4a1d-bbe8-625f311bd3c3">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[14]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1">#We can see how many inverted hammer candlestick patterns are present</span>
<span class="n">finance_df</span><span class="p">[</span><span class="s1">'InvertedHammer_pattern'</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[14]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>InvertedHammer_pattern
No     6266
Yes      52
Name: count, dtype: int64</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=f75491ff-021d-4652-ae9f-247c6ee270c6">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p><strong>3) Identify the Bullish Engulfing Pattern</strong></p>
<p>Remember my following definition of the bullish engulfing candlestick pattern:</p>
<p><em>Looking for a small bearish candle followed by a large bullish candle that completely engulfs the range (high and low prices) of the first candle (two-candle pattern). The second candle's body (open and close price) must be larger by at least 2 times the body of the first candle and fully engulf the body of the first candle. This pattern must occur during a downtrend; to confirm a downtrend, I will fit a regression line (line of best fit) through the closing prices for the previous five candles-if the slope is negative or equal to zero, this means that a downtrend is present. Note that the first candle (the bearish candle) in the identified pattern will be treated as the fifth candle in the slope calculation for confirming the downtrend.</em></p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=047be669-faf9-4c5b-b08a-7faf68f4d6b3">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[15]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">bullish_engulfing</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1">#initialize empty list</span>
<span class="n">slope</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([])</span>
<span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">finance_df</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
    <span class="n">start_index</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Row_index'</span><span class="p">]</span> <span class="o">-</span> <span class="mi">5</span> <span class="c1">#Get the starting index of the fifth previous candle</span>
    <span class="n">end_index</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Row_index'</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span> <span class="c1">#subtacting 1 because only getting previous 5 candlestick closing prices, not current candle's close</span>
    
    <span class="k">if</span> <span class="p">(</span><span class="n">start_index</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">):</span> <span class="c1">#since we are subtracting to get starting index, it will be a negative number; skip these</span>
        <span class="n">bullish_engulfing</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"No"</span><span class="p">)</span>
        <span class="k">continue</span>
        
    <span class="c1">#get data for five previous days worth of candlestick data</span>
    <span class="n">temp_df</span> <span class="o">=</span> <span class="n">finance_df</span><span class="p">[(</span><span class="n">finance_df</span><span class="p">[</span><span class="s1">'Row_index'</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">start_index</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">finance_df</span><span class="p">[</span><span class="s1">'Row_index'</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">end_index</span><span class="p">)]</span>
    <span class="n">closing_prices</span> <span class="o">=</span> <span class="n">temp_df</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>

    <span class="c1">#check to see if the directly previous candle is a bearish candle, this is a requirement for identifying bullish engulfing pattern</span>
    <span class="n">check_df</span> <span class="o">=</span> <span class="n">finance_df</span><span class="p">[</span><span class="n">finance_df</span><span class="p">[</span><span class="s1">'Row_index'</span><span class="p">]</span> <span class="o">==</span> <span class="n">end_index</span><span class="p">]</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">check_df</span><span class="p">[</span><span class="s1">'Candle'</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="s1">'Bullish'</span><span class="p">):</span>
        <span class="n">bullish_engulfing</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"No"</span><span class="p">)</span>
        <span class="k">continue</span>
        

    <span class="c1">#Fit a regression line (line of best fit) through the closing prices; negative slope represents current downtrend</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">closing_prices</span><span class="p">))</span>
    <span class="n">slope</span><span class="p">,</span> <span class="n">intercept</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">polyfit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">closing_prices</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> 

    <span class="c1">#if slope &lt;= 0, then in a current downtrend the previous 5 days in terms of closing prices</span>
    <span class="c1">#to clarify, although slope may be zero which means no change, I will still be counting this as a downtrend.</span>
    <span class="c1">#The final candle in the two-candlestick pattern must be bullish</span>
    <span class="k">if</span> <span class="p">((</span><span class="n">slope</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s1">'Candle'</span><span class="p">]</span> <span class="o">==</span> <span class="s1">'Bullish'</span><span class="p">)):</span>
        
        <span class="c1">#Looking for a small bearish candle followed by a large bullish candle that completely engulfs the range (high and low prices) of the first candle (two-candle pattern)</span>
        <span class="k">if</span> <span class="p">((</span><span class="n">row</span><span class="p">[</span><span class="s1">'High'</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">check_df</span><span class="p">[</span><span class="s1">'High'</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s1">'Low'</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">check_df</span><span class="p">[</span><span class="s1">'Low'</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">])):</span>
        
            <span class="k">if</span> <span class="p">((</span><span class="n">row</span><span class="p">[</span><span class="s1">'Body_length'</span><span class="p">])</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="n">check_df</span><span class="p">[</span><span class="s1">'Body_length'</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mf">2.0</span><span class="p">)):</span> <span class="c1">#the second candle's body must be larger by at least 2 times the body of the previous candle</span>
                
                <span class="c1">#The second candle's body must fully engulf the body of the first candle.</span>
                <span class="c1">#Specifically, the second candle's close must be larger than the previous candle's open and the second candle's open must be less</span>
                <span class="c1">#than the second candle's close. This is because the second candle is a bullish candle and the previous candle is a bearish candle.</span>
                <span class="k">if</span> <span class="p">((</span><span class="n">row</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">check_df</span><span class="p">[</span><span class="s1">'Open'</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s1">'Open'</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">check_df</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">])):</span> 
                    <span class="n">bullish_engulfing</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"Yes"</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">bullish_engulfing</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"No"</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                    <span class="n">bullish_engulfing</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"No"</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">bullish_engulfing</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"No"</span><span class="p">)</span>
                
    <span class="k">else</span><span class="p">:</span>
        <span class="n">bullish_engulfing</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"No"</span><span class="p">)</span>

<span class="c1">#Create new column</span>
<span class="n">finance_df</span><span class="p">[</span><span class="s1">'BullishEngulfing_pattern'</span><span class="p">]</span> <span class="o">=</span> <span class="n">bullish_engulfing</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=652064ca-d1c5-438d-829a-47fb933fef0a">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[16]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1">#We can see how many bullish engulfing candlestick patterns are present</span>
<span class="n">finance_df</span><span class="p">[</span><span class="s1">'BullishEngulfing_pattern'</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[16]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>BullishEngulfing_pattern
No     6294
Yes      24
Name: count, dtype: int64</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=b9d531d5-24e4-42d1-ba54-a8980aaaa074">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p><strong>4) Identify the Bullish Harami Pattern</strong></p>
<p>Remember my following definition of the bullish harami candlestick pattern:</p>
<p><em>Looking for a large bearish candle followed by a small bullish candle that is entirely within the range of the first candle; this means that the small bullish candle should be fully contained within the previous candle's high and low. The large bearish candle will be at least twice the entire length of the following bullish candle. The body of the large bearish candle will also completely engulf the body of the small bullish candle. This pattern must occur during a downtrend; to confirm a downtrend, I will fit a regression line (line of best fit) through the closing prices for the previous five candles-if the slope is negative or equal to zero, this means that a downtrend is present. Note that the first candle (the bearish candle) in the identified pattern will be treated as the fifth candle in the slope calculation for confirming the downtrend.</em></p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=f4f428b9-4a1e-4c67-8b77-d53af14149d8">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[17]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">bullish_harami</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1">#initialize empty list</span>
<span class="n">slope</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([])</span>
<span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">finance_df</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
    <span class="n">start_index</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Row_index'</span><span class="p">]</span> <span class="o">-</span> <span class="mi">5</span> <span class="c1">#Get the starting index of the fifth previous candle</span>
    <span class="n">end_index</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Row_index'</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span> <span class="c1">#subtacting 1 because only getting previous 5 candlestick closing prices, not current candle's close</span>
    
    <span class="k">if</span> <span class="p">(</span><span class="n">start_index</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">):</span> <span class="c1">#since we are subtracting to get starting index, it will be a negative number; skip these</span>
        <span class="n">bullish_harami</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"No"</span><span class="p">)</span>
        <span class="k">continue</span>
        
    <span class="c1">#get data for five previous days worth of candlestick data</span>
    <span class="n">temp_df</span> <span class="o">=</span> <span class="n">finance_df</span><span class="p">[(</span><span class="n">finance_df</span><span class="p">[</span><span class="s1">'Row_index'</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">start_index</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">finance_df</span><span class="p">[</span><span class="s1">'Row_index'</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">end_index</span><span class="p">)]</span>
    <span class="n">closing_prices</span> <span class="o">=</span> <span class="n">temp_df</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>

    <span class="c1">#check to see if the directly previous candle is a bearish candle, this is a requirement for identifying bullish harami pattern</span>
    <span class="n">check_df</span> <span class="o">=</span> <span class="n">finance_df</span><span class="p">[</span><span class="n">finance_df</span><span class="p">[</span><span class="s1">'Row_index'</span><span class="p">]</span> <span class="o">==</span> <span class="n">end_index</span><span class="p">]</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">check_df</span><span class="p">[</span><span class="s1">'Candle'</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="s1">'Bullish'</span><span class="p">):</span>
        <span class="n">bullish_harami</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"No"</span><span class="p">)</span>
        <span class="k">continue</span>
        

    <span class="c1">#Fit a regression line (line of best fit) through the closing prices; negative slope represents current downtrend</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">closing_prices</span><span class="p">))</span>
    <span class="n">slope</span><span class="p">,</span> <span class="n">intercept</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">polyfit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">closing_prices</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> 

    <span class="c1">#if slope &lt;= 0, then in a current downtrend the previous 5 days in terms of closing prices</span>
    <span class="c1">#to clarify, although slope may be zero which means no change, I will still be counting this as a downtrend.</span>
    <span class="c1">#The final candle in the two-candlestick pattern must be bullish.</span>
    <span class="k">if</span> <span class="p">((</span><span class="n">slope</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s1">'Candle'</span><span class="p">]</span> <span class="o">==</span> <span class="s1">'Bullish'</span><span class="p">)):</span>
        <span class="k">if</span> <span class="p">((</span><span class="n">check_df</span><span class="p">[</span><span class="s1">'Total_candle_length'</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s1">'Total_candle_length'</span><span class="p">]</span> <span class="o">*</span> <span class="mf">2.0</span><span class="p">)):</span> <span class="c1">#the bearish candle will be at least twice the length of the bullish candle</span>
            
            <span class="c1">#The second candle must be fully contained within the previous candle's high and low.</span>
            <span class="k">if</span> <span class="p">((</span><span class="n">row</span><span class="p">[</span><span class="s1">'High'</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">check_df</span><span class="p">[</span><span class="s1">'High'</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s1">'Low'</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">check_df</span><span class="p">[</span><span class="s1">'Low'</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">])):</span> 

                <span class="c1">#The second candle's body must also be fully contained within the previous candle's body (close and open).</span>
                <span class="k">if</span> <span class="p">((</span><span class="n">row</span><span class="p">[</span><span class="s1">'Open'</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">check_df</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">check_df</span><span class="p">[</span><span class="s1">'Open'</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">])):</span> 
                    <span class="n">bullish_harami</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"Yes"</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">bullish_harami</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"No"</span><span class="p">)</span>
            
            <span class="k">else</span><span class="p">:</span>
                <span class="n">bullish_harami</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"No"</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
                <span class="n">bullish_harami</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"No"</span><span class="p">)</span>
                
    <span class="k">else</span><span class="p">:</span>
        <span class="n">bullish_harami</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"No"</span><span class="p">)</span>

<span class="c1">#Create new column</span>
<span class="n">finance_df</span><span class="p">[</span><span class="s1">'BullishHarami_pattern'</span><span class="p">]</span> <span class="o">=</span> <span class="n">bullish_harami</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=3ea892c9-7b42-45ab-b524-a64a8b81c381">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[18]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1">#We can see how many bullish harami candlestick patterns are present</span>
<span class="n">finance_df</span><span class="p">[</span><span class="s1">'BullishHarami_pattern'</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[18]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>BullishHarami_pattern
No     6288
Yes      30
Name: count, dtype: int64</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=57a9715b-e17c-45bc-ae24-08612c1ea4e5">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p><strong>5) Identify the Three White Soldiers Pattern</strong></p>
<p>Remember my following definition of the three white soliders candlestick pattern:</p>
<p><em>Looking for three consecutive bullish candles with each one closing higher than the previous candle. The candles should show a steady upward movement without large wicks. The upper and lower wicks should each be no more than 20% of the total candle length. Unlike the other patterns, this pattern does not need to occur during a downtrend. The visual below is a representation of how I am going to identify the three white soliders pattern as highlighted with blue dots.</em></p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=4de8e6ed-fc51-46aa-8156-c32dcc11d82c">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[19]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">three_white_soldiers</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1">#initialize empty list</span>
<span class="n">slope</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([])</span>
<span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">finance_df</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
    <span class="n">start_index</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Row_index'</span><span class="p">]</span> <span class="o">-</span> <span class="mi">2</span> <span class="c1">#Get the starting index of the second previous candle</span>
    <span class="n">end_index</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Row_index'</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span> <span class="c1">#subtacting 1 because only getting previous 2 candlestick closing prices, not current candle's close</span>
    
    <span class="k">if</span> <span class="p">(</span><span class="n">start_index</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">):</span> <span class="c1">#since we are subtracting to get starting index, it will be a negative number; skip these</span>
        <span class="n">three_white_soldiers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"No"</span><span class="p">)</span>
        <span class="k">continue</span>
        
    <span class="c1">#get data for five previous days worth of candlestick data. Check to see if previous two candles are bullish; since need three total bullish candles</span>
    <span class="c1">#for this pattern.</span>
    <span class="n">check_df</span> <span class="o">=</span> <span class="n">finance_df</span><span class="p">[(</span><span class="n">finance_df</span><span class="p">[</span><span class="s1">'Row_index'</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">start_index</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">finance_df</span><span class="p">[</span><span class="s1">'Row_index'</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">end_index</span><span class="p">)]</span>
    <span class="k">if</span> <span class="p">((</span><span class="n">check_df</span><span class="p">[</span><span class="s1">'Candle'</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="s1">'Bearish'</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">check_df</span><span class="p">[</span><span class="s1">'Candle'</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="s1">'Bearish'</span><span class="p">)):</span>
        <span class="n">three_white_soldiers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"No"</span><span class="p">)</span>
        <span class="k">continue</span>
    

    <span class="c1">#The final candle in the three-candlestick pattern must be bullish.</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s1">'Candle'</span><span class="p">]</span> <span class="o">==</span> <span class="s1">'Bullish'</span><span class="p">):</span>

        <span class="c1">#the first candle in the pattern should have upper and lower wicks no more than 20% of the total candle length</span>
        <span class="k">if</span> <span class="p">(((</span><span class="n">check_df</span><span class="p">[</span><span class="s1">'Lower_shadow_length'</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">&lt;=</span> <span class="p">(</span><span class="n">check_df</span><span class="p">[</span><span class="s1">'Total_candle_length'</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mf">0.20</span><span class="p">))</span> <span class="o">&amp;</span> <span class="p">((</span><span class="n">check_df</span><span class="p">[</span><span class="s1">'Upper_shadow_length'</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">&lt;=</span> <span class="p">(</span><span class="n">check_df</span><span class="p">[</span><span class="s1">'Total_candle_length'</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mf">0.20</span><span class="p">))):</span>

            <span class="c1">#the second candle in the pattern should have upper and lower wicks no more than 20% of the total candle length</span>
            <span class="k">if</span> <span class="p">(((</span><span class="n">check_df</span><span class="p">[</span><span class="s1">'Lower_shadow_length'</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">&lt;=</span> <span class="p">(</span><span class="n">check_df</span><span class="p">[</span><span class="s1">'Total_candle_length'</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mf">0.20</span><span class="p">))</span> <span class="o">&amp;</span> <span class="p">((</span><span class="n">check_df</span><span class="p">[</span><span class="s1">'Upper_shadow_length'</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">&lt;=</span> <span class="p">(</span><span class="n">check_df</span><span class="p">[</span><span class="s1">'Total_candle_length'</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mf">0.20</span><span class="p">))):</span> 
                
                <span class="c1">#The three candles should have increasing closing prices</span>
                <span class="k">if</span> <span class="p">((</span><span class="n">row</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">]</span> <span class="o">&gt;</span> <span class="p">(</span><span class="n">check_df</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span> <span class="o">&amp;</span> <span class="p">((</span><span class="n">check_df</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">&gt;</span> <span class="p">(</span><span class="n">check_df</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]))):</span> 
                    <span class="n">three_white_soldiers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"Yes"</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">three_white_soldiers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"No"</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">three_white_soldiers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"No"</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">three_white_soldiers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"No"</span><span class="p">)</span>
                
    <span class="k">else</span><span class="p">:</span>
        <span class="n">three_white_soldiers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"No"</span><span class="p">)</span>

<span class="c1">#Create new column</span>
<span class="n">finance_df</span><span class="p">[</span><span class="s1">'ThreeWhiteSoldiers_pattern'</span><span class="p">]</span> <span class="o">=</span> <span class="n">three_white_soldiers</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=82b3ff14-6e3f-42d5-b27f-e2a37d957405">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[20]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1">#We can see how many three white soldier candlestick patterns are present</span>
<span class="n">finance_df</span><span class="p">[</span><span class="s1">'ThreeWhiteSoldiers_pattern'</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[20]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>ThreeWhiteSoldiers_pattern
No     6284
Yes      34
Name: count, dtype: int64</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=e75a7eca-5e4b-4f96-b15e-5e0440cbec4e">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Variable-Cleaning">Variable Cleaning<a class="anchor-link" href="#Variable-Cleaning"></a></h3><p><strong>6) Perform Further Cleaning on Variables</strong></p>
<p>I will also perform further cleaning below to calculate the following values needed to train my model:</p>
<ul>
<li>MACD and Signal Line</li>
<li>RSI</li>
<li>MFI</li>
<li>Normalize stock prices two different ways: Log transform &amp; Sklearn's scaler function</li>
<li>Creating the 'random' column in the dataset with randomly assigned "Yes" and "No" values</li>
</ul>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=52c9a8d7-6205-48be-b7e1-1d4fe49baaa0">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[21]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">MinMaxScaler</span> <span class="c1">#Used to normalize data</span>

<span class="c1">###Calculate MACD and Signal Line</span>
<span class="c1"># Calculate the 12-day EMA</span>
<span class="n">finance_df</span><span class="p">[</span><span class="s1">'EMA12'</span><span class="p">]</span> <span class="o">=</span> <span class="n">finance_df</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">]</span><span class="o">.</span><span class="n">ewm</span><span class="p">(</span><span class="n">span</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">adjust</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="c1"># Calculate the 26-day EMA</span>
<span class="n">finance_df</span><span class="p">[</span><span class="s1">'EMA26'</span><span class="p">]</span> <span class="o">=</span> <span class="n">finance_df</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">]</span><span class="o">.</span><span class="n">ewm</span><span class="p">(</span><span class="n">span</span><span class="o">=</span><span class="mi">26</span><span class="p">,</span> <span class="n">adjust</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="c1"># Calculate the MACD (12-day EMA - 26-day EMA)</span>
<span class="n">finance_df</span><span class="p">[</span><span class="s1">'MACD'</span><span class="p">]</span> <span class="o">=</span> <span class="n">finance_df</span><span class="p">[</span><span class="s1">'EMA12'</span><span class="p">]</span> <span class="o">-</span> <span class="n">finance_df</span><span class="p">[</span><span class="s1">'EMA26'</span><span class="p">]</span>

<span class="c1"># Calculate the Signal Line (9-day EMA of MACD)</span>
<span class="n">finance_df</span><span class="p">[</span><span class="s1">'Signal_Line'</span><span class="p">]</span> <span class="o">=</span> <span class="n">finance_df</span><span class="p">[</span><span class="s1">'MACD'</span><span class="p">]</span><span class="o">.</span><span class="n">ewm</span><span class="p">(</span><span class="n">span</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span> <span class="n">adjust</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>



<span class="c1">###RSI</span>
<span class="c1"># Calculate the daily price changes</span>
<span class="n">finance_df</span><span class="p">[</span><span class="s1">'Price_Change'</span><span class="p">]</span> <span class="o">=</span> <span class="n">finance_df</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">]</span><span class="o">.</span><span class="n">diff</span><span class="p">()</span>

<span class="c1"># Separate gains and losses</span>
<span class="n">finance_df</span><span class="p">[</span><span class="s1">'Gain'</span><span class="p">]</span> <span class="o">=</span> <span class="n">finance_df</span><span class="p">[</span><span class="s1">'Price_Change'</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="k">if</span> <span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">finance_df</span><span class="p">[</span><span class="s1">'Loss'</span><span class="p">]</span> <span class="o">=</span> <span class="n">finance_df</span><span class="p">[</span><span class="s1">'Price_Change'</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="o">-</span><span class="n">x</span> <span class="k">if</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span><span class="p">)</span>

<span class="c1"># Calculate the average gain and loss over a 14-day period</span>
<span class="n">period</span> <span class="o">=</span> <span class="mi">14</span>
<span class="n">finance_df</span><span class="p">[</span><span class="s1">'Avg_Gain'</span><span class="p">]</span> <span class="o">=</span> <span class="n">finance_df</span><span class="p">[</span><span class="s1">'Gain'</span><span class="p">]</span><span class="o">.</span><span class="n">rolling</span><span class="p">(</span><span class="n">window</span><span class="o">=</span><span class="n">period</span><span class="p">,</span> <span class="n">min_periods</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">finance_df</span><span class="p">[</span><span class="s1">'Avg_Loss'</span><span class="p">]</span> <span class="o">=</span> <span class="n">finance_df</span><span class="p">[</span><span class="s1">'Loss'</span><span class="p">]</span><span class="o">.</span><span class="n">rolling</span><span class="p">(</span><span class="n">window</span><span class="o">=</span><span class="n">period</span><span class="p">,</span> <span class="n">min_periods</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="c1"># Calculate the relative strength (RS)</span>
<span class="n">finance_df</span><span class="p">[</span><span class="s1">'RS'</span><span class="p">]</span> <span class="o">=</span> <span class="n">finance_df</span><span class="p">[</span><span class="s1">'Avg_Gain'</span><span class="p">]</span> <span class="o">/</span> <span class="n">finance_df</span><span class="p">[</span><span class="s1">'Avg_Loss'</span><span class="p">]</span>

<span class="c1"># Calculate the RSI</span>
<span class="n">finance_df</span><span class="p">[</span><span class="s1">'RSI'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">-</span> <span class="p">(</span><span class="mi">100</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">finance_df</span><span class="p">[</span><span class="s1">'RS'</span><span class="p">]))</span>


<span class="c1">####Used to calculate MFI</span>
<span class="c1"># Step 1: Calculate the Typical Price (TP)</span>
<span class="n">finance_df</span><span class="p">[</span><span class="s1">'TP'</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">finance_df</span><span class="p">[</span><span class="s1">'High'</span><span class="p">]</span> <span class="o">+</span> <span class="n">finance_df</span><span class="p">[</span><span class="s1">'Low'</span><span class="p">]</span> <span class="o">+</span> <span class="n">finance_df</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">])</span> <span class="o">/</span> <span class="mi">3</span>

<span class="c1"># Step 2: Calculate the Money Flow (MF)</span>
<span class="n">finance_df</span><span class="p">[</span><span class="s1">'MF'</span><span class="p">]</span> <span class="o">=</span> <span class="n">finance_df</span><span class="p">[</span><span class="s1">'TP'</span><span class="p">]</span> <span class="o">*</span> <span class="n">finance_df</span><span class="p">[</span><span class="s1">'Volume'</span><span class="p">]</span>

<span class="c1"># Step 3: Calculate Positive and Negative Money Flow</span>
<span class="n">finance_df</span><span class="p">[</span><span class="s1">'Positive_MF'</span><span class="p">]</span> <span class="o">=</span> <span class="n">finance_df</span><span class="p">[</span><span class="s1">'MF'</span><span class="p">]</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">finance_df</span><span class="p">[</span><span class="s1">'TP'</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">finance_df</span><span class="p">[</span><span class="s1">'TP'</span><span class="p">]</span><span class="o">.</span><span class="n">shift</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">finance_df</span><span class="p">[</span><span class="s1">'Negative_MF'</span><span class="p">]</span> <span class="o">=</span> <span class="n">finance_df</span><span class="p">[</span><span class="s1">'MF'</span><span class="p">]</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">finance_df</span><span class="p">[</span><span class="s1">'TP'</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">finance_df</span><span class="p">[</span><span class="s1">'TP'</span><span class="p">]</span><span class="o">.</span><span class="n">shift</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>

<span class="c1"># Step 4: Calculate the rolling sum of Positive and Negative Money Flow over the specified period (e.g., 14 periods)</span>
<span class="n">window</span> <span class="o">=</span> <span class="mi">14</span>
<span class="n">finance_df</span><span class="p">[</span><span class="s1">'Positive_MF_sum'</span><span class="p">]</span> <span class="o">=</span> <span class="n">finance_df</span><span class="p">[</span><span class="s1">'Positive_MF'</span><span class="p">]</span><span class="o">.</span><span class="n">rolling</span><span class="p">(</span><span class="n">window</span><span class="o">=</span><span class="n">window</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="n">finance_df</span><span class="p">[</span><span class="s1">'Negative_MF_sum'</span><span class="p">]</span> <span class="o">=</span> <span class="n">finance_df</span><span class="p">[</span><span class="s1">'Negative_MF'</span><span class="p">]</span><span class="o">.</span><span class="n">rolling</span><span class="p">(</span><span class="n">window</span><span class="o">=</span><span class="n">window</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

<span class="c1"># Step 5: Calculate the Money Flow Ratio</span>
<span class="n">finance_df</span><span class="p">[</span><span class="s1">'Money_Flow_Ratio'</span><span class="p">]</span> <span class="o">=</span> <span class="n">finance_df</span><span class="p">[</span><span class="s1">'Positive_MF_sum'</span><span class="p">]</span> <span class="o">/</span> <span class="n">finance_df</span><span class="p">[</span><span class="s1">'Negative_MF_sum'</span><span class="p">]</span>

<span class="c1"># Step 6: Calculate the Money Flow Index (MFI)</span>
<span class="n">finance_df</span><span class="p">[</span><span class="s1">'MFI'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">-</span> <span class="p">(</span><span class="mi">100</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">finance_df</span><span class="p">[</span><span class="s1">'Money_Flow_Ratio'</span><span class="p">]))</span>


<span class="c1">###Normalize stock price variables</span>
<span class="c1">#normalize via log transform</span>
<span class="n">finance_df</span><span class="p">[</span><span class="s1">'Log_Close'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">finance_df</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">])</span>
<span class="n">finance_df</span><span class="p">[</span><span class="s1">'Log_Open'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">finance_df</span><span class="p">[</span><span class="s1">'Open'</span><span class="p">])</span>
<span class="n">finance_df</span><span class="p">[</span><span class="s1">'Log_High'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">finance_df</span><span class="p">[</span><span class="s1">'High'</span><span class="p">])</span>
<span class="n">finance_df</span><span class="p">[</span><span class="s1">'Log_Low'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">finance_df</span><span class="p">[</span><span class="s1">'Low'</span><span class="p">])</span>

<span class="c1">#normalize via Sklearn's scaler function</span>
<span class="n">scaler_close</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span>
<span class="n">scaler_open</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span>
<span class="n">scaler_high</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span>
<span class="n">scaler_low</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span>
<span class="n">finance_df</span><span class="p">[</span><span class="s1">'Normalized_Close'</span><span class="p">]</span> <span class="o">=</span> <span class="n">scaler_close</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">finance_df</span><span class="p">[[</span><span class="s1">'Close'</span><span class="p">]])</span>
<span class="n">finance_df</span><span class="p">[</span><span class="s1">'Normalized_Open'</span><span class="p">]</span> <span class="o">=</span> <span class="n">scaler_open</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">finance_df</span><span class="p">[[</span><span class="s1">'Open'</span><span class="p">]])</span>
<span class="n">finance_df</span><span class="p">[</span><span class="s1">'Normalized_High'</span><span class="p">]</span> <span class="o">=</span> <span class="n">scaler_high</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">finance_df</span><span class="p">[[</span><span class="s1">'High'</span><span class="p">]])</span>
<span class="n">finance_df</span><span class="p">[</span><span class="s1">'Normalized_Low'</span><span class="p">]</span> <span class="o">=</span> <span class="n">scaler_low</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">finance_df</span><span class="p">[[</span><span class="s1">'Low'</span><span class="p">]])</span>


<span class="c1">####Used to create a new column to test random values of 'yes' to simulate presence of a random pattern</span>
<span class="c1"># Specify the number of "Yes" values you want, may show up as less during training due to location of the "Yes" value, as need at least 30 days</span>
<span class="c1">#of data for the 30-day sequence, or if the future closing price is not available (only have data to 2/14)</span>
<span class="n">num_yes</span> <span class="o">=</span> <span class="mi">200</span>

<span class="c1"># Create a list of "Yes" and "No" values</span>
<span class="n">yes_no_list</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"Yes"</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_yes</span> <span class="o">+</span> <span class="p">[</span><span class="s2">"No"</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">finance_df</span><span class="p">)</span> <span class="o">-</span> <span class="n">num_yes</span><span class="p">)</span>

<span class="c1">#set seed for reproducibility</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span> 

<span class="c1"># Shuffle the list to randomize the order</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">yes_no_list</span><span class="p">)</span>

<span class="c1"># Add the list as a new column in the DataFrame</span>
<span class="n">finance_df</span><span class="p">[</span><span class="s1">'Random_Yes_No'</span><span class="p">]</span> <span class="o">=</span> <span class="n">yes_no_list</span>

<span class="c1">#exclude the first 26 rows because calculations from MACD needs at least 26 days to calculate</span>
<span class="n">finance_df</span> <span class="o">=</span> <span class="n">finance_df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">26</span><span class="p">:]</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1">#write out CSV of cleaned dataset</span>
<span class="n">finance_df</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="sa">f</span><span class="s1">'</span><span class="si">{</span><span class="n">ticker_symbol</span><span class="si">}</span><span class="s1">_financials_cleaned_output.csv'</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># `index=False` avoids writing the index column</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=923edfe2-3271-45ab-a6ef-0688ae9a67ca">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="Advantages-and-Disadvantages-During-Data-Collection-Phase">Advantages and Disadvantages During Data Collection Phase<a class="anchor-link" href="#Advantages-and-Disadvantages-During-Data-Collection-Phase"></a></h4><p>An advantage of collecting the data is that there is built in stock packages to collect stock market data. I used the 'yfinance' package to easily get stock price data for the ticker 'SPY'. A disadvantage would be that after collecting the data for the candle stick patterns, I realized that they occur very rarely. For that reason I have decided to not implement the bullish harami, bullish engulfing, and three white soldiers patterns as they are the most rarely occurring patterns and I want to make sure I have enough data to train a reliable model.</p>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=463efaa8-fb83-4b1d-af68-a59f53949bcc">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h2 id="Data-Modeling-/-Machine-Learning---D">Data Modeling / Machine Learning - <em>D</em><a class="anchor-link" href="#Data-Modeling-/-Machine-Learning---D"></a></h2>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=c5823806-8c4c-49fe-baf8-44b76d8b91e7">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="LSTM-Classification-Model">LSTM Classification Model<a class="anchor-link" href="#LSTM-Classification-Model"></a></h4><p>Prior to running my machine learning model, the code below is designed to analyze a financial dataset (finance_df) by identifying a specific trading pattern, such as the "Hammer pattern," and subsequently predicting whether the stock's price will increase based on the pattern's occurrence. The code first filters the dataset to only include rows where the 'Hammer_pattern' is marked as "Yes." The pattern_df dataframe contains these filtered rows, and the code then iterates through these rows to collect a sequence of independent variables that can be used for prediction. These independent variables include various stock metrics like open, close, high, low prices, as well as technical indicators such as RSI (Relative Strength Index), MFI (Money Flow Index), and MACD (Moving Average Convergence Divergence). The code ensures that for each identified pattern, a series of previous days' data (up to 30 days) is gathered to build these feature sets, which are stored in separate lists. Additionally, a dependent variable is created to capture whether the stock price increases by a specified percentage (e.g., 1% or more) in the days following the identified pattern.</p>
<p>The code also handles the logic of determining whether the stock's price has increased by the desired percentage after the pattern appears. For each identified pattern, the code checks if the stock price on the following day (determined by 'days_out') exceeds the original price by the specified percentage. If the price increase criterion is met, the dependent variable is set to 1 (indicating a positive class), and if it isn't, it is set to 0 (indicating a negative class). The independent variables, which include multiple arrays of stock data for each identified pattern, are stored in corresponding lists and then converted into NumPy arrays for further processing or machine learning tasks. This approach prepares the data for machine learning models to predict future stock price movements based on the identified pattern and accompanying technical indicators.</p>
<p><strong>Note!!!</strong>: I have chosen not to implement machine learning models to predict the future prices for three candlestick patterns. The reason is because of the small sample size of data. These are the Bullish Engulfing, Bullish Harami, and Three White Soldiers pattern.</p>
<p>To summarize or expound on what I have just said, the code directly below will allow us to:</p>
<ol>
<li>Select the candlestick pattern ("Random", "Hammer", "Inverted Hammer"): The random pattern is a column in the dataset with randomly assigned "Yes" and "No" values. These "Yes" and "No" values are distributed randomly, and my goal is to compare the model's performance using these random patterns labeled "Yes" versus patterns that are specifically identified as candlestick patterns. This will help me understand if the model behaves differently when dealing with randomly assigned patterns versus known candlestick patterns.
<br/></li>
<li>Generate our independent variables that can be used for prediction. There will be 15 different sets of independent variables. Each one of them has a different shape and includes a different subset of independent variables. This allows us to compare the performance of independent variables so that we can evaluate the best performing combination of independent variables.
<br/></li>
<li>Generate our dependent variable. Our value of the dependent variable is dependent on the following two model parameters that is specified by me:<ul>
<li>My research question states that I will predict a stock's closing price sometime after the candlestick pattern concludes. I will try different values for this such as one days afterwards, three days, five, ten, and fifteen.</li>
<li>Another parameter for defining the dependent variable is the percentage increase required to classify a future stock closing price as a positive class. For example, if the parameter is set to '1.0', the future closing price only needs to be greater than the last closing price identified within the 30-day sequence. However, if set to '1.1', the future closing price must exceed the last closing price in the sequence by 1%, i.e., the last closing price multiplied by 1.01.</li>
</ul>
</li>
</ol>
<p>The result of the code will be a numpy array with values of from all of our cleaned variables. This array incorporates the idea that one observation (one sequence) is 30 days of stock data. Each of these 30 time-steps (remember, one time-step = one day), will represent daily data of my eight chosen independent variables. The dependent variable in my array will be future closing stock price.</p>
<p>Depending on my selected candlestick pattern, I may have more observations as some candlestick patterns occur more often than others. Here is an example on how my independent and dependent variables will look like:</p>
<p>-Shape: (30, 8)<br/>
<em>independent_observation = np.array([ <br/>
[0.054, 0.062, 0.054, 0.059, 45.3, 1000000, 0.0012, 0.0010],  # Day 1 <br/>
[0.059, 0.063, 0.059, 0.062, 46.1, 1010000, 0.0013, 0.0011],  # Day 2 <br/>
[0.061, 0.064, 0.060, 0.063, 47.2, 1025000, 0.0014, 0.0012],  # Day 3 <br/>
# ... 27 more rows ... <br/>
[0.070, 0.074, 0.071, 0.073, 55.0, 1200000, 0.0020, 0.0018]   # Day 30 <br/>
])</em></p>
<p>-Binary label: 1 if the future closing price is higher, 0 otherwise <br/>
<em>dependent_classification = np.array([1])</em></p>
<p>-Future day's closing price (e.g., 0.075) <br/>
<em>dependent_regression = np.array([0.075])</em></p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=2fabf69e-7422-4478-9bdd-31244647edfc">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[127]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1">#Subset data frame for desired pattern</span>
<span class="n">pattern_df</span> <span class="o">=</span> <span class="n">finance_df</span><span class="p">[</span><span class="n">finance_df</span><span class="p">[</span><span class="s1">'Hammer_pattern'</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"Yes"</span><span class="p">]</span>
<span class="c1">#pattern_df = finance_df[finance_df['Random_Yes_No'] == "Yes"]</span>

<span class="c1">#How many days after the pattern is identified to use for the dependent variable</span>
<span class="n">days_out</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1">#What percent increase from the current price is considered a positive class. For example 1.01 = 1% increase; 100 * 1.01 = 101. So if original</span>
<span class="c1">#price is $100, anything greater than $101 is considered a positive class.</span>
<span class="n">pct_increase</span> <span class="o">=</span> <span class="mf">1.00</span>

<span class="c1">#Gather independent variables</span>
<span class="n">independent_list1</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">independent_list2</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">independent_list3</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">independent_list4</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">independent_list5</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">independent_list6</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">independent_list7</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">independent_list8</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">independent_list9</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">independent_list10</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">independent_list11</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">independent_list12</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">independent_list13</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">independent_list14</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">independent_list15</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1">#gather dependent variables</span>
<span class="n">dependent_list</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">pattern_index</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">pattern_df</span><span class="p">[</span><span class="s2">"Row_index"</span><span class="p">])</span>
<span class="c1">#pattern_index = [60, 62]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">pattern_index</span><span class="p">:</span>
    <span class="c1">#if (i == 62):</span>
    <span class="c1">#    break</span>
    
    <span class="c1">#unable to get 30 days worth of data if index is less than 56, because previously removed first 26 observations</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="mi">56</span><span class="p">):</span>
        <span class="k">continue</span>

    <span class="c1">#get 30 days worth of data to gather data for indpendent variables</span>
    <span class="n">subset_df</span> <span class="o">=</span> <span class="n">finance_df</span><span class="p">[(</span><span class="n">finance_df</span><span class="p">[</span><span class="s2">"Row_index"</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="n">i</span> <span class="o">-</span> <span class="mi">29</span><span class="p">))</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">finance_df</span><span class="p">[</span><span class="s2">"Row_index"</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="p">(</span><span class="n">i</span><span class="p">))]</span>
    <span class="c1">#subset_df = finance_df[(finance_df["Row_index"] &gt;= (i - 13)) &amp; (finance_df["Row_index"] &lt;= (i))]</span>
    
    <span class="c1">#Get day after data to gather closing price for dependent variable</span>
    <span class="n">dependent_df</span> <span class="o">=</span> <span class="n">finance_df</span><span class="p">[</span><span class="n">finance_df</span><span class="p">[</span><span class="s2">"Row_index"</span><span class="p">]</span> <span class="o">==</span> <span class="p">(</span><span class="n">i</span><span class="p">)]</span>
    <span class="n">dependent2_df</span> <span class="o">=</span> <span class="n">finance_df</span><span class="p">[</span><span class="n">finance_df</span><span class="p">[</span><span class="s2">"Row_index"</span><span class="p">]</span> <span class="o">==</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="n">days_out</span><span class="p">)]</span>
    
    <span class="n">temp_list1</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">temp_list2</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">temp_list3</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">temp_list4</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">temp_list5</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">temp_list6</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">temp_list7</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">temp_list8</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">temp_list9</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">temp_list10</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">temp_list11</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">temp_list12</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">temp_list13</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">temp_list14</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">temp_list15</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1">#append temp_list to independent_list</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">dependent2_df</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span> <span class="c1">#dependent2_df may have length of zero as it is a future date, data may not be available</span>
    

        <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">subset_df</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
                
                <span class="n">test_array1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Low'</span><span class="p">]])</span>
                <span class="n">test_array2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Log_Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_Low'</span><span class="p">]])</span>
                <span class="n">test_array3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Low'</span><span class="p">]])</span>
        
                <span class="n">test_array4</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'RSI'</span><span class="p">]])</span>
                <span class="n">test_array5</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Log_Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'RSI'</span><span class="p">]])</span>
                <span class="n">test_array6</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'RSI'</span><span class="p">]])</span>
        
                <span class="n">test_array7</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MFI'</span><span class="p">]])</span>
                <span class="n">test_array8</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Log_Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MFI'</span><span class="p">]])</span>
                <span class="n">test_array9</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MFI'</span><span class="p">]])</span>
        
                <span class="n">test_array10</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MACD'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Signal_Line'</span><span class="p">]])</span>
                <span class="n">test_array11</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Log_Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MACD'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Signal_Line'</span><span class="p">]])</span>
                <span class="n">test_array12</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MACD'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Signal_Line'</span><span class="p">]])</span>
        
                <span class="n">test_array13</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'RSI'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MFI'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MACD'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Signal_Line'</span><span class="p">]])</span>
                <span class="n">test_array14</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Log_Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'RSI'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MFI'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MACD'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Signal_Line'</span><span class="p">]])</span>
                <span class="n">test_array15</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'RSI'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MFI'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MACD'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Signal_Line'</span><span class="p">]])</span>
        
                
                <span class="n">temp_list1</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array1</span><span class="p">)</span>
                <span class="n">temp_list2</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array2</span><span class="p">)</span>
                <span class="n">temp_list3</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array3</span><span class="p">)</span>
                <span class="n">temp_list4</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array4</span><span class="p">)</span>
                <span class="n">temp_list5</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array5</span><span class="p">)</span>
                <span class="n">temp_list6</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array6</span><span class="p">)</span>
                <span class="n">temp_list7</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array7</span><span class="p">)</span>
                <span class="n">temp_list8</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array8</span><span class="p">)</span>
                <span class="n">temp_list9</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array9</span><span class="p">)</span>
                <span class="n">temp_list10</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array10</span><span class="p">)</span>
                <span class="n">temp_list11</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array11</span><span class="p">)</span>
                <span class="n">temp_list12</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array12</span><span class="p">)</span>
                <span class="n">temp_list13</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array13</span><span class="p">)</span>
                <span class="n">temp_list14</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array14</span><span class="p">)</span>
                <span class="n">temp_list15</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array15</span><span class="p">)</span>
                
        <span class="n">independent_list1</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list1</span><span class="p">)</span>
        <span class="n">independent_list2</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list2</span><span class="p">)</span>
        <span class="n">independent_list3</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list3</span><span class="p">)</span>
        <span class="n">independent_list4</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list4</span><span class="p">)</span>
        <span class="n">independent_list5</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list5</span><span class="p">)</span>
        <span class="n">independent_list6</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list6</span><span class="p">)</span>
        <span class="n">independent_list7</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list7</span><span class="p">)</span>
        <span class="n">independent_list8</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list8</span><span class="p">)</span>
        <span class="n">independent_list9</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list9</span><span class="p">)</span>
        <span class="n">independent_list10</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list10</span><span class="p">)</span>
        <span class="n">independent_list11</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list11</span><span class="p">)</span>
        <span class="n">independent_list12</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list12</span><span class="p">)</span>
        <span class="n">independent_list13</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list13</span><span class="p">)</span>
        <span class="n">independent_list14</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list14</span><span class="p">)</span>
        <span class="n">independent_list15</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list15</span><span class="p">)</span>
    
        <span class="k">if</span> <span class="p">(</span><span class="n">dependent2_df</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">dependent_df</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">pct_increase</span><span class="p">):</span>
            <span class="n">dependent_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">dependent_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>


<span class="n">independent_array1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list1</span><span class="p">)</span>
<span class="n">independent_array2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list2</span><span class="p">)</span>
<span class="n">independent_array3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list3</span><span class="p">)</span>
<span class="n">independent_array4</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list4</span><span class="p">)</span>
<span class="n">independent_array5</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list5</span><span class="p">)</span>
<span class="n">independent_array6</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list6</span><span class="p">)</span>
<span class="n">independent_array7</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list7</span><span class="p">)</span>
<span class="n">independent_array8</span><span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list8</span><span class="p">)</span>
<span class="n">independent_array9</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list9</span><span class="p">)</span>
<span class="n">independent_array10</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list10</span><span class="p">)</span>
<span class="n">independent_array11</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list11</span><span class="p">)</span>
<span class="n">independent_array12</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list12</span><span class="p">)</span>
<span class="n">independent_array13</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list13</span><span class="p">)</span>
<span class="n">independent_array14</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list14</span><span class="p">)</span>
<span class="n">independent_array15</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list15</span><span class="p">)</span>
<span class="n">dependent_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dependent_list</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=6ad04cfe-33f2-443d-85a5-b52ac112f7be">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[128]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">independent_array1</span><span class="p">))</span> <span class="c1"># 30 time-steps and 4 features per time-step</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">independent_array4</span><span class="p">))</span> <span class="c1"># 30 time-steps and 5 features per time-step</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">independent_array10</span><span class="p">))</span> <span class="c1"># 30 time-steps and 6 features per time-step</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">independent_array15</span><span class="p">))</span> <span class="c1"># 30 time-steps and 8 features per time-step</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>(69, 30, 4)
(69, 30, 5)
(69, 30, 6)
(69, 30, 8)
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=de192799-9a6b-4d91-ae33-a1b8f127368b">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[129]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1">#the number of observations of each class</span>
<span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">dependent_array</span><span class="p">,</span> <span class="n">return_counts</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[129]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>(array([0, 1]), array([37, 32]))</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=bff446ed-2fa0-4989-8319-9ee0dde40a26">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>This is my LSTM classification model as shown below.</p>
<p>This model is designed for binary classification tasks, where the goal is to predict one of two possible outcomes ("Yes" or "No"). It uses an LSTM (Long Short-Term Memory) network, which is a type of Recurrent Neural Network (RNN) that works well with sequential data, like time series in my case. The model begins with an LSTM layer of 128 units, using the "tanh" activation function to capture patterns in the sequential input data. The return_sequences=True means this layer outputs sequences, which are passed on to the next LSTM layer. A dropout layer is added to reduce overfitting by randomly "dropping" some of the units during training.</p>
<p>The second LSTM layer, with 64 units, processes the sequence further, but without returning sequences (return_sequences=False). This makes the output a single vector, which is then passed through another dropout layer. The model ends with a dense output layer, which has one unit with a sigmoid activation function, giving a probability between 0 and 1 for binary classification. The model is compiled using the Adam optimizer, a commonly used optimization algorithm, with binary cross-entropy as the loss function, as this is suitable for binary classification problems. The performance of the model is evaluated using accuracy as the metric.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=d76c0e0b-6c7b-44e8-a261-bc9358caf8d3">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[130]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tensorflow.keras.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tensorflow.keras.layers</span><span class="w"> </span><span class="kn">import</span> <span class="n">LSTM</span><span class="p">,</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Dropout</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="c1">#Define the LSTM classification model</span>
<span class="k">def</span><span class="w"> </span><span class="nf">create_lstm_classification</span><span class="p">(</span><span class="n">input_shape</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
    
    <span class="c1"># LSTM layers</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'tanh'</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">))</span>  <span class="c1"># Dropout to reduce overfitting</span>
    
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'tanh'</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>  <span class="c1"># Final LSTM layer</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">))</span>
    
    <span class="c1"># Dense output layer for binary classification</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'sigmoid'</span><span class="p">))</span>  <span class="c1"># Sigmoid for binary classification (probability)</span>
    
    <span class="c1"># Compile the model</span>
    <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">'adam'</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s1">'binary_crossentropy'</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'accuracy'</span><span class="p">])</span>  <span class="c1"># Binary cross-entropy for classification</span>
    
    <span class="k">return</span> <span class="n">model</span>

<span class="c1">#independent_array(s) and dependent_array are already defined</span>
<span class="c1">#independent variables (features)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">independent_array1</span>  <span class="c1"># Shape: (890, 30, [4, 5, 6, or 8])</span>

<span class="c1">#dependent variable (target)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">dependent_array</span>  <span class="c1"># Shape: (890,) (binary labels, 0 or 1)</span>

<span class="c1">#split data into training and testing sets (80% training, 20% testing), random state for reproducible results</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>

<span class="c1">#define the input shape based on your data; for example independent array_1 has input_shape of (30, 4); independent array_15's shape is (30,8)</span>
<span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>  <span class="c1"># 30 time-steps and 8 features per time-step for independent_array_15</span>

<span class="c1">#create the LSTM model</span>
<span class="n">classification_model</span> <span class="o">=</span> <span class="n">create_lstm_classification</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>

<span class="c1">#train the classification model and store history. verbose = 0 -&gt; hides the training output</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">classification_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">),</span> <span class="n">verbose</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>

<span class="c1">#get the validation accuracy from history object</span>
<span class="n">val_accuracy</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">'val_accuracy'</span><span class="p">]</span>

<span class="c1">#find the highest validation accuracy achieved during training</span>
<span class="n">best_val_accuracy</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">val_accuracy</span><span class="p">)</span>
<span class="n">avg_val_accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">val_accuracy</span><span class="p">)</span>

<span class="c1"># Evaluate the model on the test data, returning best validation accuracy and average validation accuracy throughout all epochs</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Max Accuracy: </span><span class="si">{</span><span class="n">best_val_accuracy</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">100</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%; Average accuray: </span><span class="si">{</span><span class="n">avg_val_accuracy</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Max Accuracy: 57.14%; Average accuray: 0.48571428954601287
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=0627637b-1d38-46c8-bbee-3894d3b7807e">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>The model above was ran using data from independent_array #1, which had the shape of (69, 30, 4) or in other words 69 observations, 30 time-steps and 4 features per time-step. Independent array #1 as shown in the code only includes non-normalized stock price variables (open, low, close, high).</p>
<p>The model was also run using 30-day sequences, with each sequence concluding on the 30th day featuring a hammer candlestick pattern on that 30th day. Lastly, I selected the two parameter combinations for the dependent variable: that the future closing price was one day in the future, the day directly after the 30 day sequence. The second parameter was that the dependent variable was labeled as a positive class if it's future closing price was higher than it was previously than the closingn price of the last candle in the 30 day sequence.</p>
<p>We can see that the best accuracy score achieved by the model was 57.14% after 10 epochs.</p>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=2b80c49b-8b06-49f6-bada-bdbcb685ff06">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="Exploring-Model-Performance-with-Parameter-Variations-and-Stratified-K-Fold-Cross-Validation---LSTM-Classification-Model">Exploring Model Performance with Parameter Variations and Stratified K-Fold Cross Validation - LSTM Classification Model<a class="anchor-link" href="#Exploring-Model-Performance-with-Parameter-Variations-and-Stratified-K-Fold-Cross-Validation---LSTM-Classification-Model"></a></h4><p>Now, using the same LSTM model as defined in the previous step I will try various combinations of parameters and variables for my model.</p>
<p>For the parameter combinations, I will evaluate my model using three patterns: random days, the hammer pattern, and the inverted hammer pattern. Ive chosen not to evaluate the other patterns due to insufficient observations in the dataset. When I refer to evaluating my model on random days, I mean that I previously created a column in the dataset with randomly assigned "Yes" values. These "Yes" values are distributed randomly, and my goal is to compare the model's performance using these random patterns versus patterns that are specifically identified as candlestick patterns. This will help me understand if the model behaves differently when dealing with randomly assigned patterns versus known candlestick patterns.</p>
<p>Other parameters I will test will be how many days out after the pattern is identified to use for the dependent variable. For example, if this is set to the value of "1", the closing price for the day directly after the candlestick pattern will be what influences the dependent variable. Another example, is if this parameter is "10", then the future closing price is associated with 10 days after the last candle of the candle stick pattern.</p>
<p>To build off from the previous paragraph, my last parameter will be what percent increase from the original price is considered a positive class. For example 1.01 = 1% increase; 100 * 1.01 = 101. So if the original price is 100 dollars, anything greater than 101 dollars is considered a positive class. In my analysis I have set these values to [1.0, 1.01, and 1.02]. Note, if the value is set to '1.0' then if the original price is 100 dollars, anything greater than 100 dollars is considered a positive class.</p>
<p>I will also use statified K-fold Cross validation. Stratified K-Fold Cross Validation is often preferred over regular K-Fold because it ensures that each fold of the data has a similar distribution of the target classes. This is particularly important when dealing with imbalanced datasets where some classes may be underrepresented. In regular K-Fold cross-validation, the data is randomly split, which could result in some folds having disproportionately many samples from one class and too few from another. This can lead to biased model performance estimates, as the model might not be exposed to enough of the minority class to learn effectively.</p>
<p>In contrast, Stratified K-Fold ensures that each fold contains roughly the same proportion of each class as in the original dataset. This helps the model train and validate on a more balanced representation of the target variable, leading to more reliable and generalizable performance metrics. Stratified K-Fold is particularly beneficial for classification tasks where the goal is to maintain fairness in model evaluation, and it can help prevent skewed results caused by class imbalances.</p>
<p>In my case, when splitting my dataset, I want to make sure I have the same ratio of positive and negative classes in each fold as in the original dataset. This will ensure that my model is consistently evaluated on balanced data and help improve its ability to predict both classes effectively.</p>
<p>The result of running the code below will output a CSV file which shows the accuracy scores of each parameter and variable combination. I am going to have to run this code below multiple times, each time changing the stock ticker (if I choose to analyze another stock, but I won't in this project; I will only evaluate the stock ticker "SPY" to save resources.) I want to evaluate and the selected pattern (the code below can only run one selected pattern and one stock ticker at a time).</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=fc13439a-d1d6-4f1d-b076-a041c964c703">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[133]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tensorflow.keras.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tensorflow.keras.layers</span><span class="w"> </span><span class="kn">import</span> <span class="n">LSTM</span><span class="p">,</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Dropout</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">KFold</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">StratifiedKFold</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tensorflow.keras.optimizers</span><span class="w"> </span><span class="kn">import</span> <span class="n">RMSprop</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>


<span class="c1">### User inputs ###</span>
<span class="n">selected_pattern</span> <span class="o">=</span> <span class="s2">"InvertedHammer"</span>   <span class="c1">#choices: 'Random', 'Hammer', 'InvertedHammer'</span>

<span class="c1">#How many days after the pattern is identified to use for the dependent variable</span>
<span class="n">days_out</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">15</span><span class="p">]</span>

<span class="c1">#What percent increase from the current price is considered a positive class. For example 1.01 = 1% increase; 100 * 1.01 = 101. So if original</span>
<span class="c1">#price is $100, anything greater than $101 is considered a positive class.</span>
<span class="n">pct_increase</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.00</span><span class="p">,</span> <span class="mf">1.01</span><span class="p">,</span> <span class="mf">1.02</span><span class="p">]</span>

<span class="c1">######</span>


<span class="c1"># Define the classification model</span>
<span class="k">def</span><span class="w"> </span><span class="nf">create_lstm_classification</span><span class="p">(</span><span class="n">input_shape</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
    
    <span class="c1"># LSTM layers</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'tanh'</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">))</span>  <span class="c1"># Dropout to reduce overfitting</span>
    
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'tanh'</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>  <span class="c1"># Final LSTM layer</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">))</span>
    
    <span class="c1"># Dense output layer for binary classification</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'sigmoid'</span><span class="p">))</span>  <span class="c1"># Sigmoid for binary classification (probability)</span>
    
    <span class="c1"># Compile the model</span>
    <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">'adam'</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s1">'binary_crossentropy'</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'accuracy'</span><span class="p">])</span>  <span class="c1"># Binary cross-entropy for classification</span>
    <span class="k">return</span> <span class="n">model</span>

    

<span class="c1">#Subset data frame for desired pattern</span>
<span class="k">if</span> <span class="p">(</span><span class="n">selected_pattern</span> <span class="o">==</span> <span class="s2">"Random"</span><span class="p">):</span>
    <span class="n">pattern_df</span> <span class="o">=</span> <span class="n">finance_df</span><span class="p">[</span><span class="n">finance_df</span><span class="p">[</span><span class="s1">'Random_Yes_No'</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"Yes"</span><span class="p">]</span>
<span class="k">elif</span> <span class="p">(</span><span class="n">selected_pattern</span> <span class="o">==</span> <span class="s2">"Hammer"</span><span class="p">):</span>
    <span class="n">pattern_df</span> <span class="o">=</span> <span class="n">finance_df</span><span class="p">[</span><span class="n">finance_df</span><span class="p">[</span><span class="s1">'Hammer_pattern'</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"Yes"</span><span class="p">]</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">pattern_df</span> <span class="o">=</span> <span class="n">finance_df</span><span class="p">[</span><span class="n">finance_df</span><span class="p">[</span><span class="s1">'InvertedHammer_pattern'</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"Yes"</span><span class="p">]</span>


<span class="c1">#initialize an empty DataFrame with column names</span>
<span class="n">accuracy_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">'ticker'</span><span class="p">,</span> <span class="s1">'pattern'</span><span class="p">,</span> <span class="s1">'independent_array'</span><span class="p">,</span> <span class="s1">'best_accuracy'</span><span class="p">,</span> <span class="s1">'avg_accuracy'</span><span class="p">,</span> <span class="s1">'days_out'</span><span class="p">,</span> <span class="s1">'Total_observations'</span><span class="p">,</span> 
                                   <span class="s1">'Negative_observations'</span><span class="p">,</span> <span class="s1">'Positive_observations'</span><span class="p">,</span> <span class="s1">'Percent_increase_parameter'</span><span class="p">])</span>


<span class="k">for</span> <span class="n">percent</span> <span class="ow">in</span> <span class="n">pct_increase</span><span class="p">:</span>

    <span class="k">for</span> <span class="n">day</span> <span class="ow">in</span> <span class="n">days_out</span><span class="p">:</span>
        <span class="c1">#Gather independent variables</span>
        <span class="n">independent_list1</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">independent_list2</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">independent_list3</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">independent_list4</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">independent_list5</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">independent_list6</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">independent_list7</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">independent_list8</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">independent_list9</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">independent_list10</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">independent_list11</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">independent_list12</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">independent_list13</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">independent_list14</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">independent_list15</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="c1">#gather dependent variables</span>
        <span class="n">dependent_list</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1">#these are the row indexes that have the identified patterns; loop through</span>
        <span class="n">pattern_index</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">pattern_df</span><span class="p">[</span><span class="s2">"Row_index"</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">pattern_index</span><span class="p">:</span>
            <span class="c1">#if (i == 62):</span>
            <span class="c1">#    break</span>
            
            <span class="c1">#unable to get 30 days worth of data if index is less than 56, because previously removed first 26 observations</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="mi">56</span><span class="p">):</span>
                <span class="k">continue</span>
        
            <span class="c1">#get 30 days worth of data to gather data for indpendent variables</span>
            <span class="n">subset_df</span> <span class="o">=</span> <span class="n">finance_df</span><span class="p">[(</span><span class="n">finance_df</span><span class="p">[</span><span class="s2">"Row_index"</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="n">i</span> <span class="o">-</span> <span class="mi">29</span><span class="p">))</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">finance_df</span><span class="p">[</span><span class="s2">"Row_index"</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="p">(</span><span class="n">i</span><span class="p">))]</span>
            <span class="c1">#subset_df = finance_df[(finance_df["Row_index"] &gt;= (i - 13)) &amp; (finance_df["Row_index"] &lt;= (i))]</span>
            
            <span class="c1">#Get day after data to gather closing price for dependent variable</span>
            <span class="n">dependent_df</span> <span class="o">=</span> <span class="n">finance_df</span><span class="p">[</span><span class="n">finance_df</span><span class="p">[</span><span class="s2">"Row_index"</span><span class="p">]</span> <span class="o">==</span> <span class="p">(</span><span class="n">i</span><span class="p">)]</span>
            <span class="n">dependent2_df</span> <span class="o">=</span> <span class="n">finance_df</span><span class="p">[</span><span class="n">finance_df</span><span class="p">[</span><span class="s2">"Row_index"</span><span class="p">]</span> <span class="o">==</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="n">day</span><span class="p">)]</span>
            
            <span class="n">temp_list1</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">temp_list2</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">temp_list3</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">temp_list4</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">temp_list5</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">temp_list6</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">temp_list7</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">temp_list8</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">temp_list9</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">temp_list10</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">temp_list11</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">temp_list12</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">temp_list13</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">temp_list14</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">temp_list15</span> <span class="o">=</span> <span class="p">[]</span>
        
            <span class="c1">#append temp_list to independent_list</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">dependent2_df</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span> <span class="c1">#dependent2_df may have length of zero as it is a future date, data may not be available</span>
            
        
                <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">subset_df</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
                        
                        <span class="n">test_array1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Low'</span><span class="p">]])</span>
                        <span class="n">test_array2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Log_Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_Low'</span><span class="p">]])</span>
                        <span class="n">test_array3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Low'</span><span class="p">]])</span>
                
                        <span class="n">test_array4</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'RSI'</span><span class="p">]])</span>
                        <span class="n">test_array5</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Log_Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'RSI'</span><span class="p">]])</span>
                        <span class="n">test_array6</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'RSI'</span><span class="p">]])</span>
                
                        <span class="n">test_array7</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MFI'</span><span class="p">]])</span>
                        <span class="n">test_array8</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Log_Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MFI'</span><span class="p">]])</span>
                        <span class="n">test_array9</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MFI'</span><span class="p">]])</span>
                
                        <span class="n">test_array10</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MACD'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Signal_Line'</span><span class="p">]])</span>
                        <span class="n">test_array11</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Log_Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MACD'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Signal_Line'</span><span class="p">]])</span>
                        <span class="n">test_array12</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MACD'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Signal_Line'</span><span class="p">]])</span>
                
                        <span class="n">test_array13</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'RSI'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MFI'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MACD'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Signal_Line'</span><span class="p">]])</span>
                        <span class="n">test_array14</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Log_Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'RSI'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MFI'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MACD'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Signal_Line'</span><span class="p">]])</span>
                        <span class="n">test_array15</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'RSI'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MFI'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MACD'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Signal_Line'</span><span class="p">]])</span>
                
                
                        <span class="n">temp_list1</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array1</span><span class="p">)</span>
                        <span class="n">temp_list2</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array2</span><span class="p">)</span>
                        <span class="n">temp_list3</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array3</span><span class="p">)</span>
                        <span class="n">temp_list4</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array4</span><span class="p">)</span>
                        <span class="n">temp_list5</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array5</span><span class="p">)</span>
                        <span class="n">temp_list6</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array6</span><span class="p">)</span>
                        <span class="n">temp_list7</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array7</span><span class="p">)</span>
                        <span class="n">temp_list8</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array8</span><span class="p">)</span>
                        <span class="n">temp_list9</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array9</span><span class="p">)</span>
                        <span class="n">temp_list10</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array10</span><span class="p">)</span>
                        <span class="n">temp_list11</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array11</span><span class="p">)</span>
                        <span class="n">temp_list12</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array12</span><span class="p">)</span>
                        <span class="n">temp_list13</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array13</span><span class="p">)</span>
                        <span class="n">temp_list14</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array14</span><span class="p">)</span>
                        <span class="n">temp_list15</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array15</span><span class="p">)</span>
                        
                <span class="n">independent_list1</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list1</span><span class="p">)</span>
                <span class="n">independent_list2</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list2</span><span class="p">)</span>
                <span class="n">independent_list3</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list3</span><span class="p">)</span>
                <span class="n">independent_list4</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list4</span><span class="p">)</span>
                <span class="n">independent_list5</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list5</span><span class="p">)</span>
                <span class="n">independent_list6</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list6</span><span class="p">)</span>
                <span class="n">independent_list7</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list7</span><span class="p">)</span>
                <span class="n">independent_list8</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list8</span><span class="p">)</span>
                <span class="n">independent_list9</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list9</span><span class="p">)</span>
                <span class="n">independent_list10</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list10</span><span class="p">)</span>
                <span class="n">independent_list11</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list11</span><span class="p">)</span>
                <span class="n">independent_list12</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list12</span><span class="p">)</span>
                <span class="n">independent_list13</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list13</span><span class="p">)</span>
                <span class="n">independent_list14</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list14</span><span class="p">)</span>
                <span class="n">independent_list15</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list15</span><span class="p">)</span>
            
                <span class="k">if</span> <span class="p">(</span><span class="n">dependent2_df</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">dependent_df</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">percent</span><span class="p">):</span>
                    <span class="n">dependent_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">dependent_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        
        <span class="n">independent_array1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list1</span><span class="p">)</span>
        <span class="n">independent_array2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list2</span><span class="p">)</span>
        <span class="n">independent_array3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list3</span><span class="p">)</span>
        <span class="n">independent_array4</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list4</span><span class="p">)</span>
        <span class="n">independent_array5</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list5</span><span class="p">)</span>
        <span class="n">independent_array6</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list6</span><span class="p">)</span>
        <span class="n">independent_array7</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list7</span><span class="p">)</span>
        <span class="n">independent_array8</span><span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list8</span><span class="p">)</span>
        <span class="n">independent_array9</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list9</span><span class="p">)</span>
        <span class="n">independent_array10</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list10</span><span class="p">)</span>
        <span class="n">independent_array11</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list11</span><span class="p">)</span>
        <span class="n">independent_array12</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list12</span><span class="p">)</span>
        <span class="n">independent_array13</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list13</span><span class="p">)</span>
        <span class="n">independent_array14</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list14</span><span class="p">)</span>
        <span class="n">independent_array15</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list15</span><span class="p">)</span>
        <span class="n">dependent_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dependent_list</span><span class="p">)</span>
    
    
        <span class="n">y</span> <span class="o">=</span> <span class="n">dependent_array</span>
        <span class="n">independent_array</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">best_accuracy</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">avg_accuracy</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">counter_independentarray</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">):</span>
            <span class="c1">#if i != 12: #testing what seems is the most well performing model</span>
                <span class="c1">#continue</span>
            
            <span class="c1"># Select which independent_array to use</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">independent_array1</span>  <span class="c1"># Shape: (890, 30, 4)</span>
                <span class="n">independent_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"independent_array1"</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">independent_array2</span>  <span class="c1"># Shape: (890, 30, 4)</span>
                <span class="n">independent_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"independent_array2"</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">independent_array3</span>  <span class="c1"># Shape: (890, 30, 4)</span>
                <span class="n">independent_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"independent_array3"</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">independent_array4</span>  <span class="c1"># Shape: (890, 30, 5)</span>
                <span class="n">independent_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"independent_array4"</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">5</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">independent_array5</span>  <span class="c1"># Shape: (890, 30, 5)</span>
                <span class="n">independent_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"independent_array5"</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">6</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">independent_array6</span>  <span class="c1"># Shape: (890, 30, 5)</span>
                <span class="n">independent_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"independent_array6"</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">7</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">independent_array7</span>  <span class="c1"># Shape: (890, 30, 5)</span>
                <span class="n">independent_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"independent_array7"</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">8</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">independent_array8</span>  <span class="c1"># Shape: (890, 30, 5)</span>
                <span class="n">independent_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"independent_array8"</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">9</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">independent_array9</span>  <span class="c1"># Shape: (890, 30, 5)</span>
                <span class="n">independent_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"independent_array9"</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">10</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">independent_array10</span>  <span class="c1"># Shape: (890, 30, 6)</span>
                <span class="n">independent_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"independent_array10"</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">11</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">independent_array11</span>  <span class="c1"># Shape: (890, 30, 6)</span>
                <span class="n">independent_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"independent_array11"</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">12</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">independent_array12</span>  <span class="c1"># Shape: (890, 30, 6)</span>
                <span class="n">independent_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"independent_array12"</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">13</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">independent_array13</span>  <span class="c1"># Shape: (890, 30, 8)</span>
                <span class="n">independent_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"independent_array13"</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">14</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">independent_array14</span>  <span class="c1"># Shape: (890, 30, 8)</span>
                <span class="n">independent_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"independent_array14"</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">15</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">independent_array15</span>  <span class="c1"># Shape: (890, 30, 8)</span>
                <span class="n">independent_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"independent_array15"</span><span class="p">)</span>
        
            <span class="n">counter_independentarray</span> <span class="o">=</span> <span class="n">counter_independentarray</span> <span class="o">+</span> <span class="mi">1</span>
            
            <span class="c1"># Define the input shape based on the number of features</span>
            <span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>  <span class="c1"># 30 time-steps and `X.shape[2]` features per time-step</span>
            
            <span class="c1"># Create the LSTM model</span>
            <span class="n">classification_model</span> <span class="o">=</span> <span class="n">create_lstm_classification</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
            
            <span class="c1"># Initialize k-fold cross-validation</span>
            <span class="c1">#kf = KFold(n_splits=5, shuffle=True, random_state=6)  #regular 5-fold cross-validation w/out stratification</span>
            <span class="n">kf</span> <span class="o">=</span> <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>  <span class="c1"># 5-fold cross-validation with stratification</span>

            <span class="c1">#initialize to gather all the accuracy scores at each epoch for all 5 folds</span>
            <span class="n">fold_accuracies</span> <span class="o">=</span> <span class="p">[]</span>
            
            <span class="c1">#stratified K-fold Cross-Validation</span>
            <span class="n">counter_kfold</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">train_index</span><span class="p">,</span> <span class="n">val_index</span> <span class="ow">in</span> <span class="n">kf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span> <span class="c1">#used for stratified k-fold</span>
            <span class="c1">#for train_index, val_index in kf.split(X): #used for regular k-fold</span>
                
                <span class="n">counter_kfold</span> <span class="o">=</span> <span class="n">counter_kfold</span> <span class="o">+</span> <span class="mi">1</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Now running, pct_increase: </span><span class="si">{</span><span class="n">percent</span><span class="si">}</span><span class="s2">; days out: </span><span class="si">{</span><span class="n">day</span><span class="si">}</span><span class="s2">; independent_array: </span><span class="si">{</span><span class="n">counter_independentarray</span><span class="si">}</span><span class="s2">; K-fold: </span><span class="si">{</span><span class="n">counter_kfold</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
                
                <span class="n">X_train</span><span class="p">,</span> <span class="n">X_val</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">val_index</span><span class="p">]</span>
                <span class="n">y_train</span><span class="p">,</span> <span class="n">y_val</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">val_index</span><span class="p">]</span>
                
                <span class="c1"># Train the classification model and store the history; verbose = 0 to hide epoch running info in cell output</span>
                <span class="n">history</span> <span class="o">=</span> <span class="n">classification_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">),</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                
                <span class="c1"># Get the validation accuracies for this fold. What this does is that an accuracy score is calculated at each epoch,</span>
                <span class="c1">#and in this list I am getting all the accuracy scores from all five folds</span>
                <span class="n">val_accuracy</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">'val_accuracy'</span><span class="p">]</span>
                <span class="n">fold_accuracies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">val_accuracy</span><span class="p">)</span>
        
        
            
            <span class="c1"># Calculate the best and average validation accuracy across all folds</span>
            <span class="n">best_val_accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">fold_accuracies</span><span class="p">)</span> <span class="c1">#get the max accuracy across all epochs across all five folds</span>
            <span class="n">avg_val_accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">fold_accuracies</span><span class="p">)</span> <span class="c1">#get the mean accuracy across all epochs across all five folds</span>
            <span class="n">best_accuracy</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">best_val_accuracy</span><span class="p">)</span>
            <span class="n">avg_accuracy</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">avg_val_accuracy</span><span class="p">)</span>
        
        
        <span class="c1"># Example of new data to add</span>
        <span class="n">df_new</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
            <span class="s1">'ticker'</span><span class="p">:</span> <span class="n">ticker_symbol</span><span class="p">,</span>
            <span class="s1">'pattern'</span><span class="p">:</span> <span class="n">selected_pattern</span><span class="p">,</span>
            <span class="s1">'independent_array'</span><span class="p">:</span> <span class="n">independent_array</span><span class="p">,</span>
            <span class="s1">'best_accuracy'</span><span class="p">:</span> <span class="n">best_accuracy</span><span class="p">,</span>
            <span class="s1">'avg_accuracy'</span><span class="p">:</span> <span class="n">avg_accuracy</span><span class="p">,</span>
            <span class="s1">'days_out'</span><span class="p">:</span> <span class="n">day</span><span class="p">,</span>
            <span class="s1">'Total_observations'</span><span class="p">:</span> <span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">dependent_array</span><span class="p">,</span> <span class="n">return_counts</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">1</span><span class="p">]),</span>
            <span class="s1">'Negative_observations'</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">dependent_array</span><span class="p">,</span> <span class="n">return_counts</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
            <span class="s1">'Positive_observations'</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">dependent_array</span><span class="p">,</span> <span class="n">return_counts</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span>
            <span class="s1">'Percent_increase_parameter'</span><span class="p">:</span> <span class="n">percent</span>
        <span class="p">})</span>
    
        <span class="c1"># Concatenate the new data to the empty DataFrame</span>
        <span class="n">accuracy_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">accuracy_df</span><span class="p">,</span> <span class="n">df_new</span><span class="p">],</span> <span class="n">ignore_index</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 1; independent_array: 1; K-fold: 1
Now running, pct_increase: 1.0; days out: 1; independent_array: 1; K-fold: 2
Now running, pct_increase: 1.0; days out: 1; independent_array: 1; K-fold: 3
Now running, pct_increase: 1.0; days out: 1; independent_array: 1; K-fold: 4
Now running, pct_increase: 1.0; days out: 1; independent_array: 1; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 1; independent_array: 2; K-fold: 1
Now running, pct_increase: 1.0; days out: 1; independent_array: 2; K-fold: 2
Now running, pct_increase: 1.0; days out: 1; independent_array: 2; K-fold: 3
Now running, pct_increase: 1.0; days out: 1; independent_array: 2; K-fold: 4
Now running, pct_increase: 1.0; days out: 1; independent_array: 2; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 1; independent_array: 3; K-fold: 1
Now running, pct_increase: 1.0; days out: 1; independent_array: 3; K-fold: 2
Now running, pct_increase: 1.0; days out: 1; independent_array: 3; K-fold: 3
Now running, pct_increase: 1.0; days out: 1; independent_array: 3; K-fold: 4
Now running, pct_increase: 1.0; days out: 1; independent_array: 3; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 1; independent_array: 4; K-fold: 1
Now running, pct_increase: 1.0; days out: 1; independent_array: 4; K-fold: 2
Now running, pct_increase: 1.0; days out: 1; independent_array: 4; K-fold: 3
Now running, pct_increase: 1.0; days out: 1; independent_array: 4; K-fold: 4
Now running, pct_increase: 1.0; days out: 1; independent_array: 4; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 1; independent_array: 5; K-fold: 1
Now running, pct_increase: 1.0; days out: 1; independent_array: 5; K-fold: 2
Now running, pct_increase: 1.0; days out: 1; independent_array: 5; K-fold: 3
Now running, pct_increase: 1.0; days out: 1; independent_array: 5; K-fold: 4
Now running, pct_increase: 1.0; days out: 1; independent_array: 5; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 1; independent_array: 6; K-fold: 1
Now running, pct_increase: 1.0; days out: 1; independent_array: 6; K-fold: 2
Now running, pct_increase: 1.0; days out: 1; independent_array: 6; K-fold: 3
Now running, pct_increase: 1.0; days out: 1; independent_array: 6; K-fold: 4
Now running, pct_increase: 1.0; days out: 1; independent_array: 6; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 1; independent_array: 7; K-fold: 1
Now running, pct_increase: 1.0; days out: 1; independent_array: 7; K-fold: 2
Now running, pct_increase: 1.0; days out: 1; independent_array: 7; K-fold: 3
Now running, pct_increase: 1.0; days out: 1; independent_array: 7; K-fold: 4
Now running, pct_increase: 1.0; days out: 1; independent_array: 7; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 1; independent_array: 8; K-fold: 1
Now running, pct_increase: 1.0; days out: 1; independent_array: 8; K-fold: 2
Now running, pct_increase: 1.0; days out: 1; independent_array: 8; K-fold: 3
Now running, pct_increase: 1.0; days out: 1; independent_array: 8; K-fold: 4
Now running, pct_increase: 1.0; days out: 1; independent_array: 8; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 1; independent_array: 9; K-fold: 1
Now running, pct_increase: 1.0; days out: 1; independent_array: 9; K-fold: 2
Now running, pct_increase: 1.0; days out: 1; independent_array: 9; K-fold: 3
Now running, pct_increase: 1.0; days out: 1; independent_array: 9; K-fold: 4
Now running, pct_increase: 1.0; days out: 1; independent_array: 9; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 1; independent_array: 10; K-fold: 1
Now running, pct_increase: 1.0; days out: 1; independent_array: 10; K-fold: 2
Now running, pct_increase: 1.0; days out: 1; independent_array: 10; K-fold: 3
Now running, pct_increase: 1.0; days out: 1; independent_array: 10; K-fold: 4
Now running, pct_increase: 1.0; days out: 1; independent_array: 10; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 1; independent_array: 11; K-fold: 1
Now running, pct_increase: 1.0; days out: 1; independent_array: 11; K-fold: 2
Now running, pct_increase: 1.0; days out: 1; independent_array: 11; K-fold: 3
Now running, pct_increase: 1.0; days out: 1; independent_array: 11; K-fold: 4
Now running, pct_increase: 1.0; days out: 1; independent_array: 11; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 1; independent_array: 12; K-fold: 1
Now running, pct_increase: 1.0; days out: 1; independent_array: 12; K-fold: 2
Now running, pct_increase: 1.0; days out: 1; independent_array: 12; K-fold: 3
Now running, pct_increase: 1.0; days out: 1; independent_array: 12; K-fold: 4
Now running, pct_increase: 1.0; days out: 1; independent_array: 12; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 1; independent_array: 13; K-fold: 1
Now running, pct_increase: 1.0; days out: 1; independent_array: 13; K-fold: 2
Now running, pct_increase: 1.0; days out: 1; independent_array: 13; K-fold: 3
Now running, pct_increase: 1.0; days out: 1; independent_array: 13; K-fold: 4
Now running, pct_increase: 1.0; days out: 1; independent_array: 13; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 1; independent_array: 14; K-fold: 1
Now running, pct_increase: 1.0; days out: 1; independent_array: 14; K-fold: 2
Now running, pct_increase: 1.0; days out: 1; independent_array: 14; K-fold: 3
Now running, pct_increase: 1.0; days out: 1; independent_array: 14; K-fold: 4
Now running, pct_increase: 1.0; days out: 1; independent_array: 14; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 1; independent_array: 15; K-fold: 1
Now running, pct_increase: 1.0; days out: 1; independent_array: 15; K-fold: 2
Now running, pct_increase: 1.0; days out: 1; independent_array: 15; K-fold: 3
Now running, pct_increase: 1.0; days out: 1; independent_array: 15; K-fold: 4
Now running, pct_increase: 1.0; days out: 1; independent_array: 15; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\AppData\Local\Temp\ipykernel_27512\1563598923.py:311: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  accuracy_df = pd.concat([accuracy_df, df_new], ignore_index=True)
C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 3; independent_array: 1; K-fold: 1
Now running, pct_increase: 1.0; days out: 3; independent_array: 1; K-fold: 2
Now running, pct_increase: 1.0; days out: 3; independent_array: 1; K-fold: 3
Now running, pct_increase: 1.0; days out: 3; independent_array: 1; K-fold: 4
Now running, pct_increase: 1.0; days out: 3; independent_array: 1; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 3; independent_array: 2; K-fold: 1
Now running, pct_increase: 1.0; days out: 3; independent_array: 2; K-fold: 2
Now running, pct_increase: 1.0; days out: 3; independent_array: 2; K-fold: 3
Now running, pct_increase: 1.0; days out: 3; independent_array: 2; K-fold: 4
Now running, pct_increase: 1.0; days out: 3; independent_array: 2; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 3; independent_array: 3; K-fold: 1
Now running, pct_increase: 1.0; days out: 3; independent_array: 3; K-fold: 2
Now running, pct_increase: 1.0; days out: 3; independent_array: 3; K-fold: 3
Now running, pct_increase: 1.0; days out: 3; independent_array: 3; K-fold: 4
Now running, pct_increase: 1.0; days out: 3; independent_array: 3; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 3; independent_array: 4; K-fold: 1
Now running, pct_increase: 1.0; days out: 3; independent_array: 4; K-fold: 2
Now running, pct_increase: 1.0; days out: 3; independent_array: 4; K-fold: 3
Now running, pct_increase: 1.0; days out: 3; independent_array: 4; K-fold: 4
Now running, pct_increase: 1.0; days out: 3; independent_array: 4; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 3; independent_array: 5; K-fold: 1
Now running, pct_increase: 1.0; days out: 3; independent_array: 5; K-fold: 2
Now running, pct_increase: 1.0; days out: 3; independent_array: 5; K-fold: 3
Now running, pct_increase: 1.0; days out: 3; independent_array: 5; K-fold: 4
Now running, pct_increase: 1.0; days out: 3; independent_array: 5; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 3; independent_array: 6; K-fold: 1
Now running, pct_increase: 1.0; days out: 3; independent_array: 6; K-fold: 2
Now running, pct_increase: 1.0; days out: 3; independent_array: 6; K-fold: 3
Now running, pct_increase: 1.0; days out: 3; independent_array: 6; K-fold: 4
Now running, pct_increase: 1.0; days out: 3; independent_array: 6; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 3; independent_array: 7; K-fold: 1
Now running, pct_increase: 1.0; days out: 3; independent_array: 7; K-fold: 2
Now running, pct_increase: 1.0; days out: 3; independent_array: 7; K-fold: 3
Now running, pct_increase: 1.0; days out: 3; independent_array: 7; K-fold: 4
Now running, pct_increase: 1.0; days out: 3; independent_array: 7; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 3; independent_array: 8; K-fold: 1
Now running, pct_increase: 1.0; days out: 3; independent_array: 8; K-fold: 2
Now running, pct_increase: 1.0; days out: 3; independent_array: 8; K-fold: 3
Now running, pct_increase: 1.0; days out: 3; independent_array: 8; K-fold: 4
Now running, pct_increase: 1.0; days out: 3; independent_array: 8; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 3; independent_array: 9; K-fold: 1
Now running, pct_increase: 1.0; days out: 3; independent_array: 9; K-fold: 2
Now running, pct_increase: 1.0; days out: 3; independent_array: 9; K-fold: 3
Now running, pct_increase: 1.0; days out: 3; independent_array: 9; K-fold: 4
Now running, pct_increase: 1.0; days out: 3; independent_array: 9; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 3; independent_array: 10; K-fold: 1
Now running, pct_increase: 1.0; days out: 3; independent_array: 10; K-fold: 2
Now running, pct_increase: 1.0; days out: 3; independent_array: 10; K-fold: 3
Now running, pct_increase: 1.0; days out: 3; independent_array: 10; K-fold: 4
Now running, pct_increase: 1.0; days out: 3; independent_array: 10; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 3; independent_array: 11; K-fold: 1
Now running, pct_increase: 1.0; days out: 3; independent_array: 11; K-fold: 2
Now running, pct_increase: 1.0; days out: 3; independent_array: 11; K-fold: 3
Now running, pct_increase: 1.0; days out: 3; independent_array: 11; K-fold: 4
Now running, pct_increase: 1.0; days out: 3; independent_array: 11; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 3; independent_array: 12; K-fold: 1
Now running, pct_increase: 1.0; days out: 3; independent_array: 12; K-fold: 2
Now running, pct_increase: 1.0; days out: 3; independent_array: 12; K-fold: 3
Now running, pct_increase: 1.0; days out: 3; independent_array: 12; K-fold: 4
Now running, pct_increase: 1.0; days out: 3; independent_array: 12; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 3; independent_array: 13; K-fold: 1
Now running, pct_increase: 1.0; days out: 3; independent_array: 13; K-fold: 2
Now running, pct_increase: 1.0; days out: 3; independent_array: 13; K-fold: 3
Now running, pct_increase: 1.0; days out: 3; independent_array: 13; K-fold: 4
Now running, pct_increase: 1.0; days out: 3; independent_array: 13; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 3; independent_array: 14; K-fold: 1
Now running, pct_increase: 1.0; days out: 3; independent_array: 14; K-fold: 2
Now running, pct_increase: 1.0; days out: 3; independent_array: 14; K-fold: 3
Now running, pct_increase: 1.0; days out: 3; independent_array: 14; K-fold: 4
Now running, pct_increase: 1.0; days out: 3; independent_array: 14; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 3; independent_array: 15; K-fold: 1
Now running, pct_increase: 1.0; days out: 3; independent_array: 15; K-fold: 2
Now running, pct_increase: 1.0; days out: 3; independent_array: 15; K-fold: 3
Now running, pct_increase: 1.0; days out: 3; independent_array: 15; K-fold: 4
Now running, pct_increase: 1.0; days out: 3; independent_array: 15; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 5; independent_array: 1; K-fold: 1
Now running, pct_increase: 1.0; days out: 5; independent_array: 1; K-fold: 2
Now running, pct_increase: 1.0; days out: 5; independent_array: 1; K-fold: 3
Now running, pct_increase: 1.0; days out: 5; independent_array: 1; K-fold: 4
Now running, pct_increase: 1.0; days out: 5; independent_array: 1; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 5; independent_array: 2; K-fold: 1
Now running, pct_increase: 1.0; days out: 5; independent_array: 2; K-fold: 2
Now running, pct_increase: 1.0; days out: 5; independent_array: 2; K-fold: 3
Now running, pct_increase: 1.0; days out: 5; independent_array: 2; K-fold: 4
Now running, pct_increase: 1.0; days out: 5; independent_array: 2; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 5; independent_array: 3; K-fold: 1
Now running, pct_increase: 1.0; days out: 5; independent_array: 3; K-fold: 2
Now running, pct_increase: 1.0; days out: 5; independent_array: 3; K-fold: 3
Now running, pct_increase: 1.0; days out: 5; independent_array: 3; K-fold: 4
Now running, pct_increase: 1.0; days out: 5; independent_array: 3; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 5; independent_array: 4; K-fold: 1
Now running, pct_increase: 1.0; days out: 5; independent_array: 4; K-fold: 2
Now running, pct_increase: 1.0; days out: 5; independent_array: 4; K-fold: 3
Now running, pct_increase: 1.0; days out: 5; independent_array: 4; K-fold: 4
Now running, pct_increase: 1.0; days out: 5; independent_array: 4; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 5; independent_array: 5; K-fold: 1
Now running, pct_increase: 1.0; days out: 5; independent_array: 5; K-fold: 2
Now running, pct_increase: 1.0; days out: 5; independent_array: 5; K-fold: 3
Now running, pct_increase: 1.0; days out: 5; independent_array: 5; K-fold: 4
Now running, pct_increase: 1.0; days out: 5; independent_array: 5; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 5; independent_array: 6; K-fold: 1
Now running, pct_increase: 1.0; days out: 5; independent_array: 6; K-fold: 2
Now running, pct_increase: 1.0; days out: 5; independent_array: 6; K-fold: 3
Now running, pct_increase: 1.0; days out: 5; independent_array: 6; K-fold: 4
Now running, pct_increase: 1.0; days out: 5; independent_array: 6; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 5; independent_array: 7; K-fold: 1
Now running, pct_increase: 1.0; days out: 5; independent_array: 7; K-fold: 2
Now running, pct_increase: 1.0; days out: 5; independent_array: 7; K-fold: 3
Now running, pct_increase: 1.0; days out: 5; independent_array: 7; K-fold: 4
Now running, pct_increase: 1.0; days out: 5; independent_array: 7; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 5; independent_array: 8; K-fold: 1
Now running, pct_increase: 1.0; days out: 5; independent_array: 8; K-fold: 2
Now running, pct_increase: 1.0; days out: 5; independent_array: 8; K-fold: 3
Now running, pct_increase: 1.0; days out: 5; independent_array: 8; K-fold: 4
Now running, pct_increase: 1.0; days out: 5; independent_array: 8; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 5; independent_array: 9; K-fold: 1
Now running, pct_increase: 1.0; days out: 5; independent_array: 9; K-fold: 2
Now running, pct_increase: 1.0; days out: 5; independent_array: 9; K-fold: 3
Now running, pct_increase: 1.0; days out: 5; independent_array: 9; K-fold: 4
Now running, pct_increase: 1.0; days out: 5; independent_array: 9; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 5; independent_array: 10; K-fold: 1
Now running, pct_increase: 1.0; days out: 5; independent_array: 10; K-fold: 2
Now running, pct_increase: 1.0; days out: 5; independent_array: 10; K-fold: 3
Now running, pct_increase: 1.0; days out: 5; independent_array: 10; K-fold: 4
Now running, pct_increase: 1.0; days out: 5; independent_array: 10; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 5; independent_array: 11; K-fold: 1
Now running, pct_increase: 1.0; days out: 5; independent_array: 11; K-fold: 2
Now running, pct_increase: 1.0; days out: 5; independent_array: 11; K-fold: 3
Now running, pct_increase: 1.0; days out: 5; independent_array: 11; K-fold: 4
Now running, pct_increase: 1.0; days out: 5; independent_array: 11; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 5; independent_array: 12; K-fold: 1
Now running, pct_increase: 1.0; days out: 5; independent_array: 12; K-fold: 2
Now running, pct_increase: 1.0; days out: 5; independent_array: 12; K-fold: 3
Now running, pct_increase: 1.0; days out: 5; independent_array: 12; K-fold: 4
Now running, pct_increase: 1.0; days out: 5; independent_array: 12; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 5; independent_array: 13; K-fold: 1
Now running, pct_increase: 1.0; days out: 5; independent_array: 13; K-fold: 2
Now running, pct_increase: 1.0; days out: 5; independent_array: 13; K-fold: 3
Now running, pct_increase: 1.0; days out: 5; independent_array: 13; K-fold: 4
Now running, pct_increase: 1.0; days out: 5; independent_array: 13; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 5; independent_array: 14; K-fold: 1
Now running, pct_increase: 1.0; days out: 5; independent_array: 14; K-fold: 2
Now running, pct_increase: 1.0; days out: 5; independent_array: 14; K-fold: 3
Now running, pct_increase: 1.0; days out: 5; independent_array: 14; K-fold: 4
Now running, pct_increase: 1.0; days out: 5; independent_array: 14; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 5; independent_array: 15; K-fold: 1
Now running, pct_increase: 1.0; days out: 5; independent_array: 15; K-fold: 2
Now running, pct_increase: 1.0; days out: 5; independent_array: 15; K-fold: 3
Now running, pct_increase: 1.0; days out: 5; independent_array: 15; K-fold: 4
Now running, pct_increase: 1.0; days out: 5; independent_array: 15; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 10; independent_array: 1; K-fold: 1
Now running, pct_increase: 1.0; days out: 10; independent_array: 1; K-fold: 2
Now running, pct_increase: 1.0; days out: 10; independent_array: 1; K-fold: 3
Now running, pct_increase: 1.0; days out: 10; independent_array: 1; K-fold: 4
Now running, pct_increase: 1.0; days out: 10; independent_array: 1; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 10; independent_array: 2; K-fold: 1
Now running, pct_increase: 1.0; days out: 10; independent_array: 2; K-fold: 2
Now running, pct_increase: 1.0; days out: 10; independent_array: 2; K-fold: 3
Now running, pct_increase: 1.0; days out: 10; independent_array: 2; K-fold: 4
Now running, pct_increase: 1.0; days out: 10; independent_array: 2; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 10; independent_array: 3; K-fold: 1
Now running, pct_increase: 1.0; days out: 10; independent_array: 3; K-fold: 2
Now running, pct_increase: 1.0; days out: 10; independent_array: 3; K-fold: 3
Now running, pct_increase: 1.0; days out: 10; independent_array: 3; K-fold: 4
Now running, pct_increase: 1.0; days out: 10; independent_array: 3; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 10; independent_array: 4; K-fold: 1
Now running, pct_increase: 1.0; days out: 10; independent_array: 4; K-fold: 2
Now running, pct_increase: 1.0; days out: 10; independent_array: 4; K-fold: 3
Now running, pct_increase: 1.0; days out: 10; independent_array: 4; K-fold: 4
Now running, pct_increase: 1.0; days out: 10; independent_array: 4; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 10; independent_array: 5; K-fold: 1
Now running, pct_increase: 1.0; days out: 10; independent_array: 5; K-fold: 2
Now running, pct_increase: 1.0; days out: 10; independent_array: 5; K-fold: 3
Now running, pct_increase: 1.0; days out: 10; independent_array: 5; K-fold: 4
Now running, pct_increase: 1.0; days out: 10; independent_array: 5; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 10; independent_array: 6; K-fold: 1
Now running, pct_increase: 1.0; days out: 10; independent_array: 6; K-fold: 2
Now running, pct_increase: 1.0; days out: 10; independent_array: 6; K-fold: 3
Now running, pct_increase: 1.0; days out: 10; independent_array: 6; K-fold: 4
Now running, pct_increase: 1.0; days out: 10; independent_array: 6; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 10; independent_array: 7; K-fold: 1
Now running, pct_increase: 1.0; days out: 10; independent_array: 7; K-fold: 2
Now running, pct_increase: 1.0; days out: 10; independent_array: 7; K-fold: 3
Now running, pct_increase: 1.0; days out: 10; independent_array: 7; K-fold: 4
Now running, pct_increase: 1.0; days out: 10; independent_array: 7; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 10; independent_array: 8; K-fold: 1
Now running, pct_increase: 1.0; days out: 10; independent_array: 8; K-fold: 2
Now running, pct_increase: 1.0; days out: 10; independent_array: 8; K-fold: 3
Now running, pct_increase: 1.0; days out: 10; independent_array: 8; K-fold: 4
Now running, pct_increase: 1.0; days out: 10; independent_array: 8; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 10; independent_array: 9; K-fold: 1
Now running, pct_increase: 1.0; days out: 10; independent_array: 9; K-fold: 2
Now running, pct_increase: 1.0; days out: 10; independent_array: 9; K-fold: 3
Now running, pct_increase: 1.0; days out: 10; independent_array: 9; K-fold: 4
Now running, pct_increase: 1.0; days out: 10; independent_array: 9; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 10; independent_array: 10; K-fold: 1
Now running, pct_increase: 1.0; days out: 10; independent_array: 10; K-fold: 2
Now running, pct_increase: 1.0; days out: 10; independent_array: 10; K-fold: 3
Now running, pct_increase: 1.0; days out: 10; independent_array: 10; K-fold: 4
Now running, pct_increase: 1.0; days out: 10; independent_array: 10; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 10; independent_array: 11; K-fold: 1
Now running, pct_increase: 1.0; days out: 10; independent_array: 11; K-fold: 2
Now running, pct_increase: 1.0; days out: 10; independent_array: 11; K-fold: 3
Now running, pct_increase: 1.0; days out: 10; independent_array: 11; K-fold: 4
Now running, pct_increase: 1.0; days out: 10; independent_array: 11; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 10; independent_array: 12; K-fold: 1
Now running, pct_increase: 1.0; days out: 10; independent_array: 12; K-fold: 2
Now running, pct_increase: 1.0; days out: 10; independent_array: 12; K-fold: 3
Now running, pct_increase: 1.0; days out: 10; independent_array: 12; K-fold: 4
Now running, pct_increase: 1.0; days out: 10; independent_array: 12; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 10; independent_array: 13; K-fold: 1
Now running, pct_increase: 1.0; days out: 10; independent_array: 13; K-fold: 2
Now running, pct_increase: 1.0; days out: 10; independent_array: 13; K-fold: 3
Now running, pct_increase: 1.0; days out: 10; independent_array: 13; K-fold: 4
Now running, pct_increase: 1.0; days out: 10; independent_array: 13; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 10; independent_array: 14; K-fold: 1
Now running, pct_increase: 1.0; days out: 10; independent_array: 14; K-fold: 2
Now running, pct_increase: 1.0; days out: 10; independent_array: 14; K-fold: 3
Now running, pct_increase: 1.0; days out: 10; independent_array: 14; K-fold: 4
Now running, pct_increase: 1.0; days out: 10; independent_array: 14; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 10; independent_array: 15; K-fold: 1
Now running, pct_increase: 1.0; days out: 10; independent_array: 15; K-fold: 2
Now running, pct_increase: 1.0; days out: 10; independent_array: 15; K-fold: 3
Now running, pct_increase: 1.0; days out: 10; independent_array: 15; K-fold: 4
Now running, pct_increase: 1.0; days out: 10; independent_array: 15; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 15; independent_array: 1; K-fold: 1
Now running, pct_increase: 1.0; days out: 15; independent_array: 1; K-fold: 2
Now running, pct_increase: 1.0; days out: 15; independent_array: 1; K-fold: 3
Now running, pct_increase: 1.0; days out: 15; independent_array: 1; K-fold: 4
Now running, pct_increase: 1.0; days out: 15; independent_array: 1; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 15; independent_array: 2; K-fold: 1
Now running, pct_increase: 1.0; days out: 15; independent_array: 2; K-fold: 2
Now running, pct_increase: 1.0; days out: 15; independent_array: 2; K-fold: 3
Now running, pct_increase: 1.0; days out: 15; independent_array: 2; K-fold: 4
Now running, pct_increase: 1.0; days out: 15; independent_array: 2; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 15; independent_array: 3; K-fold: 1
Now running, pct_increase: 1.0; days out: 15; independent_array: 3; K-fold: 2
Now running, pct_increase: 1.0; days out: 15; independent_array: 3; K-fold: 3
Now running, pct_increase: 1.0; days out: 15; independent_array: 3; K-fold: 4
Now running, pct_increase: 1.0; days out: 15; independent_array: 3; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 15; independent_array: 4; K-fold: 1
Now running, pct_increase: 1.0; days out: 15; independent_array: 4; K-fold: 2
Now running, pct_increase: 1.0; days out: 15; independent_array: 4; K-fold: 3
Now running, pct_increase: 1.0; days out: 15; independent_array: 4; K-fold: 4
Now running, pct_increase: 1.0; days out: 15; independent_array: 4; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 15; independent_array: 5; K-fold: 1
Now running, pct_increase: 1.0; days out: 15; independent_array: 5; K-fold: 2
Now running, pct_increase: 1.0; days out: 15; independent_array: 5; K-fold: 3
Now running, pct_increase: 1.0; days out: 15; independent_array: 5; K-fold: 4
Now running, pct_increase: 1.0; days out: 15; independent_array: 5; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 15; independent_array: 6; K-fold: 1
Now running, pct_increase: 1.0; days out: 15; independent_array: 6; K-fold: 2
Now running, pct_increase: 1.0; days out: 15; independent_array: 6; K-fold: 3
Now running, pct_increase: 1.0; days out: 15; independent_array: 6; K-fold: 4
Now running, pct_increase: 1.0; days out: 15; independent_array: 6; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 15; independent_array: 7; K-fold: 1
Now running, pct_increase: 1.0; days out: 15; independent_array: 7; K-fold: 2
Now running, pct_increase: 1.0; days out: 15; independent_array: 7; K-fold: 3
Now running, pct_increase: 1.0; days out: 15; independent_array: 7; K-fold: 4
Now running, pct_increase: 1.0; days out: 15; independent_array: 7; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 15; independent_array: 8; K-fold: 1
Now running, pct_increase: 1.0; days out: 15; independent_array: 8; K-fold: 2
Now running, pct_increase: 1.0; days out: 15; independent_array: 8; K-fold: 3
Now running, pct_increase: 1.0; days out: 15; independent_array: 8; K-fold: 4
Now running, pct_increase: 1.0; days out: 15; independent_array: 8; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 15; independent_array: 9; K-fold: 1
Now running, pct_increase: 1.0; days out: 15; independent_array: 9; K-fold: 2
Now running, pct_increase: 1.0; days out: 15; independent_array: 9; K-fold: 3
Now running, pct_increase: 1.0; days out: 15; independent_array: 9; K-fold: 4
Now running, pct_increase: 1.0; days out: 15; independent_array: 9; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 15; independent_array: 10; K-fold: 1
Now running, pct_increase: 1.0; days out: 15; independent_array: 10; K-fold: 2
Now running, pct_increase: 1.0; days out: 15; independent_array: 10; K-fold: 3
Now running, pct_increase: 1.0; days out: 15; independent_array: 10; K-fold: 4
Now running, pct_increase: 1.0; days out: 15; independent_array: 10; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 15; independent_array: 11; K-fold: 1
Now running, pct_increase: 1.0; days out: 15; independent_array: 11; K-fold: 2
Now running, pct_increase: 1.0; days out: 15; independent_array: 11; K-fold: 3
Now running, pct_increase: 1.0; days out: 15; independent_array: 11; K-fold: 4
Now running, pct_increase: 1.0; days out: 15; independent_array: 11; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 15; independent_array: 12; K-fold: 1
Now running, pct_increase: 1.0; days out: 15; independent_array: 12; K-fold: 2
Now running, pct_increase: 1.0; days out: 15; independent_array: 12; K-fold: 3
Now running, pct_increase: 1.0; days out: 15; independent_array: 12; K-fold: 4
Now running, pct_increase: 1.0; days out: 15; independent_array: 12; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 15; independent_array: 13; K-fold: 1
Now running, pct_increase: 1.0; days out: 15; independent_array: 13; K-fold: 2
Now running, pct_increase: 1.0; days out: 15; independent_array: 13; K-fold: 3
Now running, pct_increase: 1.0; days out: 15; independent_array: 13; K-fold: 4
Now running, pct_increase: 1.0; days out: 15; independent_array: 13; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 15; independent_array: 14; K-fold: 1
Now running, pct_increase: 1.0; days out: 15; independent_array: 14; K-fold: 2
Now running, pct_increase: 1.0; days out: 15; independent_array: 14; K-fold: 3
Now running, pct_increase: 1.0; days out: 15; independent_array: 14; K-fold: 4
Now running, pct_increase: 1.0; days out: 15; independent_array: 14; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 15; independent_array: 15; K-fold: 1
Now running, pct_increase: 1.0; days out: 15; independent_array: 15; K-fold: 2
Now running, pct_increase: 1.0; days out: 15; independent_array: 15; K-fold: 3
Now running, pct_increase: 1.0; days out: 15; independent_array: 15; K-fold: 4
Now running, pct_increase: 1.0; days out: 15; independent_array: 15; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 1; independent_array: 1; K-fold: 1
Now running, pct_increase: 1.01; days out: 1; independent_array: 1; K-fold: 2
Now running, pct_increase: 1.01; days out: 1; independent_array: 1; K-fold: 3
Now running, pct_increase: 1.01; days out: 1; independent_array: 1; K-fold: 4
Now running, pct_increase: 1.01; days out: 1; independent_array: 1; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 1; independent_array: 2; K-fold: 1
Now running, pct_increase: 1.01; days out: 1; independent_array: 2; K-fold: 2
Now running, pct_increase: 1.01; days out: 1; independent_array: 2; K-fold: 3
Now running, pct_increase: 1.01; days out: 1; independent_array: 2; K-fold: 4
Now running, pct_increase: 1.01; days out: 1; independent_array: 2; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 1; independent_array: 3; K-fold: 1
Now running, pct_increase: 1.01; days out: 1; independent_array: 3; K-fold: 2
Now running, pct_increase: 1.01; days out: 1; independent_array: 3; K-fold: 3
Now running, pct_increase: 1.01; days out: 1; independent_array: 3; K-fold: 4
Now running, pct_increase: 1.01; days out: 1; independent_array: 3; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 1; independent_array: 4; K-fold: 1
Now running, pct_increase: 1.01; days out: 1; independent_array: 4; K-fold: 2
Now running, pct_increase: 1.01; days out: 1; independent_array: 4; K-fold: 3
Now running, pct_increase: 1.01; days out: 1; independent_array: 4; K-fold: 4
Now running, pct_increase: 1.01; days out: 1; independent_array: 4; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 1; independent_array: 5; K-fold: 1
Now running, pct_increase: 1.01; days out: 1; independent_array: 5; K-fold: 2
Now running, pct_increase: 1.01; days out: 1; independent_array: 5; K-fold: 3
Now running, pct_increase: 1.01; days out: 1; independent_array: 5; K-fold: 4
Now running, pct_increase: 1.01; days out: 1; independent_array: 5; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 1; independent_array: 6; K-fold: 1
Now running, pct_increase: 1.01; days out: 1; independent_array: 6; K-fold: 2
Now running, pct_increase: 1.01; days out: 1; independent_array: 6; K-fold: 3
Now running, pct_increase: 1.01; days out: 1; independent_array: 6; K-fold: 4
Now running, pct_increase: 1.01; days out: 1; independent_array: 6; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 1; independent_array: 7; K-fold: 1
Now running, pct_increase: 1.01; days out: 1; independent_array: 7; K-fold: 2
Now running, pct_increase: 1.01; days out: 1; independent_array: 7; K-fold: 3
Now running, pct_increase: 1.01; days out: 1; independent_array: 7; K-fold: 4
Now running, pct_increase: 1.01; days out: 1; independent_array: 7; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 1; independent_array: 8; K-fold: 1
Now running, pct_increase: 1.01; days out: 1; independent_array: 8; K-fold: 2
Now running, pct_increase: 1.01; days out: 1; independent_array: 8; K-fold: 3
Now running, pct_increase: 1.01; days out: 1; independent_array: 8; K-fold: 4
Now running, pct_increase: 1.01; days out: 1; independent_array: 8; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 1; independent_array: 9; K-fold: 1
Now running, pct_increase: 1.01; days out: 1; independent_array: 9; K-fold: 2
Now running, pct_increase: 1.01; days out: 1; independent_array: 9; K-fold: 3
Now running, pct_increase: 1.01; days out: 1; independent_array: 9; K-fold: 4
Now running, pct_increase: 1.01; days out: 1; independent_array: 9; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 1; independent_array: 10; K-fold: 1
Now running, pct_increase: 1.01; days out: 1; independent_array: 10; K-fold: 2
Now running, pct_increase: 1.01; days out: 1; independent_array: 10; K-fold: 3
Now running, pct_increase: 1.01; days out: 1; independent_array: 10; K-fold: 4
Now running, pct_increase: 1.01; days out: 1; independent_array: 10; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 1; independent_array: 11; K-fold: 1
Now running, pct_increase: 1.01; days out: 1; independent_array: 11; K-fold: 2
Now running, pct_increase: 1.01; days out: 1; independent_array: 11; K-fold: 3
Now running, pct_increase: 1.01; days out: 1; independent_array: 11; K-fold: 4
Now running, pct_increase: 1.01; days out: 1; independent_array: 11; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 1; independent_array: 12; K-fold: 1
Now running, pct_increase: 1.01; days out: 1; independent_array: 12; K-fold: 2
Now running, pct_increase: 1.01; days out: 1; independent_array: 12; K-fold: 3
Now running, pct_increase: 1.01; days out: 1; independent_array: 12; K-fold: 4
Now running, pct_increase: 1.01; days out: 1; independent_array: 12; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 1; independent_array: 13; K-fold: 1
Now running, pct_increase: 1.01; days out: 1; independent_array: 13; K-fold: 2
Now running, pct_increase: 1.01; days out: 1; independent_array: 13; K-fold: 3
Now running, pct_increase: 1.01; days out: 1; independent_array: 13; K-fold: 4
Now running, pct_increase: 1.01; days out: 1; independent_array: 13; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 1; independent_array: 14; K-fold: 1
Now running, pct_increase: 1.01; days out: 1; independent_array: 14; K-fold: 2
Now running, pct_increase: 1.01; days out: 1; independent_array: 14; K-fold: 3
Now running, pct_increase: 1.01; days out: 1; independent_array: 14; K-fold: 4
Now running, pct_increase: 1.01; days out: 1; independent_array: 14; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 1; independent_array: 15; K-fold: 1
Now running, pct_increase: 1.01; days out: 1; independent_array: 15; K-fold: 2
Now running, pct_increase: 1.01; days out: 1; independent_array: 15; K-fold: 3
Now running, pct_increase: 1.01; days out: 1; independent_array: 15; K-fold: 4
Now running, pct_increase: 1.01; days out: 1; independent_array: 15; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 3; independent_array: 1; K-fold: 1
Now running, pct_increase: 1.01; days out: 3; independent_array: 1; K-fold: 2
Now running, pct_increase: 1.01; days out: 3; independent_array: 1; K-fold: 3
Now running, pct_increase: 1.01; days out: 3; independent_array: 1; K-fold: 4
Now running, pct_increase: 1.01; days out: 3; independent_array: 1; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 3; independent_array: 2; K-fold: 1
Now running, pct_increase: 1.01; days out: 3; independent_array: 2; K-fold: 2
Now running, pct_increase: 1.01; days out: 3; independent_array: 2; K-fold: 3
Now running, pct_increase: 1.01; days out: 3; independent_array: 2; K-fold: 4
Now running, pct_increase: 1.01; days out: 3; independent_array: 2; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 3; independent_array: 3; K-fold: 1
Now running, pct_increase: 1.01; days out: 3; independent_array: 3; K-fold: 2
Now running, pct_increase: 1.01; days out: 3; independent_array: 3; K-fold: 3
Now running, pct_increase: 1.01; days out: 3; independent_array: 3; K-fold: 4
Now running, pct_increase: 1.01; days out: 3; independent_array: 3; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 3; independent_array: 4; K-fold: 1
Now running, pct_increase: 1.01; days out: 3; independent_array: 4; K-fold: 2
Now running, pct_increase: 1.01; days out: 3; independent_array: 4; K-fold: 3
Now running, pct_increase: 1.01; days out: 3; independent_array: 4; K-fold: 4
Now running, pct_increase: 1.01; days out: 3; independent_array: 4; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 3; independent_array: 5; K-fold: 1
Now running, pct_increase: 1.01; days out: 3; independent_array: 5; K-fold: 2
Now running, pct_increase: 1.01; days out: 3; independent_array: 5; K-fold: 3
Now running, pct_increase: 1.01; days out: 3; independent_array: 5; K-fold: 4
Now running, pct_increase: 1.01; days out: 3; independent_array: 5; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 3; independent_array: 6; K-fold: 1
Now running, pct_increase: 1.01; days out: 3; independent_array: 6; K-fold: 2
Now running, pct_increase: 1.01; days out: 3; independent_array: 6; K-fold: 3
Now running, pct_increase: 1.01; days out: 3; independent_array: 6; K-fold: 4
Now running, pct_increase: 1.01; days out: 3; independent_array: 6; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 3; independent_array: 7; K-fold: 1
Now running, pct_increase: 1.01; days out: 3; independent_array: 7; K-fold: 2
Now running, pct_increase: 1.01; days out: 3; independent_array: 7; K-fold: 3
Now running, pct_increase: 1.01; days out: 3; independent_array: 7; K-fold: 4
Now running, pct_increase: 1.01; days out: 3; independent_array: 7; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 3; independent_array: 8; K-fold: 1
Now running, pct_increase: 1.01; days out: 3; independent_array: 8; K-fold: 2
Now running, pct_increase: 1.01; days out: 3; independent_array: 8; K-fold: 3
Now running, pct_increase: 1.01; days out: 3; independent_array: 8; K-fold: 4
Now running, pct_increase: 1.01; days out: 3; independent_array: 8; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 3; independent_array: 9; K-fold: 1
Now running, pct_increase: 1.01; days out: 3; independent_array: 9; K-fold: 2
Now running, pct_increase: 1.01; days out: 3; independent_array: 9; K-fold: 3
Now running, pct_increase: 1.01; days out: 3; independent_array: 9; K-fold: 4
Now running, pct_increase: 1.01; days out: 3; independent_array: 9; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 3; independent_array: 10; K-fold: 1
Now running, pct_increase: 1.01; days out: 3; independent_array: 10; K-fold: 2
Now running, pct_increase: 1.01; days out: 3; independent_array: 10; K-fold: 3
Now running, pct_increase: 1.01; days out: 3; independent_array: 10; K-fold: 4
Now running, pct_increase: 1.01; days out: 3; independent_array: 10; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 3; independent_array: 11; K-fold: 1
Now running, pct_increase: 1.01; days out: 3; independent_array: 11; K-fold: 2
Now running, pct_increase: 1.01; days out: 3; independent_array: 11; K-fold: 3
Now running, pct_increase: 1.01; days out: 3; independent_array: 11; K-fold: 4
Now running, pct_increase: 1.01; days out: 3; independent_array: 11; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 3; independent_array: 12; K-fold: 1
Now running, pct_increase: 1.01; days out: 3; independent_array: 12; K-fold: 2
Now running, pct_increase: 1.01; days out: 3; independent_array: 12; K-fold: 3
Now running, pct_increase: 1.01; days out: 3; independent_array: 12; K-fold: 4
Now running, pct_increase: 1.01; days out: 3; independent_array: 12; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 3; independent_array: 13; K-fold: 1
Now running, pct_increase: 1.01; days out: 3; independent_array: 13; K-fold: 2
Now running, pct_increase: 1.01; days out: 3; independent_array: 13; K-fold: 3
Now running, pct_increase: 1.01; days out: 3; independent_array: 13; K-fold: 4
Now running, pct_increase: 1.01; days out: 3; independent_array: 13; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 3; independent_array: 14; K-fold: 1
Now running, pct_increase: 1.01; days out: 3; independent_array: 14; K-fold: 2
Now running, pct_increase: 1.01; days out: 3; independent_array: 14; K-fold: 3
Now running, pct_increase: 1.01; days out: 3; independent_array: 14; K-fold: 4
Now running, pct_increase: 1.01; days out: 3; independent_array: 14; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 3; independent_array: 15; K-fold: 1
Now running, pct_increase: 1.01; days out: 3; independent_array: 15; K-fold: 2
Now running, pct_increase: 1.01; days out: 3; independent_array: 15; K-fold: 3
Now running, pct_increase: 1.01; days out: 3; independent_array: 15; K-fold: 4
Now running, pct_increase: 1.01; days out: 3; independent_array: 15; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 5; independent_array: 1; K-fold: 1
Now running, pct_increase: 1.01; days out: 5; independent_array: 1; K-fold: 2
Now running, pct_increase: 1.01; days out: 5; independent_array: 1; K-fold: 3
Now running, pct_increase: 1.01; days out: 5; independent_array: 1; K-fold: 4
Now running, pct_increase: 1.01; days out: 5; independent_array: 1; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 5; independent_array: 2; K-fold: 1
Now running, pct_increase: 1.01; days out: 5; independent_array: 2; K-fold: 2
Now running, pct_increase: 1.01; days out: 5; independent_array: 2; K-fold: 3
Now running, pct_increase: 1.01; days out: 5; independent_array: 2; K-fold: 4
Now running, pct_increase: 1.01; days out: 5; independent_array: 2; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 5; independent_array: 3; K-fold: 1
Now running, pct_increase: 1.01; days out: 5; independent_array: 3; K-fold: 2
Now running, pct_increase: 1.01; days out: 5; independent_array: 3; K-fold: 3
Now running, pct_increase: 1.01; days out: 5; independent_array: 3; K-fold: 4
Now running, pct_increase: 1.01; days out: 5; independent_array: 3; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 5; independent_array: 4; K-fold: 1
Now running, pct_increase: 1.01; days out: 5; independent_array: 4; K-fold: 2
Now running, pct_increase: 1.01; days out: 5; independent_array: 4; K-fold: 3
Now running, pct_increase: 1.01; days out: 5; independent_array: 4; K-fold: 4
Now running, pct_increase: 1.01; days out: 5; independent_array: 4; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 5; independent_array: 5; K-fold: 1
Now running, pct_increase: 1.01; days out: 5; independent_array: 5; K-fold: 2
Now running, pct_increase: 1.01; days out: 5; independent_array: 5; K-fold: 3
Now running, pct_increase: 1.01; days out: 5; independent_array: 5; K-fold: 4
Now running, pct_increase: 1.01; days out: 5; independent_array: 5; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 5; independent_array: 6; K-fold: 1
Now running, pct_increase: 1.01; days out: 5; independent_array: 6; K-fold: 2
Now running, pct_increase: 1.01; days out: 5; independent_array: 6; K-fold: 3
Now running, pct_increase: 1.01; days out: 5; independent_array: 6; K-fold: 4
Now running, pct_increase: 1.01; days out: 5; independent_array: 6; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 5; independent_array: 7; K-fold: 1
Now running, pct_increase: 1.01; days out: 5; independent_array: 7; K-fold: 2
Now running, pct_increase: 1.01; days out: 5; independent_array: 7; K-fold: 3
Now running, pct_increase: 1.01; days out: 5; independent_array: 7; K-fold: 4
Now running, pct_increase: 1.01; days out: 5; independent_array: 7; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 5; independent_array: 8; K-fold: 1
Now running, pct_increase: 1.01; days out: 5; independent_array: 8; K-fold: 2
Now running, pct_increase: 1.01; days out: 5; independent_array: 8; K-fold: 3
Now running, pct_increase: 1.01; days out: 5; independent_array: 8; K-fold: 4
Now running, pct_increase: 1.01; days out: 5; independent_array: 8; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 5; independent_array: 9; K-fold: 1
Now running, pct_increase: 1.01; days out: 5; independent_array: 9; K-fold: 2
Now running, pct_increase: 1.01; days out: 5; independent_array: 9; K-fold: 3
Now running, pct_increase: 1.01; days out: 5; independent_array: 9; K-fold: 4
Now running, pct_increase: 1.01; days out: 5; independent_array: 9; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 5; independent_array: 10; K-fold: 1
Now running, pct_increase: 1.01; days out: 5; independent_array: 10; K-fold: 2
Now running, pct_increase: 1.01; days out: 5; independent_array: 10; K-fold: 3
Now running, pct_increase: 1.01; days out: 5; independent_array: 10; K-fold: 4
Now running, pct_increase: 1.01; days out: 5; independent_array: 10; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 5; independent_array: 11; K-fold: 1
Now running, pct_increase: 1.01; days out: 5; independent_array: 11; K-fold: 2
Now running, pct_increase: 1.01; days out: 5; independent_array: 11; K-fold: 3
Now running, pct_increase: 1.01; days out: 5; independent_array: 11; K-fold: 4
Now running, pct_increase: 1.01; days out: 5; independent_array: 11; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 5; independent_array: 12; K-fold: 1
Now running, pct_increase: 1.01; days out: 5; independent_array: 12; K-fold: 2
Now running, pct_increase: 1.01; days out: 5; independent_array: 12; K-fold: 3
Now running, pct_increase: 1.01; days out: 5; independent_array: 12; K-fold: 4
Now running, pct_increase: 1.01; days out: 5; independent_array: 12; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 5; independent_array: 13; K-fold: 1
Now running, pct_increase: 1.01; days out: 5; independent_array: 13; K-fold: 2
Now running, pct_increase: 1.01; days out: 5; independent_array: 13; K-fold: 3
Now running, pct_increase: 1.01; days out: 5; independent_array: 13; K-fold: 4
Now running, pct_increase: 1.01; days out: 5; independent_array: 13; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 5; independent_array: 14; K-fold: 1
Now running, pct_increase: 1.01; days out: 5; independent_array: 14; K-fold: 2
Now running, pct_increase: 1.01; days out: 5; independent_array: 14; K-fold: 3
Now running, pct_increase: 1.01; days out: 5; independent_array: 14; K-fold: 4
Now running, pct_increase: 1.01; days out: 5; independent_array: 14; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 5; independent_array: 15; K-fold: 1
Now running, pct_increase: 1.01; days out: 5; independent_array: 15; K-fold: 2
Now running, pct_increase: 1.01; days out: 5; independent_array: 15; K-fold: 3
Now running, pct_increase: 1.01; days out: 5; independent_array: 15; K-fold: 4
Now running, pct_increase: 1.01; days out: 5; independent_array: 15; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 10; independent_array: 1; K-fold: 1
Now running, pct_increase: 1.01; days out: 10; independent_array: 1; K-fold: 2
Now running, pct_increase: 1.01; days out: 10; independent_array: 1; K-fold: 3
Now running, pct_increase: 1.01; days out: 10; independent_array: 1; K-fold: 4
Now running, pct_increase: 1.01; days out: 10; independent_array: 1; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 10; independent_array: 2; K-fold: 1
Now running, pct_increase: 1.01; days out: 10; independent_array: 2; K-fold: 2
Now running, pct_increase: 1.01; days out: 10; independent_array: 2; K-fold: 3
Now running, pct_increase: 1.01; days out: 10; independent_array: 2; K-fold: 4
Now running, pct_increase: 1.01; days out: 10; independent_array: 2; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 10; independent_array: 3; K-fold: 1
Now running, pct_increase: 1.01; days out: 10; independent_array: 3; K-fold: 2
Now running, pct_increase: 1.01; days out: 10; independent_array: 3; K-fold: 3
Now running, pct_increase: 1.01; days out: 10; independent_array: 3; K-fold: 4
Now running, pct_increase: 1.01; days out: 10; independent_array: 3; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 10; independent_array: 4; K-fold: 1
Now running, pct_increase: 1.01; days out: 10; independent_array: 4; K-fold: 2
Now running, pct_increase: 1.01; days out: 10; independent_array: 4; K-fold: 3
Now running, pct_increase: 1.01; days out: 10; independent_array: 4; K-fold: 4
Now running, pct_increase: 1.01; days out: 10; independent_array: 4; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 10; independent_array: 5; K-fold: 1
Now running, pct_increase: 1.01; days out: 10; independent_array: 5; K-fold: 2
Now running, pct_increase: 1.01; days out: 10; independent_array: 5; K-fold: 3
Now running, pct_increase: 1.01; days out: 10; independent_array: 5; K-fold: 4
Now running, pct_increase: 1.01; days out: 10; independent_array: 5; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 10; independent_array: 6; K-fold: 1
Now running, pct_increase: 1.01; days out: 10; independent_array: 6; K-fold: 2
Now running, pct_increase: 1.01; days out: 10; independent_array: 6; K-fold: 3
Now running, pct_increase: 1.01; days out: 10; independent_array: 6; K-fold: 4
Now running, pct_increase: 1.01; days out: 10; independent_array: 6; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 10; independent_array: 7; K-fold: 1
Now running, pct_increase: 1.01; days out: 10; independent_array: 7; K-fold: 2
Now running, pct_increase: 1.01; days out: 10; independent_array: 7; K-fold: 3
Now running, pct_increase: 1.01; days out: 10; independent_array: 7; K-fold: 4
Now running, pct_increase: 1.01; days out: 10; independent_array: 7; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 10; independent_array: 8; K-fold: 1
Now running, pct_increase: 1.01; days out: 10; independent_array: 8; K-fold: 2
Now running, pct_increase: 1.01; days out: 10; independent_array: 8; K-fold: 3
Now running, pct_increase: 1.01; days out: 10; independent_array: 8; K-fold: 4
Now running, pct_increase: 1.01; days out: 10; independent_array: 8; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 10; independent_array: 9; K-fold: 1
Now running, pct_increase: 1.01; days out: 10; independent_array: 9; K-fold: 2
Now running, pct_increase: 1.01; days out: 10; independent_array: 9; K-fold: 3
Now running, pct_increase: 1.01; days out: 10; independent_array: 9; K-fold: 4
Now running, pct_increase: 1.01; days out: 10; independent_array: 9; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 10; independent_array: 10; K-fold: 1
Now running, pct_increase: 1.01; days out: 10; independent_array: 10; K-fold: 2
Now running, pct_increase: 1.01; days out: 10; independent_array: 10; K-fold: 3
Now running, pct_increase: 1.01; days out: 10; independent_array: 10; K-fold: 4
Now running, pct_increase: 1.01; days out: 10; independent_array: 10; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 10; independent_array: 11; K-fold: 1
Now running, pct_increase: 1.01; days out: 10; independent_array: 11; K-fold: 2
Now running, pct_increase: 1.01; days out: 10; independent_array: 11; K-fold: 3
Now running, pct_increase: 1.01; days out: 10; independent_array: 11; K-fold: 4
Now running, pct_increase: 1.01; days out: 10; independent_array: 11; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 10; independent_array: 12; K-fold: 1
Now running, pct_increase: 1.01; days out: 10; independent_array: 12; K-fold: 2
Now running, pct_increase: 1.01; days out: 10; independent_array: 12; K-fold: 3
Now running, pct_increase: 1.01; days out: 10; independent_array: 12; K-fold: 4
Now running, pct_increase: 1.01; days out: 10; independent_array: 12; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 10; independent_array: 13; K-fold: 1
Now running, pct_increase: 1.01; days out: 10; independent_array: 13; K-fold: 2
Now running, pct_increase: 1.01; days out: 10; independent_array: 13; K-fold: 3
Now running, pct_increase: 1.01; days out: 10; independent_array: 13; K-fold: 4
Now running, pct_increase: 1.01; days out: 10; independent_array: 13; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 10; independent_array: 14; K-fold: 1
Now running, pct_increase: 1.01; days out: 10; independent_array: 14; K-fold: 2
Now running, pct_increase: 1.01; days out: 10; independent_array: 14; K-fold: 3
Now running, pct_increase: 1.01; days out: 10; independent_array: 14; K-fold: 4
Now running, pct_increase: 1.01; days out: 10; independent_array: 14; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 10; independent_array: 15; K-fold: 1
Now running, pct_increase: 1.01; days out: 10; independent_array: 15; K-fold: 2
Now running, pct_increase: 1.01; days out: 10; independent_array: 15; K-fold: 3
Now running, pct_increase: 1.01; days out: 10; independent_array: 15; K-fold: 4
Now running, pct_increase: 1.01; days out: 10; independent_array: 15; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 15; independent_array: 1; K-fold: 1
Now running, pct_increase: 1.01; days out: 15; independent_array: 1; K-fold: 2
Now running, pct_increase: 1.01; days out: 15; independent_array: 1; K-fold: 3
Now running, pct_increase: 1.01; days out: 15; independent_array: 1; K-fold: 4
Now running, pct_increase: 1.01; days out: 15; independent_array: 1; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 15; independent_array: 2; K-fold: 1
Now running, pct_increase: 1.01; days out: 15; independent_array: 2; K-fold: 2
Now running, pct_increase: 1.01; days out: 15; independent_array: 2; K-fold: 3
Now running, pct_increase: 1.01; days out: 15; independent_array: 2; K-fold: 4
Now running, pct_increase: 1.01; days out: 15; independent_array: 2; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 15; independent_array: 3; K-fold: 1
Now running, pct_increase: 1.01; days out: 15; independent_array: 3; K-fold: 2
Now running, pct_increase: 1.01; days out: 15; independent_array: 3; K-fold: 3
Now running, pct_increase: 1.01; days out: 15; independent_array: 3; K-fold: 4
Now running, pct_increase: 1.01; days out: 15; independent_array: 3; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 15; independent_array: 4; K-fold: 1
Now running, pct_increase: 1.01; days out: 15; independent_array: 4; K-fold: 2
Now running, pct_increase: 1.01; days out: 15; independent_array: 4; K-fold: 3
Now running, pct_increase: 1.01; days out: 15; independent_array: 4; K-fold: 4
Now running, pct_increase: 1.01; days out: 15; independent_array: 4; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 15; independent_array: 5; K-fold: 1
Now running, pct_increase: 1.01; days out: 15; independent_array: 5; K-fold: 2
Now running, pct_increase: 1.01; days out: 15; independent_array: 5; K-fold: 3
Now running, pct_increase: 1.01; days out: 15; independent_array: 5; K-fold: 4
Now running, pct_increase: 1.01; days out: 15; independent_array: 5; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 15; independent_array: 6; K-fold: 1
Now running, pct_increase: 1.01; days out: 15; independent_array: 6; K-fold: 2
Now running, pct_increase: 1.01; days out: 15; independent_array: 6; K-fold: 3
Now running, pct_increase: 1.01; days out: 15; independent_array: 6; K-fold: 4
Now running, pct_increase: 1.01; days out: 15; independent_array: 6; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 15; independent_array: 7; K-fold: 1
Now running, pct_increase: 1.01; days out: 15; independent_array: 7; K-fold: 2
Now running, pct_increase: 1.01; days out: 15; independent_array: 7; K-fold: 3
Now running, pct_increase: 1.01; days out: 15; independent_array: 7; K-fold: 4
Now running, pct_increase: 1.01; days out: 15; independent_array: 7; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 15; independent_array: 8; K-fold: 1
Now running, pct_increase: 1.01; days out: 15; independent_array: 8; K-fold: 2
Now running, pct_increase: 1.01; days out: 15; independent_array: 8; K-fold: 3
Now running, pct_increase: 1.01; days out: 15; independent_array: 8; K-fold: 4
Now running, pct_increase: 1.01; days out: 15; independent_array: 8; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 15; independent_array: 9; K-fold: 1
Now running, pct_increase: 1.01; days out: 15; independent_array: 9; K-fold: 2
Now running, pct_increase: 1.01; days out: 15; independent_array: 9; K-fold: 3
Now running, pct_increase: 1.01; days out: 15; independent_array: 9; K-fold: 4
Now running, pct_increase: 1.01; days out: 15; independent_array: 9; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 15; independent_array: 10; K-fold: 1
Now running, pct_increase: 1.01; days out: 15; independent_array: 10; K-fold: 2
Now running, pct_increase: 1.01; days out: 15; independent_array: 10; K-fold: 3
Now running, pct_increase: 1.01; days out: 15; independent_array: 10; K-fold: 4
Now running, pct_increase: 1.01; days out: 15; independent_array: 10; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 15; independent_array: 11; K-fold: 1
Now running, pct_increase: 1.01; days out: 15; independent_array: 11; K-fold: 2
Now running, pct_increase: 1.01; days out: 15; independent_array: 11; K-fold: 3
Now running, pct_increase: 1.01; days out: 15; independent_array: 11; K-fold: 4
Now running, pct_increase: 1.01; days out: 15; independent_array: 11; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 15; independent_array: 12; K-fold: 1
Now running, pct_increase: 1.01; days out: 15; independent_array: 12; K-fold: 2
Now running, pct_increase: 1.01; days out: 15; independent_array: 12; K-fold: 3
Now running, pct_increase: 1.01; days out: 15; independent_array: 12; K-fold: 4
Now running, pct_increase: 1.01; days out: 15; independent_array: 12; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 15; independent_array: 13; K-fold: 1
Now running, pct_increase: 1.01; days out: 15; independent_array: 13; K-fold: 2
Now running, pct_increase: 1.01; days out: 15; independent_array: 13; K-fold: 3
Now running, pct_increase: 1.01; days out: 15; independent_array: 13; K-fold: 4
Now running, pct_increase: 1.01; days out: 15; independent_array: 13; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 15; independent_array: 14; K-fold: 1
Now running, pct_increase: 1.01; days out: 15; independent_array: 14; K-fold: 2
Now running, pct_increase: 1.01; days out: 15; independent_array: 14; K-fold: 3
Now running, pct_increase: 1.01; days out: 15; independent_array: 14; K-fold: 4
Now running, pct_increase: 1.01; days out: 15; independent_array: 14; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.01; days out: 15; independent_array: 15; K-fold: 1
Now running, pct_increase: 1.01; days out: 15; independent_array: 15; K-fold: 2
Now running, pct_increase: 1.01; days out: 15; independent_array: 15; K-fold: 3
Now running, pct_increase: 1.01; days out: 15; independent_array: 15; K-fold: 4
Now running, pct_increase: 1.01; days out: 15; independent_array: 15; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 1; independent_array: 1; K-fold: 1
Now running, pct_increase: 1.02; days out: 1; independent_array: 1; K-fold: 2
Now running, pct_increase: 1.02; days out: 1; independent_array: 1; K-fold: 3
Now running, pct_increase: 1.02; days out: 1; independent_array: 1; K-fold: 4
Now running, pct_increase: 1.02; days out: 1; independent_array: 1; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 1; independent_array: 2; K-fold: 1
Now running, pct_increase: 1.02; days out: 1; independent_array: 2; K-fold: 2
Now running, pct_increase: 1.02; days out: 1; independent_array: 2; K-fold: 3
Now running, pct_increase: 1.02; days out: 1; independent_array: 2; K-fold: 4
Now running, pct_increase: 1.02; days out: 1; independent_array: 2; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 1; independent_array: 3; K-fold: 1
Now running, pct_increase: 1.02; days out: 1; independent_array: 3; K-fold: 2
Now running, pct_increase: 1.02; days out: 1; independent_array: 3; K-fold: 3
Now running, pct_increase: 1.02; days out: 1; independent_array: 3; K-fold: 4
Now running, pct_increase: 1.02; days out: 1; independent_array: 3; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 1; independent_array: 4; K-fold: 1
Now running, pct_increase: 1.02; days out: 1; independent_array: 4; K-fold: 2
Now running, pct_increase: 1.02; days out: 1; independent_array: 4; K-fold: 3
Now running, pct_increase: 1.02; days out: 1; independent_array: 4; K-fold: 4
Now running, pct_increase: 1.02; days out: 1; independent_array: 4; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 1; independent_array: 5; K-fold: 1
Now running, pct_increase: 1.02; days out: 1; independent_array: 5; K-fold: 2
Now running, pct_increase: 1.02; days out: 1; independent_array: 5; K-fold: 3
Now running, pct_increase: 1.02; days out: 1; independent_array: 5; K-fold: 4
Now running, pct_increase: 1.02; days out: 1; independent_array: 5; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 1; independent_array: 6; K-fold: 1
Now running, pct_increase: 1.02; days out: 1; independent_array: 6; K-fold: 2
Now running, pct_increase: 1.02; days out: 1; independent_array: 6; K-fold: 3
Now running, pct_increase: 1.02; days out: 1; independent_array: 6; K-fold: 4
Now running, pct_increase: 1.02; days out: 1; independent_array: 6; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 1; independent_array: 7; K-fold: 1
Now running, pct_increase: 1.02; days out: 1; independent_array: 7; K-fold: 2
Now running, pct_increase: 1.02; days out: 1; independent_array: 7; K-fold: 3
Now running, pct_increase: 1.02; days out: 1; independent_array: 7; K-fold: 4
Now running, pct_increase: 1.02; days out: 1; independent_array: 7; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 1; independent_array: 8; K-fold: 1
Now running, pct_increase: 1.02; days out: 1; independent_array: 8; K-fold: 2
Now running, pct_increase: 1.02; days out: 1; independent_array: 8; K-fold: 3
Now running, pct_increase: 1.02; days out: 1; independent_array: 8; K-fold: 4
Now running, pct_increase: 1.02; days out: 1; independent_array: 8; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 1; independent_array: 9; K-fold: 1
Now running, pct_increase: 1.02; days out: 1; independent_array: 9; K-fold: 2
Now running, pct_increase: 1.02; days out: 1; independent_array: 9; K-fold: 3
Now running, pct_increase: 1.02; days out: 1; independent_array: 9; K-fold: 4
Now running, pct_increase: 1.02; days out: 1; independent_array: 9; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 1; independent_array: 10; K-fold: 1
Now running, pct_increase: 1.02; days out: 1; independent_array: 10; K-fold: 2
Now running, pct_increase: 1.02; days out: 1; independent_array: 10; K-fold: 3
Now running, pct_increase: 1.02; days out: 1; independent_array: 10; K-fold: 4
Now running, pct_increase: 1.02; days out: 1; independent_array: 10; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 1; independent_array: 11; K-fold: 1
Now running, pct_increase: 1.02; days out: 1; independent_array: 11; K-fold: 2
Now running, pct_increase: 1.02; days out: 1; independent_array: 11; K-fold: 3
Now running, pct_increase: 1.02; days out: 1; independent_array: 11; K-fold: 4
Now running, pct_increase: 1.02; days out: 1; independent_array: 11; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 1; independent_array: 12; K-fold: 1
Now running, pct_increase: 1.02; days out: 1; independent_array: 12; K-fold: 2
Now running, pct_increase: 1.02; days out: 1; independent_array: 12; K-fold: 3
Now running, pct_increase: 1.02; days out: 1; independent_array: 12; K-fold: 4
Now running, pct_increase: 1.02; days out: 1; independent_array: 12; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 1; independent_array: 13; K-fold: 1
Now running, pct_increase: 1.02; days out: 1; independent_array: 13; K-fold: 2
Now running, pct_increase: 1.02; days out: 1; independent_array: 13; K-fold: 3
Now running, pct_increase: 1.02; days out: 1; independent_array: 13; K-fold: 4
Now running, pct_increase: 1.02; days out: 1; independent_array: 13; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 1; independent_array: 14; K-fold: 1
Now running, pct_increase: 1.02; days out: 1; independent_array: 14; K-fold: 2
Now running, pct_increase: 1.02; days out: 1; independent_array: 14; K-fold: 3
Now running, pct_increase: 1.02; days out: 1; independent_array: 14; K-fold: 4
Now running, pct_increase: 1.02; days out: 1; independent_array: 14; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 1; independent_array: 15; K-fold: 1
Now running, pct_increase: 1.02; days out: 1; independent_array: 15; K-fold: 2
Now running, pct_increase: 1.02; days out: 1; independent_array: 15; K-fold: 3
Now running, pct_increase: 1.02; days out: 1; independent_array: 15; K-fold: 4
Now running, pct_increase: 1.02; days out: 1; independent_array: 15; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 3; independent_array: 1; K-fold: 1
Now running, pct_increase: 1.02; days out: 3; independent_array: 1; K-fold: 2
Now running, pct_increase: 1.02; days out: 3; independent_array: 1; K-fold: 3
Now running, pct_increase: 1.02; days out: 3; independent_array: 1; K-fold: 4
Now running, pct_increase: 1.02; days out: 3; independent_array: 1; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 3; independent_array: 2; K-fold: 1
Now running, pct_increase: 1.02; days out: 3; independent_array: 2; K-fold: 2
Now running, pct_increase: 1.02; days out: 3; independent_array: 2; K-fold: 3
Now running, pct_increase: 1.02; days out: 3; independent_array: 2; K-fold: 4
Now running, pct_increase: 1.02; days out: 3; independent_array: 2; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 3; independent_array: 3; K-fold: 1
Now running, pct_increase: 1.02; days out: 3; independent_array: 3; K-fold: 2
Now running, pct_increase: 1.02; days out: 3; independent_array: 3; K-fold: 3
Now running, pct_increase: 1.02; days out: 3; independent_array: 3; K-fold: 4
Now running, pct_increase: 1.02; days out: 3; independent_array: 3; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 3; independent_array: 4; K-fold: 1
Now running, pct_increase: 1.02; days out: 3; independent_array: 4; K-fold: 2
Now running, pct_increase: 1.02; days out: 3; independent_array: 4; K-fold: 3
Now running, pct_increase: 1.02; days out: 3; independent_array: 4; K-fold: 4
Now running, pct_increase: 1.02; days out: 3; independent_array: 4; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 3; independent_array: 5; K-fold: 1
Now running, pct_increase: 1.02; days out: 3; independent_array: 5; K-fold: 2
Now running, pct_increase: 1.02; days out: 3; independent_array: 5; K-fold: 3
Now running, pct_increase: 1.02; days out: 3; independent_array: 5; K-fold: 4
Now running, pct_increase: 1.02; days out: 3; independent_array: 5; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 3; independent_array: 6; K-fold: 1
Now running, pct_increase: 1.02; days out: 3; independent_array: 6; K-fold: 2
Now running, pct_increase: 1.02; days out: 3; independent_array: 6; K-fold: 3
Now running, pct_increase: 1.02; days out: 3; independent_array: 6; K-fold: 4
Now running, pct_increase: 1.02; days out: 3; independent_array: 6; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 3; independent_array: 7; K-fold: 1
Now running, pct_increase: 1.02; days out: 3; independent_array: 7; K-fold: 2
Now running, pct_increase: 1.02; days out: 3; independent_array: 7; K-fold: 3
Now running, pct_increase: 1.02; days out: 3; independent_array: 7; K-fold: 4
Now running, pct_increase: 1.02; days out: 3; independent_array: 7; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 3; independent_array: 8; K-fold: 1
Now running, pct_increase: 1.02; days out: 3; independent_array: 8; K-fold: 2
Now running, pct_increase: 1.02; days out: 3; independent_array: 8; K-fold: 3
Now running, pct_increase: 1.02; days out: 3; independent_array: 8; K-fold: 4
Now running, pct_increase: 1.02; days out: 3; independent_array: 8; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 3; independent_array: 9; K-fold: 1
Now running, pct_increase: 1.02; days out: 3; independent_array: 9; K-fold: 2
Now running, pct_increase: 1.02; days out: 3; independent_array: 9; K-fold: 3
Now running, pct_increase: 1.02; days out: 3; independent_array: 9; K-fold: 4
Now running, pct_increase: 1.02; days out: 3; independent_array: 9; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 3; independent_array: 10; K-fold: 1
Now running, pct_increase: 1.02; days out: 3; independent_array: 10; K-fold: 2
Now running, pct_increase: 1.02; days out: 3; independent_array: 10; K-fold: 3
Now running, pct_increase: 1.02; days out: 3; independent_array: 10; K-fold: 4
Now running, pct_increase: 1.02; days out: 3; independent_array: 10; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 3; independent_array: 11; K-fold: 1
Now running, pct_increase: 1.02; days out: 3; independent_array: 11; K-fold: 2
Now running, pct_increase: 1.02; days out: 3; independent_array: 11; K-fold: 3
Now running, pct_increase: 1.02; days out: 3; independent_array: 11; K-fold: 4
Now running, pct_increase: 1.02; days out: 3; independent_array: 11; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 3; independent_array: 12; K-fold: 1
Now running, pct_increase: 1.02; days out: 3; independent_array: 12; K-fold: 2
Now running, pct_increase: 1.02; days out: 3; independent_array: 12; K-fold: 3
Now running, pct_increase: 1.02; days out: 3; independent_array: 12; K-fold: 4
Now running, pct_increase: 1.02; days out: 3; independent_array: 12; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 3; independent_array: 13; K-fold: 1
Now running, pct_increase: 1.02; days out: 3; independent_array: 13; K-fold: 2
Now running, pct_increase: 1.02; days out: 3; independent_array: 13; K-fold: 3
Now running, pct_increase: 1.02; days out: 3; independent_array: 13; K-fold: 4
Now running, pct_increase: 1.02; days out: 3; independent_array: 13; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 3; independent_array: 14; K-fold: 1
Now running, pct_increase: 1.02; days out: 3; independent_array: 14; K-fold: 2
Now running, pct_increase: 1.02; days out: 3; independent_array: 14; K-fold: 3
Now running, pct_increase: 1.02; days out: 3; independent_array: 14; K-fold: 4
Now running, pct_increase: 1.02; days out: 3; independent_array: 14; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 3; independent_array: 15; K-fold: 1
Now running, pct_increase: 1.02; days out: 3; independent_array: 15; K-fold: 2
Now running, pct_increase: 1.02; days out: 3; independent_array: 15; K-fold: 3
Now running, pct_increase: 1.02; days out: 3; independent_array: 15; K-fold: 4
Now running, pct_increase: 1.02; days out: 3; independent_array: 15; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 5; independent_array: 1; K-fold: 1
Now running, pct_increase: 1.02; days out: 5; independent_array: 1; K-fold: 2
Now running, pct_increase: 1.02; days out: 5; independent_array: 1; K-fold: 3
Now running, pct_increase: 1.02; days out: 5; independent_array: 1; K-fold: 4
Now running, pct_increase: 1.02; days out: 5; independent_array: 1; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 5; independent_array: 2; K-fold: 1
Now running, pct_increase: 1.02; days out: 5; independent_array: 2; K-fold: 2
Now running, pct_increase: 1.02; days out: 5; independent_array: 2; K-fold: 3
Now running, pct_increase: 1.02; days out: 5; independent_array: 2; K-fold: 4
Now running, pct_increase: 1.02; days out: 5; independent_array: 2; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 5; independent_array: 3; K-fold: 1
Now running, pct_increase: 1.02; days out: 5; independent_array: 3; K-fold: 2
Now running, pct_increase: 1.02; days out: 5; independent_array: 3; K-fold: 3
Now running, pct_increase: 1.02; days out: 5; independent_array: 3; K-fold: 4
Now running, pct_increase: 1.02; days out: 5; independent_array: 3; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 5; independent_array: 4; K-fold: 1
Now running, pct_increase: 1.02; days out: 5; independent_array: 4; K-fold: 2
Now running, pct_increase: 1.02; days out: 5; independent_array: 4; K-fold: 3
Now running, pct_increase: 1.02; days out: 5; independent_array: 4; K-fold: 4
Now running, pct_increase: 1.02; days out: 5; independent_array: 4; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 5; independent_array: 5; K-fold: 1
Now running, pct_increase: 1.02; days out: 5; independent_array: 5; K-fold: 2
Now running, pct_increase: 1.02; days out: 5; independent_array: 5; K-fold: 3
Now running, pct_increase: 1.02; days out: 5; independent_array: 5; K-fold: 4
Now running, pct_increase: 1.02; days out: 5; independent_array: 5; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 5; independent_array: 6; K-fold: 1
Now running, pct_increase: 1.02; days out: 5; independent_array: 6; K-fold: 2
Now running, pct_increase: 1.02; days out: 5; independent_array: 6; K-fold: 3
Now running, pct_increase: 1.02; days out: 5; independent_array: 6; K-fold: 4
Now running, pct_increase: 1.02; days out: 5; independent_array: 6; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 5; independent_array: 7; K-fold: 1
Now running, pct_increase: 1.02; days out: 5; independent_array: 7; K-fold: 2
Now running, pct_increase: 1.02; days out: 5; independent_array: 7; K-fold: 3
Now running, pct_increase: 1.02; days out: 5; independent_array: 7; K-fold: 4
Now running, pct_increase: 1.02; days out: 5; independent_array: 7; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 5; independent_array: 8; K-fold: 1
Now running, pct_increase: 1.02; days out: 5; independent_array: 8; K-fold: 2
Now running, pct_increase: 1.02; days out: 5; independent_array: 8; K-fold: 3
Now running, pct_increase: 1.02; days out: 5; independent_array: 8; K-fold: 4
Now running, pct_increase: 1.02; days out: 5; independent_array: 8; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 5; independent_array: 9; K-fold: 1
Now running, pct_increase: 1.02; days out: 5; independent_array: 9; K-fold: 2
Now running, pct_increase: 1.02; days out: 5; independent_array: 9; K-fold: 3
Now running, pct_increase: 1.02; days out: 5; independent_array: 9; K-fold: 4
Now running, pct_increase: 1.02; days out: 5; independent_array: 9; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 5; independent_array: 10; K-fold: 1
Now running, pct_increase: 1.02; days out: 5; independent_array: 10; K-fold: 2
Now running, pct_increase: 1.02; days out: 5; independent_array: 10; K-fold: 3
Now running, pct_increase: 1.02; days out: 5; independent_array: 10; K-fold: 4
Now running, pct_increase: 1.02; days out: 5; independent_array: 10; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 5; independent_array: 11; K-fold: 1
Now running, pct_increase: 1.02; days out: 5; independent_array: 11; K-fold: 2
Now running, pct_increase: 1.02; days out: 5; independent_array: 11; K-fold: 3
Now running, pct_increase: 1.02; days out: 5; independent_array: 11; K-fold: 4
Now running, pct_increase: 1.02; days out: 5; independent_array: 11; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 5; independent_array: 12; K-fold: 1
Now running, pct_increase: 1.02; days out: 5; independent_array: 12; K-fold: 2
Now running, pct_increase: 1.02; days out: 5; independent_array: 12; K-fold: 3
Now running, pct_increase: 1.02; days out: 5; independent_array: 12; K-fold: 4
Now running, pct_increase: 1.02; days out: 5; independent_array: 12; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 5; independent_array: 13; K-fold: 1
Now running, pct_increase: 1.02; days out: 5; independent_array: 13; K-fold: 2
Now running, pct_increase: 1.02; days out: 5; independent_array: 13; K-fold: 3
Now running, pct_increase: 1.02; days out: 5; independent_array: 13; K-fold: 4
Now running, pct_increase: 1.02; days out: 5; independent_array: 13; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 5; independent_array: 14; K-fold: 1
Now running, pct_increase: 1.02; days out: 5; independent_array: 14; K-fold: 2
Now running, pct_increase: 1.02; days out: 5; independent_array: 14; K-fold: 3
Now running, pct_increase: 1.02; days out: 5; independent_array: 14; K-fold: 4
Now running, pct_increase: 1.02; days out: 5; independent_array: 14; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 5; independent_array: 15; K-fold: 1
Now running, pct_increase: 1.02; days out: 5; independent_array: 15; K-fold: 2
Now running, pct_increase: 1.02; days out: 5; independent_array: 15; K-fold: 3
Now running, pct_increase: 1.02; days out: 5; independent_array: 15; K-fold: 4
Now running, pct_increase: 1.02; days out: 5; independent_array: 15; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 10; independent_array: 1; K-fold: 1
Now running, pct_increase: 1.02; days out: 10; independent_array: 1; K-fold: 2
Now running, pct_increase: 1.02; days out: 10; independent_array: 1; K-fold: 3
Now running, pct_increase: 1.02; days out: 10; independent_array: 1; K-fold: 4
Now running, pct_increase: 1.02; days out: 10; independent_array: 1; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 10; independent_array: 2; K-fold: 1
Now running, pct_increase: 1.02; days out: 10; independent_array: 2; K-fold: 2
Now running, pct_increase: 1.02; days out: 10; independent_array: 2; K-fold: 3
Now running, pct_increase: 1.02; days out: 10; independent_array: 2; K-fold: 4
Now running, pct_increase: 1.02; days out: 10; independent_array: 2; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 10; independent_array: 3; K-fold: 1
Now running, pct_increase: 1.02; days out: 10; independent_array: 3; K-fold: 2
Now running, pct_increase: 1.02; days out: 10; independent_array: 3; K-fold: 3
Now running, pct_increase: 1.02; days out: 10; independent_array: 3; K-fold: 4
Now running, pct_increase: 1.02; days out: 10; independent_array: 3; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 10; independent_array: 4; K-fold: 1
Now running, pct_increase: 1.02; days out: 10; independent_array: 4; K-fold: 2
Now running, pct_increase: 1.02; days out: 10; independent_array: 4; K-fold: 3
Now running, pct_increase: 1.02; days out: 10; independent_array: 4; K-fold: 4
Now running, pct_increase: 1.02; days out: 10; independent_array: 4; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 10; independent_array: 5; K-fold: 1
Now running, pct_increase: 1.02; days out: 10; independent_array: 5; K-fold: 2
Now running, pct_increase: 1.02; days out: 10; independent_array: 5; K-fold: 3
Now running, pct_increase: 1.02; days out: 10; independent_array: 5; K-fold: 4
Now running, pct_increase: 1.02; days out: 10; independent_array: 5; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 10; independent_array: 6; K-fold: 1
Now running, pct_increase: 1.02; days out: 10; independent_array: 6; K-fold: 2
Now running, pct_increase: 1.02; days out: 10; independent_array: 6; K-fold: 3
Now running, pct_increase: 1.02; days out: 10; independent_array: 6; K-fold: 4
Now running, pct_increase: 1.02; days out: 10; independent_array: 6; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 10; independent_array: 7; K-fold: 1
Now running, pct_increase: 1.02; days out: 10; independent_array: 7; K-fold: 2
Now running, pct_increase: 1.02; days out: 10; independent_array: 7; K-fold: 3
Now running, pct_increase: 1.02; days out: 10; independent_array: 7; K-fold: 4
Now running, pct_increase: 1.02; days out: 10; independent_array: 7; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 10; independent_array: 8; K-fold: 1
Now running, pct_increase: 1.02; days out: 10; independent_array: 8; K-fold: 2
Now running, pct_increase: 1.02; days out: 10; independent_array: 8; K-fold: 3
Now running, pct_increase: 1.02; days out: 10; independent_array: 8; K-fold: 4
Now running, pct_increase: 1.02; days out: 10; independent_array: 8; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 10; independent_array: 9; K-fold: 1
Now running, pct_increase: 1.02; days out: 10; independent_array: 9; K-fold: 2
Now running, pct_increase: 1.02; days out: 10; independent_array: 9; K-fold: 3
Now running, pct_increase: 1.02; days out: 10; independent_array: 9; K-fold: 4
Now running, pct_increase: 1.02; days out: 10; independent_array: 9; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 10; independent_array: 10; K-fold: 1
Now running, pct_increase: 1.02; days out: 10; independent_array: 10; K-fold: 2
Now running, pct_increase: 1.02; days out: 10; independent_array: 10; K-fold: 3
Now running, pct_increase: 1.02; days out: 10; independent_array: 10; K-fold: 4
Now running, pct_increase: 1.02; days out: 10; independent_array: 10; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 10; independent_array: 11; K-fold: 1
Now running, pct_increase: 1.02; days out: 10; independent_array: 11; K-fold: 2
Now running, pct_increase: 1.02; days out: 10; independent_array: 11; K-fold: 3
Now running, pct_increase: 1.02; days out: 10; independent_array: 11; K-fold: 4
Now running, pct_increase: 1.02; days out: 10; independent_array: 11; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 10; independent_array: 12; K-fold: 1
Now running, pct_increase: 1.02; days out: 10; independent_array: 12; K-fold: 2
Now running, pct_increase: 1.02; days out: 10; independent_array: 12; K-fold: 3
Now running, pct_increase: 1.02; days out: 10; independent_array: 12; K-fold: 4
Now running, pct_increase: 1.02; days out: 10; independent_array: 12; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 10; independent_array: 13; K-fold: 1
Now running, pct_increase: 1.02; days out: 10; independent_array: 13; K-fold: 2
Now running, pct_increase: 1.02; days out: 10; independent_array: 13; K-fold: 3
Now running, pct_increase: 1.02; days out: 10; independent_array: 13; K-fold: 4
Now running, pct_increase: 1.02; days out: 10; independent_array: 13; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 10; independent_array: 14; K-fold: 1
Now running, pct_increase: 1.02; days out: 10; independent_array: 14; K-fold: 2
Now running, pct_increase: 1.02; days out: 10; independent_array: 14; K-fold: 3
Now running, pct_increase: 1.02; days out: 10; independent_array: 14; K-fold: 4
Now running, pct_increase: 1.02; days out: 10; independent_array: 14; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 10; independent_array: 15; K-fold: 1
Now running, pct_increase: 1.02; days out: 10; independent_array: 15; K-fold: 2
Now running, pct_increase: 1.02; days out: 10; independent_array: 15; K-fold: 3
Now running, pct_increase: 1.02; days out: 10; independent_array: 15; K-fold: 4
Now running, pct_increase: 1.02; days out: 10; independent_array: 15; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 15; independent_array: 1; K-fold: 1
Now running, pct_increase: 1.02; days out: 15; independent_array: 1; K-fold: 2
Now running, pct_increase: 1.02; days out: 15; independent_array: 1; K-fold: 3
Now running, pct_increase: 1.02; days out: 15; independent_array: 1; K-fold: 4
Now running, pct_increase: 1.02; days out: 15; independent_array: 1; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 15; independent_array: 2; K-fold: 1
Now running, pct_increase: 1.02; days out: 15; independent_array: 2; K-fold: 2
Now running, pct_increase: 1.02; days out: 15; independent_array: 2; K-fold: 3
Now running, pct_increase: 1.02; days out: 15; independent_array: 2; K-fold: 4
Now running, pct_increase: 1.02; days out: 15; independent_array: 2; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 15; independent_array: 3; K-fold: 1
Now running, pct_increase: 1.02; days out: 15; independent_array: 3; K-fold: 2
Now running, pct_increase: 1.02; days out: 15; independent_array: 3; K-fold: 3
Now running, pct_increase: 1.02; days out: 15; independent_array: 3; K-fold: 4
Now running, pct_increase: 1.02; days out: 15; independent_array: 3; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 15; independent_array: 4; K-fold: 1
Now running, pct_increase: 1.02; days out: 15; independent_array: 4; K-fold: 2
Now running, pct_increase: 1.02; days out: 15; independent_array: 4; K-fold: 3
Now running, pct_increase: 1.02; days out: 15; independent_array: 4; K-fold: 4
Now running, pct_increase: 1.02; days out: 15; independent_array: 4; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 15; independent_array: 5; K-fold: 1
Now running, pct_increase: 1.02; days out: 15; independent_array: 5; K-fold: 2
Now running, pct_increase: 1.02; days out: 15; independent_array: 5; K-fold: 3
Now running, pct_increase: 1.02; days out: 15; independent_array: 5; K-fold: 4
Now running, pct_increase: 1.02; days out: 15; independent_array: 5; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 15; independent_array: 6; K-fold: 1
Now running, pct_increase: 1.02; days out: 15; independent_array: 6; K-fold: 2
Now running, pct_increase: 1.02; days out: 15; independent_array: 6; K-fold: 3
Now running, pct_increase: 1.02; days out: 15; independent_array: 6; K-fold: 4
Now running, pct_increase: 1.02; days out: 15; independent_array: 6; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 15; independent_array: 7; K-fold: 1
Now running, pct_increase: 1.02; days out: 15; independent_array: 7; K-fold: 2
Now running, pct_increase: 1.02; days out: 15; independent_array: 7; K-fold: 3
Now running, pct_increase: 1.02; days out: 15; independent_array: 7; K-fold: 4
Now running, pct_increase: 1.02; days out: 15; independent_array: 7; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 15; independent_array: 8; K-fold: 1
Now running, pct_increase: 1.02; days out: 15; independent_array: 8; K-fold: 2
Now running, pct_increase: 1.02; days out: 15; independent_array: 8; K-fold: 3
Now running, pct_increase: 1.02; days out: 15; independent_array: 8; K-fold: 4
Now running, pct_increase: 1.02; days out: 15; independent_array: 8; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 15; independent_array: 9; K-fold: 1
Now running, pct_increase: 1.02; days out: 15; independent_array: 9; K-fold: 2
Now running, pct_increase: 1.02; days out: 15; independent_array: 9; K-fold: 3
Now running, pct_increase: 1.02; days out: 15; independent_array: 9; K-fold: 4
Now running, pct_increase: 1.02; days out: 15; independent_array: 9; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 15; independent_array: 10; K-fold: 1
Now running, pct_increase: 1.02; days out: 15; independent_array: 10; K-fold: 2
Now running, pct_increase: 1.02; days out: 15; independent_array: 10; K-fold: 3
Now running, pct_increase: 1.02; days out: 15; independent_array: 10; K-fold: 4
Now running, pct_increase: 1.02; days out: 15; independent_array: 10; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 15; independent_array: 11; K-fold: 1
Now running, pct_increase: 1.02; days out: 15; independent_array: 11; K-fold: 2
Now running, pct_increase: 1.02; days out: 15; independent_array: 11; K-fold: 3
Now running, pct_increase: 1.02; days out: 15; independent_array: 11; K-fold: 4
Now running, pct_increase: 1.02; days out: 15; independent_array: 11; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 15; independent_array: 12; K-fold: 1
Now running, pct_increase: 1.02; days out: 15; independent_array: 12; K-fold: 2
Now running, pct_increase: 1.02; days out: 15; independent_array: 12; K-fold: 3
Now running, pct_increase: 1.02; days out: 15; independent_array: 12; K-fold: 4
Now running, pct_increase: 1.02; days out: 15; independent_array: 12; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 15; independent_array: 13; K-fold: 1
Now running, pct_increase: 1.02; days out: 15; independent_array: 13; K-fold: 2
Now running, pct_increase: 1.02; days out: 15; independent_array: 13; K-fold: 3
Now running, pct_increase: 1.02; days out: 15; independent_array: 13; K-fold: 4
Now running, pct_increase: 1.02; days out: 15; independent_array: 13; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 15; independent_array: 14; K-fold: 1
Now running, pct_increase: 1.02; days out: 15; independent_array: 14; K-fold: 2
Now running, pct_increase: 1.02; days out: 15; independent_array: 14; K-fold: 3
Now running, pct_increase: 1.02; days out: 15; independent_array: 14; K-fold: 4
Now running, pct_increase: 1.02; days out: 15; independent_array: 14; K-fold: 5
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.02; days out: 15; independent_array: 15; K-fold: 1
Now running, pct_increase: 1.02; days out: 15; independent_array: 15; K-fold: 2
Now running, pct_increase: 1.02; days out: 15; independent_array: 15; K-fold: 3
Now running, pct_increase: 1.02; days out: 15; independent_array: 15; K-fold: 4
Now running, pct_increase: 1.02; days out: 15; independent_array: 15; K-fold: 5
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=ff542d41-e198-4065-9476-668ce80808fc">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[134]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1">#will output multiple CSV files with training results for classification model</span>
<span class="n">accuracy_df</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="sa">f</span><span class="s1">'</span><span class="si">{</span><span class="n">ticker_symbol</span><span class="si">}</span><span class="s1">_</span><span class="si">{</span><span class="n">selected_pattern</span><span class="si">}</span><span class="s1">_classification_output.csv'</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># `index=False` avoids writing the index column</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=6a7b066a-018e-47a0-8e89-5bf60e52f5c3">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>After I have ran the above code, multiple times for each pattern (random, hammer, and inverted hammer) and received a CSV output for each, I will determine the results in the reporting section of this project.</p>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=8ca1f3a3-51e9-4725-a54a-e465bea069c6">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="LSTM-Regression-Model">LSTM Regression Model<a class="anchor-link" href="#LSTM-Regression-Model"></a></h4><p>Now, I am going to transition from classification to the regression model. As the second part of my research question is aimed at comparing the performance between these two models.</p>
<p>Below, as we did with prior to running the classification model, the first thing we need to do is get the data to run the regression model. The independent variables will be the same as when they were run through the classification model. What changes is the dependent variable. Instead of a binary variable, we are going to have a continous dependent variable that represents the stock's closing price on a future day.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=1edde645-bf9a-4094-8ddf-6c72e03a5946">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[135]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1">#Subset data frame for desired pattern</span>
<span class="n">pattern_df</span> <span class="o">=</span> <span class="n">finance_df</span><span class="p">[</span><span class="n">finance_df</span><span class="p">[</span><span class="s1">'Hammer_pattern'</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"Yes"</span><span class="p">]</span>
<span class="c1">#pattern_df = finance_df[finance_df['Random_Yes_No'] == "Yes"]</span>

<span class="c1">#How many days after the pattern is identified to use for the dependent variable</span>
<span class="n">days_out</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1">#What percent increase from the current price is considered a positive class. For example 1.01 = 1% increase; 100 * 1.01 = 101. So if original</span>
<span class="c1">#price is $100, anything greater than $101 is considered a positive class.</span>
<span class="n">pct_increase</span> <span class="o">=</span> <span class="mf">1.00</span>

<span class="c1">#Gather independent variables</span>
<span class="n">independent_list1</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">independent_list2</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">independent_list3</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">independent_list4</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">independent_list5</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">independent_list6</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">independent_list7</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">independent_list8</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">independent_list9</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">independent_list10</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">independent_list11</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">independent_list12</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">independent_list13</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">independent_list14</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">independent_list15</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1">#gather dependent variables</span>
<span class="n">dependent_list</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1">#this is for classification tasks</span>
<span class="n">dependent_list_regression</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1">#this is for regression tasks</span>
<span class="n">dependent_list_regression_log</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">dependent_list_regression_normalized</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">pattern_index</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">pattern_df</span><span class="p">[</span><span class="s2">"Row_index"</span><span class="p">])</span>
<span class="c1">#pattern_index = [60, 62]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">pattern_index</span><span class="p">:</span>
    <span class="c1">#if (i == 62):</span>
    <span class="c1">#    break</span>
    
    <span class="c1">#unable to get 30 days worth of data if index is less than 56, because previously removed first 26 observations</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="mi">56</span><span class="p">):</span>
        <span class="k">continue</span>

    <span class="c1">#get 30 days worth of data to gather data for indpendent variables</span>
    <span class="n">subset_df</span> <span class="o">=</span> <span class="n">finance_df</span><span class="p">[(</span><span class="n">finance_df</span><span class="p">[</span><span class="s2">"Row_index"</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="n">i</span> <span class="o">-</span> <span class="mi">29</span><span class="p">))</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">finance_df</span><span class="p">[</span><span class="s2">"Row_index"</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="p">(</span><span class="n">i</span><span class="p">))]</span>
    
    <span class="c1">#Get day after data to gather closing price for dependent variable</span>
    <span class="n">dependent_df</span> <span class="o">=</span> <span class="n">finance_df</span><span class="p">[</span><span class="n">finance_df</span><span class="p">[</span><span class="s2">"Row_index"</span><span class="p">]</span> <span class="o">==</span> <span class="p">(</span><span class="n">i</span><span class="p">)]</span>
    <span class="n">dependent2_df</span> <span class="o">=</span> <span class="n">finance_df</span><span class="p">[</span><span class="n">finance_df</span><span class="p">[</span><span class="s2">"Row_index"</span><span class="p">]</span> <span class="o">==</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="n">days_out</span><span class="p">)]</span>
    
    <span class="n">temp_list1</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">temp_list2</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">temp_list3</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">temp_list4</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">temp_list5</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">temp_list6</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">temp_list7</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">temp_list8</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">temp_list9</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">temp_list10</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">temp_list11</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">temp_list12</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">temp_list13</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">temp_list14</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">temp_list15</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1">#append temp_list to independent_list</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">dependent2_df</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span> <span class="c1">#dependent2_df may have length of zero as it is a future date, data may not be available</span>
    

        <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">subset_df</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
                
                <span class="n">test_array1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Low'</span><span class="p">]])</span>
                <span class="n">test_array2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Log_Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_Low'</span><span class="p">]])</span>
                <span class="n">test_array3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Low'</span><span class="p">]])</span>
        
                <span class="n">test_array4</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'RSI'</span><span class="p">]])</span>
                <span class="n">test_array5</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Log_Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'RSI'</span><span class="p">]])</span>
                <span class="n">test_array6</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'RSI'</span><span class="p">]])</span>
        
                <span class="n">test_array7</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MFI'</span><span class="p">]])</span>
                <span class="n">test_array8</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Log_Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MFI'</span><span class="p">]])</span>
                <span class="n">test_array9</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MFI'</span><span class="p">]])</span>
        
                <span class="n">test_array10</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MACD'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Signal_Line'</span><span class="p">]])</span>
                <span class="n">test_array11</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Log_Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MACD'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Signal_Line'</span><span class="p">]])</span>
                <span class="n">test_array12</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MACD'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Signal_Line'</span><span class="p">]])</span>
        
                <span class="n">test_array13</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'RSI'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MFI'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MACD'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Signal_Line'</span><span class="p">]])</span>
                <span class="n">test_array14</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Log_Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'RSI'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MFI'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MACD'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Signal_Line'</span><span class="p">]])</span>
                <span class="n">test_array15</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'RSI'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MFI'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MACD'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Signal_Line'</span><span class="p">]])</span>
        
                
                <span class="n">temp_list1</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array1</span><span class="p">)</span>
                <span class="n">temp_list2</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array2</span><span class="p">)</span>
                <span class="n">temp_list3</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array3</span><span class="p">)</span>
                <span class="n">temp_list4</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array4</span><span class="p">)</span>
                <span class="n">temp_list5</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array5</span><span class="p">)</span>
                <span class="n">temp_list6</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array6</span><span class="p">)</span>
                <span class="n">temp_list7</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array7</span><span class="p">)</span>
                <span class="n">temp_list8</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array8</span><span class="p">)</span>
                <span class="n">temp_list9</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array9</span><span class="p">)</span>
                <span class="n">temp_list10</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array10</span><span class="p">)</span>
                <span class="n">temp_list11</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array11</span><span class="p">)</span>
                <span class="n">temp_list12</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array12</span><span class="p">)</span>
                <span class="n">temp_list13</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array13</span><span class="p">)</span>
                <span class="n">temp_list14</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array14</span><span class="p">)</span>
                <span class="n">temp_list15</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array15</span><span class="p">)</span>
               
        <span class="n">independent_list1</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list1</span><span class="p">)</span>
        <span class="n">independent_list2</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list2</span><span class="p">)</span>
        <span class="n">independent_list3</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list3</span><span class="p">)</span>
        <span class="n">independent_list4</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list4</span><span class="p">)</span>
        <span class="n">independent_list5</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list5</span><span class="p">)</span>
        <span class="n">independent_list6</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list6</span><span class="p">)</span>
        <span class="n">independent_list7</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list7</span><span class="p">)</span>
        <span class="n">independent_list8</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list8</span><span class="p">)</span>
        <span class="n">independent_list9</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list9</span><span class="p">)</span>
        <span class="n">independent_list10</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list10</span><span class="p">)</span>
        <span class="n">independent_list11</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list11</span><span class="p">)</span>
        <span class="n">independent_list12</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list12</span><span class="p">)</span>
        <span class="n">independent_list13</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list13</span><span class="p">)</span>
        <span class="n">independent_list14</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list14</span><span class="p">)</span>
        <span class="n">independent_list15</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list15</span><span class="p">)</span>
    
        <span class="n">dependent_list_regression</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dependent2_df</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">dependent_list_regression_log</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dependent2_df</span><span class="p">[</span><span class="s1">'Log_Close'</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">dependent_list_regression_normalized</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dependent2_df</span><span class="p">[</span><span class="s1">'Normalized_Close'</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

        <span class="k">if</span> <span class="p">(</span><span class="n">dependent2_df</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">dependent_df</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">pct_increase</span><span class="p">):</span>
            <span class="n">dependent_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">dependent_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>


<span class="n">independent_array1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list1</span><span class="p">)</span>
<span class="n">independent_array2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list2</span><span class="p">)</span>
<span class="n">independent_array3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list3</span><span class="p">)</span>
<span class="n">independent_array4</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list4</span><span class="p">)</span>
<span class="n">independent_array5</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list5</span><span class="p">)</span>
<span class="n">independent_array6</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list6</span><span class="p">)</span>
<span class="n">independent_array7</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list7</span><span class="p">)</span>
<span class="n">independent_array8</span><span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list8</span><span class="p">)</span>
<span class="n">independent_array9</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list9</span><span class="p">)</span>
<span class="n">independent_array10</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list10</span><span class="p">)</span>
<span class="n">independent_array11</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list11</span><span class="p">)</span>
<span class="n">independent_array12</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list12</span><span class="p">)</span>
<span class="n">independent_array13</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list13</span><span class="p">)</span>
<span class="n">independent_array14</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list14</span><span class="p">)</span>
<span class="n">independent_array15</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list15</span><span class="p">)</span>
<span class="n">dependent_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dependent_list</span><span class="p">)</span> <span class="c1">#used to see how many positive and negative classes</span>
<span class="n">dependent_array_regression</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dependent_list_regression</span><span class="p">)</span>
<span class="n">dependent_array_regression_log</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dependent_list_regression_log</span><span class="p">)</span>
<span class="n">dependent_array_regression_normalized</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dependent_list_regression_normalized</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=b7f2568c-cbb2-4f44-b1cb-3a292d5d1aef">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>This code below defines and trains a regression model using an LSTM (Long Short-Term Memory) network, a type of recurrent neural network (RNN) designed for sequence prediction tasks. The regression model aims to predict a continuous target variable based on a series of input features. First, the code imports the necessary libraries, including Keras for building and training the model, scikit-learn for data preprocessing and splitting, and other utility libraries like numpy and pandas. It defines a function create_lstm_regression() to create the LSTM-based regression model. The model consists of two LSTM layers, each followed by a Dropout layer to reduce the risk of overfitting. The first LSTM layer has 128 units and returns sequences of data, allowing the next LSTM layer to process the sequence. The second LSTM layer has 64 units and does not return sequences. The final layer is a Dense layer with a single neuron and a linear activation function, which outputs a continuous value suitable for regression tasks. The model is compiled using the Adam optimizer and mean squared error (MSE) loss function, which is commonly used for regression.</p>
<p>In the second part of the code, the input data (X) and target labels (y) are prepared for training. The independent variables (X) are chosen from the earlier dataset (e.g., independent_array1), which contains time-series features such as stock prices and technical indicators, while the dependent variable (y) contains the regression targets (e.g., future stock price movements). The data is then split into training and testing sets using train_test_split(), where 80% is used for training and 20% for testing. The input_shape for the LSTM model is determined by the number of time steps (30) and the number of features per time-step (e.g., 8 for independent_array_15). The LSTM model is then created by calling create_lstm_regression() with the input_shape, and the model is trained using the fit() method. The training process runs for 10 epochs, with a batch size of 32, and the model's performance is evaluated on the test set using the validation data. The verbose = 1 option displays the training progress during each epoch.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=2728228f-3cdc-4d6e-85e5-19d0f8658d10">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[136]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1">#code for regression</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tensorflow.keras.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tensorflow.keras.layers</span><span class="w"> </span><span class="kn">import</span> <span class="n">LSTM</span><span class="p">,</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Dropout</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">KFold</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">StratifiedKFold</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tensorflow.keras.optimizers</span><span class="w"> </span><span class="kn">import</span> <span class="n">RMSprop</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>

<span class="c1"># Define the regression model</span>
<span class="k">def</span><span class="w"> </span><span class="nf">create_lstm_regression</span><span class="p">(</span><span class="n">input_shape</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
    
    <span class="c1"># LSTM layers</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'tanh'</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">))</span>  <span class="c1"># Dropout to reduce overfitting</span>
    
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'tanh'</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>  <span class="c1"># Final LSTM layer</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">))</span>
    
    <span class="c1"># Dense output layer for regression</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'linear'</span><span class="p">))</span>  <span class="c1"># Predicting a continuous value</span>
    
    <span class="c1"># Compile the model</span>
    <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">'adam'</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s1">'mse'</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'mae'</span><span class="p">])</span>  <span class="c1"># MSE for regression tasks</span>
    
    <span class="k">return</span> <span class="n">model</span>


<span class="c1"># Independent variables (features)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">independent_array1</span>  <span class="c1"># Shape: (890, 30, )</span>

<span class="c1"># Dependent variable (target); make sure to match the dependent array with the correct independent variables-- if X == independent_array3 then y should be set to dependent_array_regression_normalized </span>
<span class="n">y</span> <span class="o">=</span> <span class="n">dependent_array_regression</span>  <span class="c1"># Shape: (890,)</span>

<span class="c1">#split data into training and testing sets (80% training, 20% testing)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>

<span class="c1">#define the input shape based on your data; for example independent array_1 has input_shape of (30, 4); independent array_15's shape is (30,8)</span>
<span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>  <span class="c1"># 30 time-steps and 8 features per time-step for independent_array_15</span>

<span class="c1">#create the LSTM model</span>
<span class="n">regression_model</span> <span class="o">=</span> <span class="n">create_lstm_regression</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>

<span class="c1">#train the classification model and store history. verbose = 0 -&gt; hides the training output</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">regression_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">),</span> <span class="n">verbose</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">2/2</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 758ms/step - loss: 65740.9219 - mae: 203.2404 - val_loss: 52114.9180 - val_mae: 187.3432
Epoch 2/10
<span class="ansi-bold">2/2</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 159ms/step - loss: 59008.4766 - mae: 191.7096 - val_loss: 51820.3789 - val_mae: 186.5379
Epoch 3/10
<span class="ansi-bold">2/2</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 161ms/step - loss: 59213.7461 - mae: 189.7480 - val_loss: 51560.4414 - val_mae: 185.8472
Epoch 4/10
<span class="ansi-bold">2/2</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 163ms/step - loss: 61183.7383 - mae: 196.5097 - val_loss: 51328.4375 - val_mae: 185.2185
Epoch 5/10
<span class="ansi-bold">2/2</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 167ms/step - loss: 64968.6641 - mae: 201.8303 - val_loss: 51115.5391 - val_mae: 184.6420
Epoch 6/10
<span class="ansi-bold">2/2</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 181ms/step - loss: 65988.2812 - mae: 202.6831 - val_loss: 50930.0391 - val_mae: 184.1364
Epoch 7/10
<span class="ansi-bold">2/2</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 175ms/step - loss: 65451.7305 - mae: 202.7320 - val_loss: 50758.2305 - val_mae: 183.6656
Epoch 8/10
<span class="ansi-bold">2/2</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 105ms/step - loss: 59947.5195 - mae: 189.7587 - val_loss: 50587.1172 - val_mae: 183.1947
Epoch 9/10
<span class="ansi-bold">2/2</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 184ms/step - loss: 73399.4453 - mae: 212.8492 - val_loss: 50407.9062 - val_mae: 182.7036
Epoch 10/10
<span class="ansi-bold">2/2</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 128ms/step - loss: 67499.9766 - mae: 201.8873 - val_loss: 50222.9688 - val_mae: 182.2100
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=7be14b42-f15d-4b0e-974e-4ce96c7e0660">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[137]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1">#get the predictions of the model when applied on the test set</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">regression_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1">#Get the actual closing prices from the test set; we know the closing price on the final day of the candlestick pattern, is always the 30th day, and 2nd item in the array</span>
<span class="n">last_closing_price</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">[:,</span> <span class="mi">29</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

<span class="n">comparison</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_pred</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">last_closing_price</span> <span class="o">*</span> <span class="n">pct_increase</span><span class="p">)</span> <span class="c1">#are the predictions greater than the actual closing prices</span>
<span class="n">comparison_2</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_pred</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span> <span class="o">&lt;=</span> <span class="n">last_closing_price</span> <span class="o">*</span> <span class="n">pct_increase</span><span class="p">)</span> <span class="c1">#are the predictions less than or equal to the actual closing prices</span>

<span class="n">comparison_3</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_test</span> <span class="o">&gt;</span> <span class="n">last_closing_price</span> <span class="o">*</span> <span class="n">pct_increase</span><span class="p">)</span> <span class="c1">#are the actual closing prices from the test set greater than the actual closing prices</span>
<span class="n">comparison_4</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_test</span> <span class="o">&lt;=</span> <span class="n">last_closing_price</span> <span class="o">*</span> <span class="n">pct_increase</span><span class="p">)</span> <span class="c1">#are the actual closing prices from the test set less than or equal to the actual closing prices</span>

<span class="c1"># Case 1: When both predicted and actual values are greater than the closing price</span>
<span class="n">correct_greater</span> <span class="o">=</span> <span class="n">comparison</span> <span class="o">&amp;</span> <span class="n">comparison_3</span>
<span class="c1"># Case 2: When both predicted and actual values are less than or equal to the closing price</span>
<span class="n">correct_lesser_or_equal</span> <span class="o">=</span> <span class="n">comparison_2</span> <span class="o">&amp;</span> <span class="n">comparison_4</span>

<span class="c1">#total correct predictions</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Total correct predictions: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">correct_greater</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">correct_lesser_or_equal</span><span class="p">)</span><span class="si">}</span><span class="s1">; out of </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">y_test</span><span class="p">)</span><span class="si">}</span><span class="s1"> observations in test set'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Total observations: </span><span class="si">{</span><span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">dependent_array</span><span class="p">,</span><span class="w"> </span><span class="n">return_counts</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">1</span><span class="p">])</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Number of observations each class from dependent variable: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">dependent_array</span><span class="p">,</span><span class="w"> </span><span class="n">return_counts</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 332ms/step
Total correct predictions: 7; out of 14 observations in test set
Total observations: 69
Number of observations each class from dependent variable: (array([0, 1]), array([37, 32]))
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=9ea1d152-81ed-43a0-9931-a35974ac4b24">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>A regression model is designed to predict continuous values, not for classification tasks. However, I needed a way to compare the performance of both my regression and classification models. To do this, I evaluated the predicted values from my regression model using the following approach:</p>
<ol>
<li>If the predicted closing price is greater than the closing price of the last candle in the identified candlestick pattern, and the actual closing price (from the test set) is also greater than the last candles closing price, this is considered a correct prediction.</li>
<li>If the predicted closing price is less than or equal to the closing price of the last candle in the identified candlestick pattern, and the actual closing price (from the test set) is also less than or equal to the last candles closing price, this is also considered a correct prediction.</li>
<li>I then count the number of correct predictions from both cases (steps 1 and 2), and divide this by the total number of observations in the test set to calculate the accuracy.</li>
</ol>
<p>In this example, we have a total of 69 observations, with 37 of those observations belonging to the negative class, meaning the future closing price was less than the closing price from the last candle in the identified candlestick pattern. Dividing 37 by 69 gives us a percentage of 53.6%. This percentage represents the proportion of negative class observations in the test set.</p>
<p>When comparing this to the performance of our regression model, we see that it correctly classified 7 out of 14 total in the test set observations, or 50.0%. This means that if I had predicted every observation to belong to the negative class, my model would have performed better than the regression model (since 53.6% of the observations were negative class).</p>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=38f6c227-0b39-4fe4-8f64-4c1818df7349">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="Exploring-Model-Performance-with-Parameter-Variations-and-Stratified-K-Fold-Cross-Validation---LSTM-Regression-Model">Exploring Model Performance with Parameter Variations and Stratified K-Fold Cross Validation - LSTM Regression Model<a class="anchor-link" href="#Exploring-Model-Performance-with-Parameter-Variations-and-Stratified-K-Fold-Cross-Validation---LSTM-Regression-Model"></a></h4><p>I am going to repeat the analysis that I did with various parameter combinations and stratified K-fold cross validation from when I performed the training of the LSTM classification model.</p>
<p>This time, as mentioned before, I will have a different dependent variable as instead of having binary dependent variables, I will have a continuous dependent variable.</p>
<p>For the parameter combinations, I will evaluate my model using three patterns: random days, the hammer pattern, and the inverted hammer pattern. Ive chosen not to evaluate the other patterns due to insufficient observations in the dataset. When I refer to evaluating my model on random days, I mean that I previously created a column in the dataset with randomly assigned "Yes" values. These "Yes" values are distributed randomly, and my goal is to compare the model's performance using these random patterns versus patterns that are specifically identified as candlestick patterns. This will help me understand if the model behaves differently when dealing with randomly assigned patterns versus known candlestick patterns.</p>
<p>Other parameters I will test will be how many days out after the pattern is identified to use for the dependent variable. For example, if this is set to the value of "1", the closing price for the day directly after the candlestick pattern will be the dependent variable. If set to "10" for example, the closing price 10 days after the last candle in the candlestick pattern will be the dependent variable.</p>
<p>My last parameter combination will be what percent increase from the original price is considered a positive class. For example 1.01 = 1% increase; 100 * 1.01 = 101. So if the original price is 100 dollars, anything greater than 101 dollars is considered a positive class.</p>
<p>I will also use statified K-fold Cross validation as I previously did when training the classification model.</p>
<p>The result of running the code below will output a CSV file which shows the accuracy scores of each parameter and variable combination. I am going to have to run this code below multiple times, each time for the selected candle stick pattern (the code below can only run one selected pattern and one stock ticker at a time). Again, I will only run the stock ticker "SPY" in an effort to save resources.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=1813d5c3-8bc7-4588-8834-78ef3994d324">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[140]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tensorflow.keras.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tensorflow.keras.layers</span><span class="w"> </span><span class="kn">import</span> <span class="n">LSTM</span><span class="p">,</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Dropout</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">KFold</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">StratifiedKFold</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>


<span class="c1">### User inputs ###</span>
<span class="n">selected_pattern</span> <span class="o">=</span> <span class="s2">"InvertedHammer"</span>   <span class="c1">#choices: 'Random', 'Hammer', 'InvertedHammer'</span>

<span class="c1">#How many days after the pattern is identified to use for the dependent variable</span>
<span class="n">days_out</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">15</span><span class="p">]</span>

<span class="c1">#What percent increase from the current price is considered a positive class. For example 1.01 = 1% increase; 100 * 1.01 = 101. So if original</span>
<span class="c1">#price is $100, anything greater than $101 is considered a positive class.</span>
<span class="n">pct_increase</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.00</span><span class="p">,</span> <span class="mf">1.01</span><span class="p">,</span> <span class="mf">1.02</span><span class="p">]</span>

<span class="c1">######</span>


<span class="c1"># Define the regression model</span>
<span class="k">def</span><span class="w"> </span><span class="nf">create_lstm_regression</span><span class="p">(</span><span class="n">input_shape</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
    
    <span class="c1"># LSTM layers</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'tanh'</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">))</span>  <span class="c1"># Dropout to reduce overfitting</span>
    
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'tanh'</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>  <span class="c1"># Final LSTM layer</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">))</span>
    
    <span class="c1"># Dense output layer for regression</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'linear'</span><span class="p">))</span>  <span class="c1"># Predicting a continuous value</span>
    
    <span class="c1"># Compile the model</span>
    <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">'adam'</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s1">'mse'</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'mae'</span><span class="p">])</span>  <span class="c1"># MSE for regression tasks</span>
    
    <span class="k">return</span> <span class="n">model</span>

    

<span class="c1">#Subset data frame for desired pattern</span>
<span class="k">if</span> <span class="p">(</span><span class="n">selected_pattern</span> <span class="o">==</span> <span class="s2">"Random"</span><span class="p">):</span>
    <span class="n">pattern_df</span> <span class="o">=</span> <span class="n">finance_df</span><span class="p">[</span><span class="n">finance_df</span><span class="p">[</span><span class="s1">'Random_Yes_No'</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"Yes"</span><span class="p">]</span>
<span class="k">elif</span> <span class="p">(</span><span class="n">selected_pattern</span> <span class="o">==</span> <span class="s2">"Hammer"</span><span class="p">):</span>
    <span class="n">pattern_df</span> <span class="o">=</span> <span class="n">finance_df</span><span class="p">[</span><span class="n">finance_df</span><span class="p">[</span><span class="s1">'Hammer_pattern'</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"Yes"</span><span class="p">]</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">pattern_df</span> <span class="o">=</span> <span class="n">finance_df</span><span class="p">[</span><span class="n">finance_df</span><span class="p">[</span><span class="s1">'InvertedHammer_pattern'</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"Yes"</span><span class="p">]</span>


<span class="c1">#initialize an empty DataFrame with column names</span>
<span class="n">accuracy_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">'ticker'</span><span class="p">,</span> <span class="s1">'pattern'</span><span class="p">,</span> <span class="s1">'independent_array'</span><span class="p">,</span> <span class="s1">'best_accuracy'</span><span class="p">,</span> <span class="s1">'avg_accuracy'</span><span class="p">,</span> <span class="s1">'days_out'</span><span class="p">,</span> <span class="s1">'Total_observations'</span><span class="p">,</span> 
                                   <span class="s1">'Negative_observations'</span><span class="p">,</span> <span class="s1">'Positive_observations'</span><span class="p">,</span> <span class="s1">'Percent_increase_parameter'</span><span class="p">])</span>


<span class="k">for</span> <span class="n">percent</span> <span class="ow">in</span> <span class="n">pct_increase</span><span class="p">:</span>

    <span class="k">for</span> <span class="n">day</span> <span class="ow">in</span> <span class="n">days_out</span><span class="p">:</span>
        <span class="c1">#Gather independent variables</span>
        <span class="n">independent_list1</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">independent_list2</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">independent_list3</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">independent_list4</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">independent_list5</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">independent_list6</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">independent_list7</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">independent_list8</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">independent_list9</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">independent_list10</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">independent_list11</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">independent_list12</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">independent_list13</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">independent_list14</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">independent_list15</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">dependent_list_regression</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">dependent_list_regression_log</span> <span class="o">=</span> <span class="p">[]</span> 
        <span class="n">dependent_list_regression_normalized</span> <span class="o">=</span> <span class="p">[]</span> 
        
        <span class="c1">#gather dependent variables</span>
        <span class="n">dependent_list</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1">#these are the row indexes that have the identified patterns; loop through</span>
        <span class="n">pattern_index</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">pattern_df</span><span class="p">[</span><span class="s2">"Row_index"</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">pattern_index</span><span class="p">:</span>
            <span class="c1">#if (i == 62):</span>
            <span class="c1">#    break</span>
            
            <span class="c1">#unable to get 30 days worth of data if index is less than 56, because previously removed first 26 observations</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="mi">56</span><span class="p">):</span>
                <span class="k">continue</span>
        
            <span class="c1">#get 30 days worth of data to gather data for indpendent variables</span>
            <span class="n">subset_df</span> <span class="o">=</span> <span class="n">finance_df</span><span class="p">[(</span><span class="n">finance_df</span><span class="p">[</span><span class="s2">"Row_index"</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="n">i</span> <span class="o">-</span> <span class="mi">29</span><span class="p">))</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">finance_df</span><span class="p">[</span><span class="s2">"Row_index"</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="p">(</span><span class="n">i</span><span class="p">))]</span>
            <span class="c1">#subset_df = finance_df[(finance_df["Row_index"] &gt;= (i - 13)) &amp; (finance_df["Row_index"] &lt;= (i))]</span>
            
            <span class="c1">#Get day after data to gather closing price for dependent variable</span>
            <span class="n">dependent_df</span> <span class="o">=</span> <span class="n">finance_df</span><span class="p">[</span><span class="n">finance_df</span><span class="p">[</span><span class="s2">"Row_index"</span><span class="p">]</span> <span class="o">==</span> <span class="p">(</span><span class="n">i</span><span class="p">)]</span>
            <span class="n">dependent2_df</span> <span class="o">=</span> <span class="n">finance_df</span><span class="p">[</span><span class="n">finance_df</span><span class="p">[</span><span class="s2">"Row_index"</span><span class="p">]</span> <span class="o">==</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="n">day</span><span class="p">)]</span>
            
            <span class="n">temp_list1</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">temp_list2</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">temp_list3</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">temp_list4</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">temp_list5</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">temp_list6</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">temp_list7</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">temp_list8</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">temp_list9</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">temp_list10</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">temp_list11</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">temp_list12</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">temp_list13</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">temp_list14</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">temp_list15</span> <span class="o">=</span> <span class="p">[]</span>
        
            <span class="c1">#append temp_list to independent_list</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">dependent2_df</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span> <span class="c1">#dependent2_df may have length of zero as it is a future date, data may not be available</span>
            
        
                <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">subset_df</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
                        
                        <span class="n">test_array1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Low'</span><span class="p">]])</span>
                        <span class="n">test_array2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Log_Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_Low'</span><span class="p">]])</span>
                        <span class="n">test_array3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Low'</span><span class="p">]])</span>
                
                        <span class="n">test_array4</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'RSI'</span><span class="p">]])</span>
                        <span class="n">test_array5</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Log_Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'RSI'</span><span class="p">]])</span>
                        <span class="n">test_array6</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'RSI'</span><span class="p">]])</span>
                
                        <span class="n">test_array7</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MFI'</span><span class="p">]])</span>
                        <span class="n">test_array8</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Log_Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MFI'</span><span class="p">]])</span>
                        <span class="n">test_array9</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MFI'</span><span class="p">]])</span>
                
                        <span class="n">test_array10</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MACD'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Signal_Line'</span><span class="p">]])</span>
                        <span class="n">test_array11</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Log_Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MACD'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Signal_Line'</span><span class="p">]])</span>
                        <span class="n">test_array12</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MACD'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Signal_Line'</span><span class="p">]])</span>
                
                        <span class="n">test_array13</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'RSI'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MFI'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MACD'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Signal_Line'</span><span class="p">]])</span>
                        <span class="n">test_array14</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Log_Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'RSI'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MFI'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MACD'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Signal_Line'</span><span class="p">]])</span>
                        <span class="n">test_array15</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'RSI'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MFI'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MACD'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Signal_Line'</span><span class="p">]])</span>
                
                
                        <span class="n">temp_list1</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array1</span><span class="p">)</span>
                        <span class="n">temp_list2</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array2</span><span class="p">)</span>
                        <span class="n">temp_list3</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array3</span><span class="p">)</span>
                        <span class="n">temp_list4</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array4</span><span class="p">)</span>
                        <span class="n">temp_list5</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array5</span><span class="p">)</span>
                        <span class="n">temp_list6</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array6</span><span class="p">)</span>
                        <span class="n">temp_list7</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array7</span><span class="p">)</span>
                        <span class="n">temp_list8</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array8</span><span class="p">)</span>
                        <span class="n">temp_list9</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array9</span><span class="p">)</span>
                        <span class="n">temp_list10</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array10</span><span class="p">)</span>
                        <span class="n">temp_list11</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array11</span><span class="p">)</span>
                        <span class="n">temp_list12</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array12</span><span class="p">)</span>
                        <span class="n">temp_list13</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array13</span><span class="p">)</span>
                        <span class="n">temp_list14</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array14</span><span class="p">)</span>
                        <span class="n">temp_list15</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array15</span><span class="p">)</span>
                        
                <span class="n">independent_list1</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list1</span><span class="p">)</span>
                <span class="n">independent_list2</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list2</span><span class="p">)</span>
                <span class="n">independent_list3</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list3</span><span class="p">)</span>
                <span class="n">independent_list4</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list4</span><span class="p">)</span>
                <span class="n">independent_list5</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list5</span><span class="p">)</span>
                <span class="n">independent_list6</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list6</span><span class="p">)</span>
                <span class="n">independent_list7</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list7</span><span class="p">)</span>
                <span class="n">independent_list8</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list8</span><span class="p">)</span>
                <span class="n">independent_list9</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list9</span><span class="p">)</span>
                <span class="n">independent_list10</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list10</span><span class="p">)</span>
                <span class="n">independent_list11</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list11</span><span class="p">)</span>
                <span class="n">independent_list12</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list12</span><span class="p">)</span>
                <span class="n">independent_list13</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list13</span><span class="p">)</span>
                <span class="n">independent_list14</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list14</span><span class="p">)</span>
                <span class="n">independent_list15</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list15</span><span class="p">)</span>
            
                <span class="n">dependent_list_regression</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dependent2_df</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
                <span class="n">dependent_list_regression_log</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dependent2_df</span><span class="p">[</span><span class="s1">'Log_Close'</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
                <span class="n">dependent_list_regression_normalized</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dependent2_df</span><span class="p">[</span><span class="s1">'Normalized_Close'</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        
                <span class="k">if</span> <span class="p">(</span><span class="n">dependent2_df</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">dependent_df</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">percent</span><span class="p">):</span>
                    <span class="n">dependent_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">dependent_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        
        <span class="n">independent_array1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list1</span><span class="p">)</span>
        <span class="n">independent_array2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list2</span><span class="p">)</span>
        <span class="n">independent_array3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list3</span><span class="p">)</span>
        <span class="n">independent_array4</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list4</span><span class="p">)</span>
        <span class="n">independent_array5</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list5</span><span class="p">)</span>
        <span class="n">independent_array6</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list6</span><span class="p">)</span>
        <span class="n">independent_array7</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list7</span><span class="p">)</span>
        <span class="n">independent_array8</span><span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list8</span><span class="p">)</span>
        <span class="n">independent_array9</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list9</span><span class="p">)</span>
        <span class="n">independent_array10</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list10</span><span class="p">)</span>
        <span class="n">independent_array11</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list11</span><span class="p">)</span>
        <span class="n">independent_array12</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list12</span><span class="p">)</span>
        <span class="n">independent_array13</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list13</span><span class="p">)</span>
        <span class="n">independent_array14</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list14</span><span class="p">)</span>
        <span class="n">independent_array15</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list15</span><span class="p">)</span>
        <span class="n">dependent_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dependent_list</span><span class="p">)</span>
        <span class="n">dependent_array_regression</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dependent_list_regression</span><span class="p">)</span>
        <span class="n">dependent_array_regression_log</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dependent_list_regression_log</span><span class="p">)</span>
        <span class="n">dependent_array_regression_normalized</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dependent_list_regression_normalized</span><span class="p">)</span>
    
    
        <span class="n">independent_array</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">best_accuracy</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">avg_accuracy</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">counter_independentarray</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">):</span>
            <span class="c1">#if i != 12: #testing what seems is the most well performing model</span>
                <span class="c1">#continue</span>
            
            <span class="c1"># Select which independent_array to use</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">independent_array1</span>  <span class="c1"># Shape: (890, 30, 4)</span>
                <span class="n">independent_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"independent_array1"</span><span class="p">)</span>
                <span class="n">y</span> <span class="o">=</span> <span class="n">dependent_array_regression</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">independent_array2</span>  <span class="c1"># Shape: (890, 30, 4)</span>
                <span class="n">independent_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"independent_array2"</span><span class="p">)</span>
                <span class="n">y</span> <span class="o">=</span> <span class="n">dependent_array_regression_log</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">independent_array3</span>  <span class="c1"># Shape: (890, 30, 4)</span>
                <span class="n">independent_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"independent_array3"</span><span class="p">)</span>
                <span class="n">y</span> <span class="o">=</span> <span class="n">dependent_array_regression_normalized</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">independent_array4</span>  <span class="c1"># Shape: (890, 30, 5)</span>
                <span class="n">independent_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"independent_array4"</span><span class="p">)</span>
                <span class="n">y</span> <span class="o">=</span> <span class="n">dependent_array_regression</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">5</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">independent_array5</span>  <span class="c1"># Shape: (890, 30, 5)</span>
                <span class="n">independent_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"independent_array5"</span><span class="p">)</span>
                <span class="n">y</span> <span class="o">=</span> <span class="n">dependent_array_regression_log</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">6</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">independent_array6</span>  <span class="c1"># Shape: (890, 30, 5)</span>
                <span class="n">independent_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"independent_array6"</span><span class="p">)</span>
                <span class="n">y</span> <span class="o">=</span> <span class="n">dependent_array_regression_normalized</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">7</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">independent_array7</span>  <span class="c1"># Shape: (890, 30, 5)</span>
                <span class="n">independent_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"independent_array7"</span><span class="p">)</span>
                <span class="n">y</span> <span class="o">=</span> <span class="n">dependent_array_regression</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">8</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">independent_array8</span>  <span class="c1"># Shape: (890, 30, 5)</span>
                <span class="n">independent_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"independent_array8"</span><span class="p">)</span>
                <span class="n">y</span> <span class="o">=</span> <span class="n">dependent_array_regression_log</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">9</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">independent_array9</span>  <span class="c1"># Shape: (890, 30, 5)</span>
                <span class="n">independent_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"independent_array9"</span><span class="p">)</span>
                <span class="n">y</span> <span class="o">=</span> <span class="n">dependent_array_regression_normalized</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">10</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">independent_array10</span>  <span class="c1"># Shape: (890, 30, 6)</span>
                <span class="n">independent_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"independent_array10"</span><span class="p">)</span>
                <span class="n">y</span> <span class="o">=</span> <span class="n">dependent_array_regression</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">11</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">independent_array11</span>  <span class="c1"># Shape: (890, 30, 6)</span>
                <span class="n">independent_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"independent_array11"</span><span class="p">)</span>
                <span class="n">y</span> <span class="o">=</span> <span class="n">dependent_array_regression_log</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">12</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">independent_array12</span>  <span class="c1"># Shape: (890, 30, 6)</span>
                <span class="n">independent_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"independent_array12"</span><span class="p">)</span>
                <span class="n">y</span> <span class="o">=</span> <span class="n">dependent_array_regression_normalized</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">13</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">independent_array13</span>  <span class="c1"># Shape: (890, 30, 8)</span>
                <span class="n">independent_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"independent_array13"</span><span class="p">)</span>
                <span class="n">y</span> <span class="o">=</span> <span class="n">dependent_array_regression</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">14</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">independent_array14</span>  <span class="c1"># Shape: (890, 30, 8)</span>
                <span class="n">independent_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"independent_array14"</span><span class="p">)</span>
                <span class="n">y</span> <span class="o">=</span> <span class="n">dependent_array_regression_log</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">15</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">independent_array15</span>  <span class="c1"># Shape: (890, 30, 8)</span>
                <span class="n">independent_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"independent_array15"</span><span class="p">)</span>
                <span class="n">y</span> <span class="o">=</span> <span class="n">dependent_array_regression_normalized</span>
        
            <span class="n">counter_independentarray</span> <span class="o">=</span> <span class="n">counter_independentarray</span> <span class="o">+</span> <span class="mi">1</span>
            
            <span class="c1"># Define the input shape based on the number of features</span>
            <span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>  <span class="c1"># 30 time-steps and `X.shape[2]` features per time-step</span>
            
            <span class="c1"># Create the LSTM model</span>
            <span class="n">regression_model</span> <span class="o">=</span> <span class="n">create_lstm_regression</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
            
            <span class="c1"># Initialize k-fold cross-validation</span>
            <span class="n">kf</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>  <span class="c1">#regular 5-fold cross-validation w/out stratification, for regression tasks because no class imbalance</span>
            <span class="c1">#kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=6)  # 5-fold cross-validation with stratification</span>
            
            <span class="c1">#create list to gather accuracy scores after training each fold</span>
            <span class="n">fold_accuracies</span> <span class="o">=</span> <span class="p">[]</span>
            
            <span class="c1">#K-fold Cross-Validation</span>
            <span class="n">counter_kfold</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="c1">#for train_index, val_index in kf.split(X, y): #used for stratified k-fold</span>
            <span class="k">for</span> <span class="n">train_index</span><span class="p">,</span> <span class="n">val_index</span> <span class="ow">in</span> <span class="n">kf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">):</span> <span class="c1">#used for regular k-fold</span>
                
                <span class="n">counter_kfold</span> <span class="o">=</span> <span class="n">counter_kfold</span> <span class="o">+</span> <span class="mi">1</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Now running, pct_increase: </span><span class="si">{</span><span class="n">percent</span><span class="si">}</span><span class="s2">; days out: </span><span class="si">{</span><span class="n">day</span><span class="si">}</span><span class="s2">; independent_array: </span><span class="si">{</span><span class="n">counter_independentarray</span><span class="si">}</span><span class="s2">; K-fold: </span><span class="si">{</span><span class="n">counter_kfold</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
                
                <span class="n">X_train</span><span class="p">,</span> <span class="n">X_val</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">val_index</span><span class="p">]</span>
                <span class="n">y_train</span><span class="p">,</span> <span class="n">y_val</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">val_index</span><span class="p">]</span>
                
                <span class="c1"># Train the classification model and store the history; verbose = 0 to hide epoch running info in cell output</span>
                <span class="n">history</span> <span class="o">=</span> <span class="n">regression_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">),</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

                <span class="c1">#get the predictions of the model when applied on the test set</span>
                <span class="n">y_pred</span> <span class="o">=</span> <span class="n">regression_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>
                
                <span class="c1">#Get the actual closing prices from the test set; we know the closing price on the final day of the candlestick pattern, </span>
                <span class="c1">#is always the 30th day, and 2nd item in the array</span>
                <span class="n">last_closing_price</span> <span class="o">=</span> <span class="n">X_val</span><span class="p">[:,</span> <span class="mi">29</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
                
                <span class="n">comparison</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_pred</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">last_closing_price</span> <span class="o">*</span> <span class="n">percent</span><span class="p">)</span> <span class="c1">#are the predictions greater than the actual closing prices</span>
                <span class="n">comparison_2</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_pred</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span> <span class="o">&lt;=</span> <span class="n">last_closing_price</span> <span class="o">*</span> <span class="n">percent</span><span class="p">)</span> <span class="c1">#are the predictions less than or equal to the actual closing prices</span>
                
                <span class="n">comparison_3</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_val</span> <span class="o">&gt;</span> <span class="n">last_closing_price</span> <span class="o">*</span> <span class="n">percent</span><span class="p">)</span> <span class="c1">#are the actual closing prices from the test set greater than the actual closing prices</span>
                <span class="n">comparison_4</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_val</span> <span class="o">&lt;=</span> <span class="n">last_closing_price</span> <span class="o">*</span> <span class="n">percent</span><span class="p">)</span> <span class="c1">#are the actual closing prices from the test set less than or equal to the actual closing prices</span>
                
                <span class="c1"># Case 1: When both predicted and actual values are greater than the closing price</span>
                <span class="n">correct_greater</span> <span class="o">=</span> <span class="n">comparison</span> <span class="o">&amp;</span> <span class="n">comparison_3</span>
                <span class="c1"># Case 2: When both predicted and actual values are less than or equal to the closing price</span>
                <span class="n">correct_lesser_or_equal</span> <span class="o">=</span> <span class="n">comparison_2</span> <span class="o">&amp;</span> <span class="n">comparison_4</span>
                
                <span class="c1">#total correct predictions</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Total correct predictions: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">correct_greater</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">correct_lesser_or_equal</span><span class="p">)</span><span class="si">}</span><span class="s1">; out of </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">y_val</span><span class="p">)</span><span class="si">}</span><span class="s1"> observations in test set'</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Total observations (combined train and test sets): </span><span class="si">{</span><span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">dependent_array</span><span class="p">,</span><span class="w"> </span><span class="n">return_counts</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">1</span><span class="p">])</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Number of observations each class from dependent variable (combined): </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">dependent_array</span><span class="p">,</span><span class="w"> </span><span class="n">return_counts</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>

                <span class="c1">#the accuracy score at the 10th epoch for each fold is appended to this list; this is different than gathering the accuracy</span>
                <span class="c1">#scores for the classification model, because the classification model gets all accuracy scores from each epoch</span>
                <span class="n">fold_accuracies</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">correct_greater</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">correct_lesser_or_equal</span><span class="p">))</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_val</span><span class="p">))</span>
        
        
            
            <span class="c1"># Calculate the best and average validation accuracy across all folds</span>
            <span class="n">best_val_accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">fold_accuracies</span><span class="p">)</span> <span class="c1">#from the 10th epoch for each of the five folds, the accuracy is collected, and the max accuracy is stored</span>
            <span class="n">avg_val_accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">fold_accuracies</span><span class="p">)</span> <span class="c1">#from the 10th epoch for each of the five folds, the accuracy is collected, and the mean accuracy is stored</span>
            <span class="n">best_accuracy</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">best_val_accuracy</span><span class="p">)</span>
            <span class="n">avg_accuracy</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">avg_val_accuracy</span><span class="p">)</span>
        
        
        <span class="c1"># Example of new data to add</span>
        <span class="n">df_new</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
            <span class="s1">'ticker'</span><span class="p">:</span> <span class="n">ticker_symbol</span><span class="p">,</span>
            <span class="s1">'pattern'</span><span class="p">:</span> <span class="n">selected_pattern</span><span class="p">,</span>
            <span class="s1">'independent_array'</span><span class="p">:</span> <span class="n">independent_array</span><span class="p">,</span>
            <span class="s1">'best_accuracy'</span><span class="p">:</span> <span class="n">best_accuracy</span><span class="p">,</span>
            <span class="s1">'avg_accuracy'</span><span class="p">:</span> <span class="n">avg_accuracy</span><span class="p">,</span>
            <span class="s1">'days_out'</span><span class="p">:</span> <span class="n">day</span><span class="p">,</span>
            <span class="s1">'Total_observations'</span><span class="p">:</span> <span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">dependent_array</span><span class="p">,</span> <span class="n">return_counts</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">1</span><span class="p">]),</span>
            <span class="s1">'Negative_observations'</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">dependent_array</span><span class="p">,</span> <span class="n">return_counts</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
            <span class="s1">'Positive_observations'</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">dependent_array</span><span class="p">,</span> <span class="n">return_counts</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span>
            <span class="s1">'Percent_increase_parameter'</span><span class="p">:</span> <span class="n">percent</span>
        <span class="p">})</span>
    
        <span class="c1"># Concatenate the new data to the empty DataFrame</span>
        <span class="n">accuracy_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">accuracy_df</span><span class="p">,</span> <span class="n">df_new</span><span class="p">],</span> <span class="n">ignore_index</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 1; independent_array: 1; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 245ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 1; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 47ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 1; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 346ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 1; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 29ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 1; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 39ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 2; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 234ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 2; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 62ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 2; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 269ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 2; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 46ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 2; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 46ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 3; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 259ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 3; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 100ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 3; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 287ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 3; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 42ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 3; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 40ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 4; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 297ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 4; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 45ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 4; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 254ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 4; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 45ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 4; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 60ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 5; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 260ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 5; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 40ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 5; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 325ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 5; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 56ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 5; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 49ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 6; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 437ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 6; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 79ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 6; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 449ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 6; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 86ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 6; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 68ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 7; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 482ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 7; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 51ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 7; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 534ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 7; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 81ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 7; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 57ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 8; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 459ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 8; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 105ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 8; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 461ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 8; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 39ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 8; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 86ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 9; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 476ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 9; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 85ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 9; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 448ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 9; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 78ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 9; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 113ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 10; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 501ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 10; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 71ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 10; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 477ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 10; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 50ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 10; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 56ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 11; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 294ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 11; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 28ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 11; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 284ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 11; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 77ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 11; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 42ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 12; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 247ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 12; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 52ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 12; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 264ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 12; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 40ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 12; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 45ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 13; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 243ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 13; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 37ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 13; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 257ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 13; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 46ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 13; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 49ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 14; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 247ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 14; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 43ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 14; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 262ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 14; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 58ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 14; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 41ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 15; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 321ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 15; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 58ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 15; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 361ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 15; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 46ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.0; days out: 1; independent_array: 15; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 50ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\AppData\Local\Temp\ipykernel_27512\4038827188.py:358: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  accuracy_df = pd.concat([accuracy_df, df_new], ignore_index=True)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 3; independent_array: 1; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 312ms/step
Total correct predictions: 3; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 1; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 42ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 1; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 251ms/step
Total correct predictions: 1; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 1; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 41ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 1; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 63ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 2; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 272ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 2; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 46ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 2; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 458ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 2; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 71ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 2; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 56ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 3; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 421ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 3; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 71ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 3; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 455ms/step
Total correct predictions: 1; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 3; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 99ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 3; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 94ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 4; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 476ms/step
Total correct predictions: 3; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 4; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 74ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 4; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 528ms/step
Total correct predictions: 1; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 4; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 90ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 4; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 58ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 5; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 461ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 5; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 80ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 5; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 502ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 5; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 112ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 5; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 51ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 6; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 481ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 6; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 81ms/step
Total correct predictions: 8; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 6; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 485ms/step
Total correct predictions: 2; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 6; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 53ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 6; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 103ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 7; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 309ms/step
Total correct predictions: 3; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 7; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 51ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 7; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 281ms/step
Total correct predictions: 1; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 7; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 43ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 7; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 57ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 8; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 275ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 8; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 46ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 8; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 262ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 8; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 40ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 8; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 39ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 9; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 246ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 9; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 40ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 9; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 241ms/step
Total correct predictions: 9; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 9; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 44ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 9; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 51ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 10; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 275ms/step
Total correct predictions: 3; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 10; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 44ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 10; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 266ms/step
Total correct predictions: 1; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 10; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 45ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 10; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 43ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 11; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 276ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 11; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 54ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 11; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 321ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 11; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 54ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 11; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 64ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 12; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 312ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 12; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 57ms/step
Total correct predictions: 8; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 12; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 272ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 12; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 45ms/step
Total correct predictions: 2; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 12; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 57ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 13; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 268ms/step
Total correct predictions: 3; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 13; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 67ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 13; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 241ms/step
Total correct predictions: 1; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 13; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 46ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 13; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 61ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 14; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 459ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 14; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 78ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 14; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 455ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 14; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 111ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 14; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 109ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 15; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 543ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 15; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 76ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 15; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 491ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 15; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 75ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 3; independent_array: 15; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 131ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 1; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 456ms/step
Total correct predictions: 2; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 1; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 107ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 1; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 469ms/step
Total correct predictions: 0; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 1; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 47ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 1; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 51ms/step
Total correct predictions: 2; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 2; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 350ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 2; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 43ms/step
Total correct predictions: 3; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 2; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 273ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 2; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 54ms/step
Total correct predictions: 2; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 2; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 49ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 3; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 270ms/step
Total correct predictions: 9; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 3; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 50ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 3; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 329ms/step
Total correct predictions: 0; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 3; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 70ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 3; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 53ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 4; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 292ms/step
Total correct predictions: 2; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 4; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 53ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 4; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 328ms/step
Total correct predictions: 0; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 4; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 46ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 4; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 66ms/step
Total correct predictions: 2; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 5; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 285ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 5; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 84ms/step
Total correct predictions: 3; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 5; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 295ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 5; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 66ms/step
Total correct predictions: 2; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 5; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 46ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 6; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 324ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 6; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 49ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 6; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 261ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 6; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 63ms/step
Total correct predictions: 2; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 6; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 54ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 7; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 274ms/step
Total correct predictions: 2; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 7; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 87ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 7; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 502ms/step
Total correct predictions: 0; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 7; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 101ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 7; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 128ms/step
Total correct predictions: 2; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 8; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 524ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 8; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 103ms/step
Total correct predictions: 3; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 8; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 478ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 8; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 98ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 8; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 85ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 9; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 498ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 9; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 68ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 9; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 463ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 9; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 93ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 9; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 72ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 10; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 489ms/step
Total correct predictions: 2; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 10; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 81ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 10; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 305ms/step
Total correct predictions: 0; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 10; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 51ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 10; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 71ms/step
Total correct predictions: 2; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 11; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 298ms/step
Total correct predictions: 3; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 11; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 54ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 11; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 298ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 11; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 55ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 11; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 58ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 12; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 303ms/step
Total correct predictions: 3; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 12; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 59ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 12; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 358ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 12; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 58ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 12; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 75ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 13; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 261ms/step
Total correct predictions: 2; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 13; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 55ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 13; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 292ms/step
Total correct predictions: 0; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 13; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 44ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 13; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 57ms/step
Total correct predictions: 2; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 14; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 403ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 14; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 52ms/step
Total correct predictions: 3; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 14; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 279ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 14; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 47ms/step
Total correct predictions: 2; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 14; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 52ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 15; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 259ms/step
Total correct predictions: 3; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 15; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 73ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 15; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 305ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 15; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 51ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 5; independent_array: 15; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 51ms/step
Total correct predictions: 2; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 1; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 446ms/step
Total correct predictions: 3; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 1; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 117ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 1; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 445ms/step
Total correct predictions: 1; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 1; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 118ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 1; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 106ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 2; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 535ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 2; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 130ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 2; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 540ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 2; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 109ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 2; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 45ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 3; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 508ms/step
Total correct predictions: 8; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 3; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 88ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 3; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 530ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 3; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 50ms/step
Total correct predictions: 2; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 3; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 91ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 4; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 490ms/step
Total correct predictions: 3; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 4; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 98ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 4; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 265ms/step
Total correct predictions: 1; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 4; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 54ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 4; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 46ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 5; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 264ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 5; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 54ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 5; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 306ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 5; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 45ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 5; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 51ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 6; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 255ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 6; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 37ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 6; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 306ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 6; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 56ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 6; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 79ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 7; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 258ms/step
Total correct predictions: 3; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 7; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 48ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 7; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 285ms/step
Total correct predictions: 1; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 7; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 44ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 7; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 62ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 8; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 296ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 8; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 47ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 8; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 294ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 8; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 67ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 8; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 48ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 9; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 281ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 9; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 70ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 9; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 284ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 9; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 44ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 9; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 43ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 10; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 480ms/step
Total correct predictions: 3; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 10; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 82ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 10; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 409ms/step
Total correct predictions: 1; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 10; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 82ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 10; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 92ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 11; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 525ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 11; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 86ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 11; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 467ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 11; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 79ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 11; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 68ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 12; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 456ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 12; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 49ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 12; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 486ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 12; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 102ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 12; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 74ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 13; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 458ms/step
Total correct predictions: 3; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 13; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 88ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 13; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 460ms/step
Total correct predictions: 1; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 13; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 54ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 13; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 48ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 14; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 252ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 14; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 52ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 14; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 261ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 14; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 43ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 14; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 63ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 15; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 260ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 15; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 42ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 15; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 271ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 15; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 51ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 10; independent_array: 15; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 48ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([18, 34]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 1; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 277ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 1; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 53ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 1; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 382ms/step
Total correct predictions: 2; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 1; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 63ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 1; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 57ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 2; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 307ms/step
Total correct predictions: 8; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 2; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 59ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 2; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 270ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 2; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 70ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 2; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 71ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 3; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 292ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 3; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 51ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 3; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 259ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 3; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 47ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 3; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 65ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 4; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 277ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 4; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 43ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 4; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 365ms/step
Total correct predictions: 2; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 4; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 92ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 4; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 74ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 5; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 418ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 5; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 93ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 5; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 467ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 5; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 97ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 5; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 101ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 6; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 433ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 6; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 112ms/step
Total correct predictions: 3; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 6; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 522ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 6; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 119ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 6; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 73ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 7; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 550ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 7; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 80ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 7; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 538ms/step
Total correct predictions: 2; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 7; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 80ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 7; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 48ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 8; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 347ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 8; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 100ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 8; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 554ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 8; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 43ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 8; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 55ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 9; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 287ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 9; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 48ms/step
Total correct predictions: 9; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 9; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 294ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 9; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 42ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 9; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 40ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 10; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 264ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 10; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 52ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 10; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 292ms/step
Total correct predictions: 2; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 10; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 37ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 10; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 51ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 11; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 281ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 11; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 50ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 11; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 274ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 11; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 40ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 11; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 42ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 12; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 295ms/step
Total correct predictions: 2; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 12; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 43ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 12; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 296ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 12; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 60ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 12; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 61ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 13; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 309ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 13; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 47ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 13; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 265ms/step
Total correct predictions: 2; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 13; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 39ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 13; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 44ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 14; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 298ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 14; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 44ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 14; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 267ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 14; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 62ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 14; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 43ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 15; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 455ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 15; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 96ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 15; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 464ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 15; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 105ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.0; days out: 15; independent_array: 15; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 59ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([19, 33]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 1; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 431ms/step
Total correct predictions: 8; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 1; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 101ms/step
Total correct predictions: 10; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 1; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 480ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 1; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 61ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 1; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 64ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 2; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 504ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 2; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 101ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 2; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 465ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 2; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 45ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 2; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 49ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 3; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 404ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 3; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 101ms/step
Total correct predictions: 9; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 3; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 556ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 3; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 86ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 3; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 111ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 4; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 297ms/step
Total correct predictions: 8; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 4; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 41ms/step
Total correct predictions: 10; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 4; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 309ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 4; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 72ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 4; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 47ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 5; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 267ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 5; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 62ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 5; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 278ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 5; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 61ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 5; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 40ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 6; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 270ms/step
Total correct predictions: 1; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 6; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 38ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 6; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 297ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 6; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 53ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 6; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 50ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 7; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 235ms/step
Total correct predictions: 8; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 7; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 52ms/step
Total correct predictions: 10; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 7; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 274ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 7; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 45ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 7; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 62ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 8; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 270ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 8; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 77ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 8; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 339ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 8; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 47ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 8; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 47ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 9; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 276ms/step
Total correct predictions: 3; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 9; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 43ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 9; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 285ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 9; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 56ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 9; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 56ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 10; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 266ms/step
Total correct predictions: 8; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 10; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 40ms/step
Total correct predictions: 10; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 10; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 255ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 10; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 50ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 10; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 99ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 11; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 550ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 11; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 105ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 11; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 456ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 11; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 90ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 11; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 83ms/step
Total correct predictions: 2; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 12; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 441ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 12; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 80ms/step
Total correct predictions: 8; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 12; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 593ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 12; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 121ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 12; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 80ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 13; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 442ms/step
Total correct predictions: 8; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 13; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 71ms/step
Total correct predictions: 10; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 13; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 530ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 13; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 95ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 13; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 46ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 14; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 510ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 14; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 98ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 14; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 482ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 14; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 85ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 14; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 75ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 15; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 249ms/step
Total correct predictions: 1; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 15; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 46ms/step
Total correct predictions: 3; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 15; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 274ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 15; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 55ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 1; independent_array: 15; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 55ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([40, 12]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 1; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 248ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 1; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 43ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 1; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 259ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 1; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 48ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 1; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 62ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 2; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 288ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 2; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 49ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 2; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 282ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 2; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 44ms/step
Total correct predictions: 2; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 2; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 38ms/step
Total correct predictions: 9; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 3; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 237ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 3; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 40ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 3; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 267ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 3; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 64ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 3; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 44ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 4; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 356ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 4; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 50ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 4; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 340ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 4; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 45ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 4; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 43ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 5; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 279ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 5; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 44ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 5; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 254ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 5; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 43ms/step
Total correct predictions: 1; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 5; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 52ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 6; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 259ms/step
Total correct predictions: 3; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 6; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 37ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 6; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 280ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 6; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 91ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 6; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 99ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 7; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 430ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 7; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 91ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 7; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 471ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 7; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 98ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 7; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 90ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 8; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 409ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 8; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 99ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 8; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 494ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 8; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 73ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 8; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 90ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 9; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 474ms/step
Total correct predictions: 3; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 9; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 126ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 9; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 416ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 9; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 85ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 9; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 100ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 10; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 503ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 10; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 100ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 10; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 482ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 10; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 113ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 10; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 95ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 11; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 481ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 11; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 49ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 11; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 262ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 11; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 37ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 11; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 42ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 12; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 255ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 12; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 41ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 12; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 273ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 12; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 56ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 12; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 39ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 13; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 238ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 13; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 35ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 13; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 266ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 13; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 36ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 13; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 54ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 14; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 284ms/step
Total correct predictions: 3; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 14; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 37ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 14; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 297ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 14; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 46ms/step
Total correct predictions: 2; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 14; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 51ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 15; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 265ms/step
Total correct predictions: 3; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 15; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 59ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 15; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 241ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 15; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 44ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 3; independent_array: 15; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 71ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([28, 24]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 1; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 274ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 1; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 54ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 1; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 293ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 1; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 56ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 1; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 48ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 2; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 270ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 2; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 55ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 2; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 253ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 2; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 60ms/step
Total correct predictions: 1; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 2; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 91ms/step
Total correct predictions: 9; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 3; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 506ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 3; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 73ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 3; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 459ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 3; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 93ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 3; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 95ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 4; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 452ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 4; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 99ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 4; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 516ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 4; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 98ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 4; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 68ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 5; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 459ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 5; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 96ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 5; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 585ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 5; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 98ms/step
Total correct predictions: 2; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 5; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 56ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 6; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 503ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 6; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 47ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 6; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 477ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 6; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 100ms/step
Total correct predictions: 2; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 6; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 107ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 7; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 476ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 7; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 73ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 7; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 282ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 7; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 84ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 7; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 39ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 8; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 272ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 8; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 52ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 8; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 284ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 8; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 59ms/step
Total correct predictions: 1; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 8; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 46ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 9; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 266ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 9; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 52ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 9; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 310ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 9; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 47ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 9; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 38ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 10; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 250ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 10; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 40ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 10; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 283ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 10; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 39ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 10; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 48ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 11; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 276ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 11; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 57ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 11; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 280ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 11; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 74ms/step
Total correct predictions: 1; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 11; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 54ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 12; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 292ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 12; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 60ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 12; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 259ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 12; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 59ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 12; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 59ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 13; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 281ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 13; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 41ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 13; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 260ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 13; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 52ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 13; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 45ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 14; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 524ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 14; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 91ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 14; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 460ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 14; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 74ms/step
Total correct predictions: 1; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 14; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 93ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 15; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 461ms/step
Total correct predictions: 3; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 15; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 84ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 15; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 506ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 15; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 112ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 5; independent_array: 15; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 86ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 1; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 508ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 1; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 97ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 1; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 487ms/step
Total correct predictions: 2; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 1; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 103ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 1; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 110ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 2; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 480ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 2; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 66ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 2; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 488ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 2; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 49ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 2; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 62ms/step
Total correct predictions: 9; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 3; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 428ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 3; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 107ms/step
Total correct predictions: 8; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 3; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 480ms/step
Total correct predictions: 2; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 3; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 72ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 3; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 65ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 4; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 262ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 4; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 57ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 4; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 249ms/step
Total correct predictions: 2; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 4; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 46ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 4; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 52ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 5; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 239ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 5; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 50ms/step
Total correct predictions: 8; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 5; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 250ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 5; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 55ms/step
Total correct predictions: 2; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 5; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 75ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 6; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 249ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 6; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 56ms/step
Total correct predictions: 3; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 6; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 284ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 6; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 53ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 6; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 45ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 7; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 298ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 7; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 41ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 7; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 290ms/step
Total correct predictions: 2; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 7; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 49ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 7; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 47ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 8; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 316ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 8; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 45ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 8; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 271ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 8; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 74ms/step
Total correct predictions: 2; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 8; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 41ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 9; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 309ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 9; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 46ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 9; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 294ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 9; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 54ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 9; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 54ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 10; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 270ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 10; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 94ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 10; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 531ms/step
Total correct predictions: 2; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 10; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 80ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 10; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 97ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 11; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 523ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 11; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 100ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 11; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 466ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 11; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 90ms/step
Total correct predictions: 2; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 11; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 68ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 12; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 484ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 12; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 70ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 12; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 482ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 12; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 91ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 12; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 53ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 13; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 399ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 13; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 114ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 13; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 594ms/step
Total correct predictions: 2; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 13; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 60ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 13; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 49ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 14; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 480ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 14; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 118ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 14; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 469ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 14; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 83ms/step
Total correct predictions: 2; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 14; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 102ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 15; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 274ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 15; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 41ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 15; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 248ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 15; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 44ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 10; independent_array: 15; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 51ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([21, 31]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 1; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 325ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 1; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 45ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 1; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 253ms/step
Total correct predictions: 2; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 1; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 55ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 1; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 40ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 2; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 239ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 2; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 51ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 2; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 363ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 2; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 45ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 2; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 30ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 3; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 233ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 3; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 45ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 3; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 271ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 3; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 46ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 3; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 61ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 4; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 333ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 4; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 38ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 4; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 277ms/step
Total correct predictions: 2; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 4; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 40ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 4; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 52ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 5; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 267ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 5; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 39ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 5; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 288ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 5; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 31ms/step
Total correct predictions: 2; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 5; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 47ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 6; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 226ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 6; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 49ms/step
Total correct predictions: 3; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 6; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 240ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 6; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 119ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 6; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 97ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 7; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 337ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 7; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 118ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 7; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 469ms/step
Total correct predictions: 2; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 7; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 101ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 7; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 128ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 8; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 443ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 8; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 107ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 8; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 488ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 8; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 107ms/step
Total correct predictions: 2; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 8; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 46ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 9; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 387ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 9; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 43ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 9; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 517ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 9; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 90ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 9; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 106ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 10; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 503ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 10; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 76ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 10; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 551ms/step
Total correct predictions: 2; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 10; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 69ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 10; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 55ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 11; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 262ms/step
Total correct predictions: 8; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 11; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 51ms/step
Total correct predictions: 3; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 11; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 346ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 11; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 40ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 11; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 32ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 12; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 312ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 12; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 58ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 12; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 297ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 12; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 47ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 12; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 67ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 13; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 303ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 13; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 57ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 13; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 246ms/step
Total correct predictions: 2; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 13; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 60ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 13; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 43ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 14; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 281ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 14; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 42ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 14; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 256ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 14; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 40ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 14; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 45ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 15; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 312ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 15; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 44ms/step
Total correct predictions: 8; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 15; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 292ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 15; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 58ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.01; days out: 15; independent_array: 15; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 59ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([22, 30]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 1; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 276ms/step
Total correct predictions: 11; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 1; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 60ms/step
Total correct predictions: 10; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 1; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 251ms/step
Total correct predictions: 9; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 1; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 42ms/step
Total correct predictions: 9; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 1; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 41ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 2; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 452ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 2; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 131ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 2; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 546ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 2; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 86ms/step
Total correct predictions: 2; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 2; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 123ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 3; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 453ms/step
Total correct predictions: 2; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 3; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 127ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 3; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 501ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 3; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 116ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 3; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 126ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 4; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 553ms/step
Total correct predictions: 11; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 4; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 136ms/step
Total correct predictions: 10; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 4; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 570ms/step
Total correct predictions: 9; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 4; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 49ms/step
Total correct predictions: 9; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 4; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 59ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 5; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 295ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 5; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 52ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 5; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 286ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 5; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 83ms/step
Total correct predictions: 2; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 5; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 59ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 6; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 252ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 6; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 63ms/step
Total correct predictions: 1; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 6; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 298ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 6; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 55ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 6; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 59ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 7; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 324ms/step
Total correct predictions: 11; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 7; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 46ms/step
Total correct predictions: 10; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 7; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 260ms/step
Total correct predictions: 9; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 7; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 74ms/step
Total correct predictions: 9; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 7; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 57ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 8; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 325ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 8; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 52ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 8; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 322ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 8; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 103ms/step
Total correct predictions: 1; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 8; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 52ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 9; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 381ms/step
Total correct predictions: 3; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 9; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 49ms/step
Total correct predictions: 2; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 9; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 313ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 9; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 66ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 9; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 121ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 10; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 539ms/step
Total correct predictions: 11; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 10; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 158ms/step
Total correct predictions: 10; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 10; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 545ms/step
Total correct predictions: 9; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 10; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 120ms/step
Total correct predictions: 9; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 10; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 149ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 11; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 547ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 11; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 77ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 11; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 666ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 11; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 103ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 11; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 125ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 12; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 531ms/step
Total correct predictions: 3; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 12; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 83ms/step
Total correct predictions: 1; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 12; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 550ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 12; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 113ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 12; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 48ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 13; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 304ms/step
Total correct predictions: 11; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 13; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 64ms/step
Total correct predictions: 10; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 13; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 319ms/step
Total correct predictions: 9; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 13; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 50ms/step
Total correct predictions: 9; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 13; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 45ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 14; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 285ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 14; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 61ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 14; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 269ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 14; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 50ms/step
Total correct predictions: 2; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 14; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 77ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 15; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 283ms/step
Total correct predictions: 8; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 15; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 60ms/step
Total correct predictions: 0; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 15; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 270ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 15; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 70ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 1; independent_array: 15; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 66ms/step
Total correct predictions: 2; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([47,  5]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 1; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 343ms/step
Total correct predictions: 8; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 1; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 82ms/step
Total correct predictions: 9; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 1; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 341ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 1; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 61ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 1; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 63ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 2; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 377ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 2; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 48ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 2; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 360ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 2; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 59ms/step
Total correct predictions: 2; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 2; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 62ms/step
Total correct predictions: 9; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 3; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 555ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 3; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 148ms/step
Total correct predictions: 8; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 3; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 689ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 3; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 100ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 3; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 100ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 4; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 594ms/step
Total correct predictions: 8; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 4; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 139ms/step
Total correct predictions: 9; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 4; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 638ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 4; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 88ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 4; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 93ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 5; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 549ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 5; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 71ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 5; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 657ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 5; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 107ms/step
Total correct predictions: 2; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 5; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 74ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 6; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 375ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 6; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 64ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 6; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 276ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 6; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 55ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 6; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 52ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 7; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 327ms/step
Total correct predictions: 8; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 7; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 49ms/step
Total correct predictions: 9; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 7; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 343ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 7; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 68ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 7; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 44ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 8; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 302ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 8; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 67ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 8; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 259ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 8; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 49ms/step
Total correct predictions: 2; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 8; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 41ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 9; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 316ms/step
Total correct predictions: 2; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 9; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 57ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 9; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 251ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 9; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 52ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 9; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 76ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 10; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 304ms/step
Total correct predictions: 8; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 10; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 51ms/step
Total correct predictions: 9; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 10; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 308ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 10; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 73ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 10; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 52ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 11; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 266ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 11; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 70ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 11; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 298ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 11; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 50ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 11; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 128ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 12; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 481ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 12; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 105ms/step
Total correct predictions: 8; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 12; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 435ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 12; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 76ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 12; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 138ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 13; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 421ms/step
Total correct predictions: 8; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 13; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 117ms/step
Total correct predictions: 9; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 13; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 549ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 13; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 68ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 13; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 100ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 14; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 533ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 14; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 101ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 14; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 495ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 14; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 71ms/step
Total correct predictions: 2; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 14; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 66ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 15; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 276ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 15; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 61ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 15; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 274ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 15; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 45ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 3; independent_array: 15; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 60ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([39, 13]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 1; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 276ms/step
Total correct predictions: 8; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 1; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 44ms/step
Total correct predictions: 8; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 1; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 269ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 1; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 71ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 1; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 59ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 2; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 299ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 2; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 61ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 2; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 311ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 2; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 41ms/step
Total correct predictions: 2; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 2; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 53ms/step
Total correct predictions: 9; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 3; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 346ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 3; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 42ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 3; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 307ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 3; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 53ms/step
Total correct predictions: 2; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 3; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 63ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 4; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 263ms/step
Total correct predictions: 8; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 4; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 69ms/step
Total correct predictions: 8; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 4; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 276ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 4; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 56ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 4; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 54ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 5; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 289ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 5; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 58ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 5; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 250ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 5; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 44ms/step
Total correct predictions: 2; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 5; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 91ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 6; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 502ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 6; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 69ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 6; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 502ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 6; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 107ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 6; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 79ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 7; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 398ms/step
Total correct predictions: 8; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 7; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 73ms/step
Total correct predictions: 8; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 7; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 408ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 7; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 108ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 7; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 73ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 8; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 468ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 8; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 124ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 8; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 399ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 8; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 72ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 8; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 106ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 9; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 428ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 9; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 152ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 9; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 272ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 9; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 57ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 9; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 44ms/step
Total correct predictions: 9; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 10; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 247ms/step
Total correct predictions: 8; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 10; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 39ms/step
Total correct predictions: 8; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 10; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 295ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 10; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 44ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 10; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 58ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 11; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 275ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 11; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 57ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 11; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 341ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 11; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 62ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 11; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 45ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 12; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 254ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 12; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 47ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 12; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 312ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 12; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 48ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 12; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 76ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 13; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 298ms/step
Total correct predictions: 8; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 13; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 48ms/step
Total correct predictions: 8; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 13; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 365ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 13; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 49ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 13; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 85ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 14; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 277ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 14; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 74ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 14; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 313ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 14; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 48ms/step
Total correct predictions: 2; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 14; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 51ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 15; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 438ms/step
Total correct predictions: 3; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 15; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 117ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 15; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 539ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 15; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 112ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 5; independent_array: 15; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 132ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([35, 17]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 1; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 578ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 1; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 121ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 1; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 540ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 1; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 154ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 1; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 133ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 2; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 602ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 2; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 47ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 2; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 683ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 2; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 53ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 2; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 111ms/step
Total correct predictions: 8; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 3; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 501ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 3; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 129ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 3; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 457ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 3; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 72ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 3; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 52ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 4; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 277ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 4; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 43ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 4; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 367ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 4; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 57ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 4; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 46ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 5; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 300ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 5; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 50ms/step
Total correct predictions: 8; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 5; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 268ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 5; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 48ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 5; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 67ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 6; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 293ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 6; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 41ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 6; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 258ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 6; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 50ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 6; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 52ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 7; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 301ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 7; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 32ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 7; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 253ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 7; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 56ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 7; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 51ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 8; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 309ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 8; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 45ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 8; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 273ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 8; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 58ms/step
Total correct predictions: 2; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 8; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 71ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 9; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 250ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 9; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 41ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 9; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 285ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 9; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 46ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 9; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 60ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 10; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 512ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 10; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 105ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 10; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 772ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 10; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 78ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 10; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 89ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 11; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 566ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 11; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 102ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 11; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 459ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 11; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 101ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 11; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 83ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 12; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 583ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 12; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 115ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 12; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 501ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 12; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 118ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 12; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 78ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 13; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 509ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 13; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 58ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 13; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 646ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 13; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 115ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 13; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 93ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 14; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 279ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 14; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 58ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 14; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 293ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 14; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 38ms/step
Total correct predictions: 2; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 14; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 51ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 15; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 287ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 15; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 47ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 15; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 303ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 15; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 37ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 10; independent_array: 15; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 49ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 1; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 291ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 1; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 50ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 1; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 277ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 1; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 52ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 1; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 46ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 2; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 311ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 2; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 53ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 2; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 320ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 2; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 38ms/step
Total correct predictions: 2; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 2; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 70ms/step
Total correct predictions: 9; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 3; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 329ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 3; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 72ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 3; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 323ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 3; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 59ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 3; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 56ms/step
Total correct predictions: 9; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 4; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 267ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 4; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 50ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 4; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 257ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 4; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 41ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 4; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 52ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 5; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 566ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 5; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 101ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 5; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 485ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 5; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 114ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 5; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 104ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 6; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 409ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 6; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 92ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 6; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 447ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 6; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 86ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 6; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 95ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 7; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 502ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 7; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 115ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 7; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 601ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 7; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 80ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 7; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 109ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 8; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 482ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 8; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 103ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 8; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 462ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 8; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 86ms/step
Total correct predictions: 2; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 8; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 61ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 9; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 480ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 9; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 53ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 9; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 260ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 9; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 68ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 9; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 50ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 10; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 297ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 10; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 47ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 10; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 294ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 10; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 56ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 10; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 33ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 11; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 274ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 11; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 33ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 11; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 272ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 11; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 44ms/step
Total correct predictions: 2; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 11; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 65ms/step
Total correct predictions: 9; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 12; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 270ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 12; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 47ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 12; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 250ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 12; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 44ms/step
Total correct predictions: 4; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 12; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 46ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 13; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 335ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 13; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 65ms/step
Total correct predictions: 7; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 13; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 347ms/step
Total correct predictions: 3; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 13; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 59ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 13; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 44ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 14; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 298ms/step
Total correct predictions: 4; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 14; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 39ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 14; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 266ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 14; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 48ms/step
Total correct predictions: 2; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 14; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 42ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 15; K-fold: 1
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 266ms/step
Total correct predictions: 6; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 15; K-fold: 2
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 49ms/step
Total correct predictions: 5; out of 11 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 15; K-fold: 3
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 273ms/step
Total correct predictions: 5; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 15; K-fold: 4
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 43ms/step
Total correct predictions: 6; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
Now running, pct_increase: 1.02; days out: 15; independent_array: 15; K-fold: 5
<span class="ansi-bold">1/1</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 96ms/step
Total correct predictions: 7; out of 10 observations in test set
Total observations (combined train and test sets): 52
Number of observations each class from dependent variable (combined): (array([0, 1]), array([26, 26]))
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=9cb5c5ca-dcc3-458e-b635-684b5eac6d69">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[141]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1">#will output multiple CSV files with training results for classification model</span>
<span class="n">accuracy_df</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="sa">f</span><span class="s1">'</span><span class="si">{</span><span class="n">ticker_symbol</span><span class="si">}</span><span class="s1">_</span><span class="si">{</span><span class="n">selected_pattern</span><span class="si">}</span><span class="s1">_regression_output.csv'</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># `index=False` avoids writing the index column</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=78ebb6b0-6033-4715-b52e-02959294bc95">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="Increasing-number-of-observations-for-best-performing-model-(Chose-Classification-Over-Regression-as-Best-Performing-Model)">Increasing number of observations for best performing model (Chose Classification Over Regression as Best Performing Model)<a class="anchor-link" href="#Increasing-number-of-observations-for-best-performing-model-(Chose-Classification-Over-Regression-as-Best-Performing-Model)"></a></h4><p>After reviewing all of the CSV outputs with the training result data and respective accuracy scores, I have determined that the classification model performs better and more consistent with independent array #15 as the best performing set of independent variables. <strong>I further explain why I chose the classification model over the regression model in the 'Reporting' section of this document.</strong></p>
<p><strong>At this point, I have enough data to answer my research question, however, now I am going to increase the number of randomly generated sequences in order to make my model more adaptable for use on any given day as the hammer and inverted candlestick patterns have around a 1% occurrence rate. My hope is that even when a true candlestick pattern is not present, the model could still make reliable predictions for future closing prices.</strong></p>
<p>Now, I am going to see if increasing the number of observations will change the accuracy scores for my best performing set of independent variables. While I cannot increase the number of observations when a candlestick pattern is identified, as that number is already fixed, I can increase the number of observations for my random "Yes" and "No" values. By simulating the presence of a random pattern with more data, I will test whether adding additional random observations (via the newly generated 'Random_Yes_No_2' column) will influence the accuracy scores. This approach will allow me to assess the impact of a larger dataset on model performance using the same model architecture.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=d65bf6af-d36e-47f2-bead-b2fc8ba972f4">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[142]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1">####Used to create another new column to test random values of 'yes' to simulate presence of a random pattern</span>
<span class="c1"># Specify the number of "Yes" values you want, may show up as less during training due to location of the "Yes" value, as need at least 30 days</span>
<span class="c1">#of data for the 30-day sequence, or if the future closing price is not available (only have data to 2/14)</span>
<span class="n">num_yes</span> <span class="o">=</span> <span class="mi">2200</span>

<span class="c1"># Create a list of "Yes" and "No" values</span>
<span class="n">yes_no_list</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"Yes"</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_yes</span> <span class="o">+</span> <span class="p">[</span><span class="s2">"No"</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">finance_df</span><span class="p">)</span> <span class="o">-</span> <span class="n">num_yes</span><span class="p">)</span>

<span class="c1">#set seed for reproducibility</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span> 

<span class="c1"># Shuffle the list to randomize the order</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">yes_no_list</span><span class="p">)</span>

<span class="c1"># Add the list as a new column in the DataFrame; we already have a column 'Random_Yes_No' which was used to train the occurence of a random pattern</span>
<span class="c1">#this mimics that idea but will now be a new column 'Random_Yes_No_2', but this time with more generated random observations</span>
<span class="n">finance_df</span><span class="p">[</span><span class="s1">'Random_Yes_No_2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">yes_no_list</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=dfd77dd9-3047-4c68-afdf-f9f6f8dab158">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[143]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tensorflow.keras.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tensorflow.keras.layers</span><span class="w"> </span><span class="kn">import</span> <span class="n">LSTM</span><span class="p">,</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Dropout</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">KFold</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">StratifiedKFold</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tensorflow.keras.optimizers</span><span class="w"> </span><span class="kn">import</span> <span class="n">RMSprop</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>


<span class="c1">### User inputs ###</span>
<span class="n">selected_pattern</span> <span class="o">=</span> <span class="s2">"Random"</span>   <span class="c1">#choices: 'Random', 'Hammer', 'InvertedHammer'</span>

<span class="c1">#How many days after the pattern is identified to use for the dependent variable</span>
<span class="n">days_out</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">15</span><span class="p">]</span>

<span class="c1">#What percent increase from the current price is considered a positive class. For example 1.01 = 1% increase; 100 * 1.01 = 101. So if original</span>
<span class="c1">#price is $100, anything greater than $101 is considered a positive class.</span>
<span class="n">pct_increase</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.00</span><span class="p">,</span> <span class="mf">1.01</span><span class="p">,</span> <span class="mf">1.02</span><span class="p">]</span>

<span class="c1">######</span>


<span class="c1"># Define the classification model</span>
<span class="k">def</span><span class="w"> </span><span class="nf">create_lstm_classification</span><span class="p">(</span><span class="n">input_shape</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
    
    <span class="c1"># LSTM layers</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'tanh'</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">))</span>  <span class="c1"># Dropout to reduce overfitting</span>
    
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'tanh'</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>  <span class="c1"># Final LSTM layer</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">))</span>
    
    <span class="c1"># Dense output layer for binary classification</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'sigmoid'</span><span class="p">))</span>  <span class="c1"># Sigmoid for binary classification (probability)</span>
    
    <span class="c1"># Compile the model</span>
    <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">'adam'</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s1">'binary_crossentropy'</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'accuracy'</span><span class="p">])</span>  <span class="c1"># Binary cross-entropy for classification</span>
    <span class="k">return</span> <span class="n">model</span>

    

<span class="c1">#Subset data frame for desired pattern</span>
<span class="k">if</span> <span class="p">(</span><span class="n">selected_pattern</span> <span class="o">==</span> <span class="s2">"Random"</span><span class="p">):</span>
    <span class="n">pattern_df</span> <span class="o">=</span> <span class="n">finance_df</span><span class="p">[</span><span class="n">finance_df</span><span class="p">[</span><span class="s1">'Random_Yes_No_2'</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"Yes"</span><span class="p">]</span> <span class="c1">#this time, will select the newly created column with more observations</span>
<span class="k">elif</span> <span class="p">(</span><span class="n">selected_pattern</span> <span class="o">==</span> <span class="s2">"Hammer"</span><span class="p">):</span>
    <span class="n">pattern_df</span> <span class="o">=</span> <span class="n">finance_df</span><span class="p">[</span><span class="n">finance_df</span><span class="p">[</span><span class="s1">'Hammer_pattern'</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"Yes"</span><span class="p">]</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">pattern_df</span> <span class="o">=</span> <span class="n">finance_df</span><span class="p">[</span><span class="n">finance_df</span><span class="p">[</span><span class="s1">'InvertedHammer_pattern'</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"Yes"</span><span class="p">]</span>


<span class="c1">#initialize an empty DataFrame with column names</span>
<span class="n">accuracy_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">'ticker'</span><span class="p">,</span> <span class="s1">'pattern'</span><span class="p">,</span> <span class="s1">'independent_array'</span><span class="p">,</span> <span class="s1">'best_accuracy'</span><span class="p">,</span> <span class="s1">'avg_accuracy'</span><span class="p">,</span> <span class="s1">'days_out'</span><span class="p">,</span> <span class="s1">'Total_observations'</span><span class="p">,</span> 
                                   <span class="s1">'Negative_observations'</span><span class="p">,</span> <span class="s1">'Positive_observations'</span><span class="p">,</span> <span class="s1">'Percent_increase_parameter'</span><span class="p">])</span>


<span class="k">for</span> <span class="n">percent</span> <span class="ow">in</span> <span class="n">pct_increase</span><span class="p">:</span>

    <span class="k">for</span> <span class="n">day</span> <span class="ow">in</span> <span class="n">days_out</span><span class="p">:</span>
        <span class="c1">#Gather independent variables</span>
        <span class="n">independent_list1</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">independent_list2</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">independent_list3</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">independent_list4</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">independent_list5</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">independent_list6</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">independent_list7</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">independent_list8</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">independent_list9</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">independent_list10</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">independent_list11</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">independent_list12</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">independent_list13</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">independent_list14</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">independent_list15</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="c1">#gather dependent variables</span>
        <span class="n">dependent_list</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1">#these are the row indexes that have the identified patterns; loop through</span>
        <span class="n">pattern_index</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">pattern_df</span><span class="p">[</span><span class="s2">"Row_index"</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">pattern_index</span><span class="p">:</span>
            <span class="c1">#if (i == 62):</span>
            <span class="c1">#    break</span>
            
            <span class="c1">#unable to get 30 days worth of data if index is less than 56, because previously removed first 26 observations</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="mi">56</span><span class="p">):</span>
                <span class="k">continue</span>
        
            <span class="c1">#get 30 days worth of data to gather data for indpendent variables</span>
            <span class="n">subset_df</span> <span class="o">=</span> <span class="n">finance_df</span><span class="p">[(</span><span class="n">finance_df</span><span class="p">[</span><span class="s2">"Row_index"</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="n">i</span> <span class="o">-</span> <span class="mi">29</span><span class="p">))</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">finance_df</span><span class="p">[</span><span class="s2">"Row_index"</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="p">(</span><span class="n">i</span><span class="p">))]</span>
            <span class="c1">#subset_df = finance_df[(finance_df["Row_index"] &gt;= (i - 13)) &amp; (finance_df["Row_index"] &lt;= (i))]</span>
            
            <span class="c1">#Get day after data to gather closing price for dependent variable</span>
            <span class="n">dependent_df</span> <span class="o">=</span> <span class="n">finance_df</span><span class="p">[</span><span class="n">finance_df</span><span class="p">[</span><span class="s2">"Row_index"</span><span class="p">]</span> <span class="o">==</span> <span class="p">(</span><span class="n">i</span><span class="p">)]</span>
            <span class="n">dependent2_df</span> <span class="o">=</span> <span class="n">finance_df</span><span class="p">[</span><span class="n">finance_df</span><span class="p">[</span><span class="s2">"Row_index"</span><span class="p">]</span> <span class="o">==</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="n">day</span><span class="p">)]</span>
            
            <span class="n">temp_list1</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">temp_list2</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">temp_list3</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">temp_list4</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">temp_list5</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">temp_list6</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">temp_list7</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">temp_list8</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">temp_list9</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">temp_list10</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">temp_list11</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">temp_list12</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">temp_list13</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">temp_list14</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">temp_list15</span> <span class="o">=</span> <span class="p">[]</span>
        
            <span class="c1">#append temp_list to independent_list</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">dependent2_df</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span> <span class="c1">#dependent2_df may have length of zero as it is a future date, data may not be available</span>
            
        
                <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">subset_df</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
                        
                        <span class="n">test_array1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Low'</span><span class="p">]])</span>
                        <span class="n">test_array2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Log_Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_Low'</span><span class="p">]])</span>
                        <span class="n">test_array3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Low'</span><span class="p">]])</span>
                
                        <span class="n">test_array4</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'RSI'</span><span class="p">]])</span>
                        <span class="n">test_array5</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Log_Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'RSI'</span><span class="p">]])</span>
                        <span class="n">test_array6</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'RSI'</span><span class="p">]])</span>
                
                        <span class="n">test_array7</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MFI'</span><span class="p">]])</span>
                        <span class="n">test_array8</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Log_Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MFI'</span><span class="p">]])</span>
                        <span class="n">test_array9</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MFI'</span><span class="p">]])</span>
                
                        <span class="n">test_array10</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MACD'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Signal_Line'</span><span class="p">]])</span>
                        <span class="n">test_array11</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Log_Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MACD'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Signal_Line'</span><span class="p">]])</span>
                        <span class="n">test_array12</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MACD'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Signal_Line'</span><span class="p">]])</span>
                
                        <span class="n">test_array13</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'RSI'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MFI'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MACD'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Signal_Line'</span><span class="p">]])</span>
                        <span class="n">test_array14</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Log_Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Log_Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'RSI'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MFI'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MACD'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Signal_Line'</span><span class="p">]])</span>
                        <span class="n">test_array15</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'RSI'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MFI'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MACD'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Signal_Line'</span><span class="p">]])</span>
                
                
                        <span class="n">temp_list1</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array1</span><span class="p">)</span>
                        <span class="n">temp_list2</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array2</span><span class="p">)</span>
                        <span class="n">temp_list3</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array3</span><span class="p">)</span>
                        <span class="n">temp_list4</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array4</span><span class="p">)</span>
                        <span class="n">temp_list5</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array5</span><span class="p">)</span>
                        <span class="n">temp_list6</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array6</span><span class="p">)</span>
                        <span class="n">temp_list7</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array7</span><span class="p">)</span>
                        <span class="n">temp_list8</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array8</span><span class="p">)</span>
                        <span class="n">temp_list9</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array9</span><span class="p">)</span>
                        <span class="n">temp_list10</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array10</span><span class="p">)</span>
                        <span class="n">temp_list11</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array11</span><span class="p">)</span>
                        <span class="n">temp_list12</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array12</span><span class="p">)</span>
                        <span class="n">temp_list13</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array13</span><span class="p">)</span>
                        <span class="n">temp_list14</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array14</span><span class="p">)</span>
                        <span class="n">temp_list15</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array15</span><span class="p">)</span>
                        
                <span class="n">independent_list1</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list1</span><span class="p">)</span>
                <span class="n">independent_list2</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list2</span><span class="p">)</span>
                <span class="n">independent_list3</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list3</span><span class="p">)</span>
                <span class="n">independent_list4</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list4</span><span class="p">)</span>
                <span class="n">independent_list5</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list5</span><span class="p">)</span>
                <span class="n">independent_list6</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list6</span><span class="p">)</span>
                <span class="n">independent_list7</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list7</span><span class="p">)</span>
                <span class="n">independent_list8</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list8</span><span class="p">)</span>
                <span class="n">independent_list9</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list9</span><span class="p">)</span>
                <span class="n">independent_list10</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list10</span><span class="p">)</span>
                <span class="n">independent_list11</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list11</span><span class="p">)</span>
                <span class="n">independent_list12</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list12</span><span class="p">)</span>
                <span class="n">independent_list13</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list13</span><span class="p">)</span>
                <span class="n">independent_list14</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list14</span><span class="p">)</span>
                <span class="n">independent_list15</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list15</span><span class="p">)</span>
            
                <span class="k">if</span> <span class="p">(</span><span class="n">dependent2_df</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">dependent_df</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">percent</span><span class="p">):</span>
                    <span class="n">dependent_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">dependent_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        
        <span class="n">independent_array1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list1</span><span class="p">)</span>
        <span class="n">independent_array2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list2</span><span class="p">)</span>
        <span class="n">independent_array3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list3</span><span class="p">)</span>
        <span class="n">independent_array4</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list4</span><span class="p">)</span>
        <span class="n">independent_array5</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list5</span><span class="p">)</span>
        <span class="n">independent_array6</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list6</span><span class="p">)</span>
        <span class="n">independent_array7</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list7</span><span class="p">)</span>
        <span class="n">independent_array8</span><span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list8</span><span class="p">)</span>
        <span class="n">independent_array9</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list9</span><span class="p">)</span>
        <span class="n">independent_array10</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list10</span><span class="p">)</span>
        <span class="n">independent_array11</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list11</span><span class="p">)</span>
        <span class="n">independent_array12</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list12</span><span class="p">)</span>
        <span class="n">independent_array13</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list13</span><span class="p">)</span>
        <span class="n">independent_array14</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list14</span><span class="p">)</span>
        <span class="n">independent_array15</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list15</span><span class="p">)</span>
        <span class="n">dependent_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dependent_list</span><span class="p">)</span>
    
    
        <span class="n">y</span> <span class="o">=</span> <span class="n">dependent_array</span>
        <span class="n">independent_array</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">best_accuracy</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">avg_accuracy</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">counter_independentarray</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">15</span><span class="p">:</span> <span class="c1">#testing what seems is the most well performing model</span>
                <span class="k">continue</span>
            
            <span class="c1"># Select which independent_array to use</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">independent_array1</span>  <span class="c1"># Shape: (890, 30, 4)</span>
                <span class="n">independent_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"independent_array1"</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">independent_array2</span>  <span class="c1"># Shape: (890, 30, 4)</span>
                <span class="n">independent_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"independent_array2"</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">independent_array3</span>  <span class="c1"># Shape: (890, 30, 4)</span>
                <span class="n">independent_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"independent_array3"</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">independent_array4</span>  <span class="c1"># Shape: (890, 30, 5)</span>
                <span class="n">independent_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"independent_array4"</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">5</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">independent_array5</span>  <span class="c1"># Shape: (890, 30, 5)</span>
                <span class="n">independent_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"independent_array5"</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">6</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">independent_array6</span>  <span class="c1"># Shape: (890, 30, 5)</span>
                <span class="n">independent_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"independent_array6"</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">7</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">independent_array7</span>  <span class="c1"># Shape: (890, 30, 5)</span>
                <span class="n">independent_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"independent_array7"</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">8</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">independent_array8</span>  <span class="c1"># Shape: (890, 30, 5)</span>
                <span class="n">independent_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"independent_array8"</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">9</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">independent_array9</span>  <span class="c1"># Shape: (890, 30, 5)</span>
                <span class="n">independent_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"independent_array9"</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">10</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">independent_array10</span>  <span class="c1"># Shape: (890, 30, 6)</span>
                <span class="n">independent_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"independent_array10"</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">11</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">independent_array11</span>  <span class="c1"># Shape: (890, 30, 6)</span>
                <span class="n">independent_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"independent_array11"</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">12</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">independent_array12</span>  <span class="c1"># Shape: (890, 30, 6)</span>
                <span class="n">independent_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"independent_array12"</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">13</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">independent_array13</span>  <span class="c1"># Shape: (890, 30, 8)</span>
                <span class="n">independent_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"independent_array13"</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">14</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">independent_array14</span>  <span class="c1"># Shape: (890, 30, 8)</span>
                <span class="n">independent_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"independent_array14"</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">15</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">independent_array15</span>  <span class="c1"># Shape: (890, 30, 8)</span>
                <span class="n">independent_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"independent_array15"</span><span class="p">)</span>
        
            <span class="c1">#counter_independentarray = counter_independentarray + 1</span>
            
            <span class="c1"># Define the input shape based on the number of features</span>
            <span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>  <span class="c1"># 30 time-steps and `X.shape[2]` features per time-step</span>
            
            <span class="c1"># Create the LSTM model</span>
            <span class="n">classification_model</span> <span class="o">=</span> <span class="n">create_lstm_classification</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
            
            <span class="c1"># Initialize k-fold cross-validation</span>
            <span class="c1">#kf = KFold(n_splits=5, shuffle=True, random_state=6)  #regular 5-fold cross-validation w/out stratification</span>
            <span class="n">kf</span> <span class="o">=</span> <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>  <span class="c1"># 5-fold cross-validation with stratification</span>

            <span class="c1">#initialize to gather all the accuracy scores at each epoch for all 5 folds</span>
            <span class="n">fold_accuracies</span> <span class="o">=</span> <span class="p">[]</span>
            
            <span class="c1">#stratified K-fold Cross-Validation</span>
            <span class="n">counter_kfold</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">train_index</span><span class="p">,</span> <span class="n">val_index</span> <span class="ow">in</span> <span class="n">kf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span> <span class="c1">#used for stratified k-fold</span>
            <span class="c1">#for train_index, val_index in kf.split(X): #used for regular k-fold</span>
                
                <span class="n">counter_kfold</span> <span class="o">=</span> <span class="n">counter_kfold</span> <span class="o">+</span> <span class="mi">1</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Now running, pct_increase: </span><span class="si">{</span><span class="n">percent</span><span class="si">}</span><span class="s2">; days out: </span><span class="si">{</span><span class="n">day</span><span class="si">}</span><span class="s2">; independent_array: </span><span class="si">{</span><span class="n">independent_array</span><span class="p">[(</span><span class="nb">len</span><span class="p">(</span><span class="n">independent_array</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="p">)]</span><span class="si">}</span><span class="s2">; K-fold: </span><span class="si">{</span><span class="n">counter_kfold</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
                
                <span class="n">X_train</span><span class="p">,</span> <span class="n">X_val</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">val_index</span><span class="p">]</span>
                <span class="n">y_train</span><span class="p">,</span> <span class="n">y_val</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">val_index</span><span class="p">]</span>
                
                <span class="c1"># Train the classification model and store the history; verbose = 0 to hide epoch running info in cell output</span>
                <span class="n">history</span> <span class="o">=</span> <span class="n">classification_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">),</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                
                <span class="c1"># Get the validation accuracies for this fold. What this does is that an accuracy score is calculated at each epoch,</span>
                <span class="c1">#and in this list I am getting all the accuracy scores from all five folds</span>
                <span class="n">val_accuracy</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">'val_accuracy'</span><span class="p">]</span>
                <span class="n">fold_accuracies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">val_accuracy</span><span class="p">)</span>
        
        
            
            <span class="c1"># Calculate the best and average validation accuracy across all folds</span>
            <span class="n">best_val_accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">fold_accuracies</span><span class="p">)</span> <span class="c1">#get the max accuracy across all epochs across all five folds</span>
            <span class="n">avg_val_accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">fold_accuracies</span><span class="p">)</span> <span class="c1">#get the mean accuracy across all epochs across all five folds</span>
            <span class="n">best_accuracy</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">best_val_accuracy</span><span class="p">)</span>
            <span class="n">avg_accuracy</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">avg_val_accuracy</span><span class="p">)</span>
        
        
        <span class="c1"># Example of new data to add</span>
        <span class="n">df_new</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
            <span class="s1">'ticker'</span><span class="p">:</span> <span class="n">ticker_symbol</span><span class="p">,</span>
            <span class="s1">'pattern'</span><span class="p">:</span> <span class="n">selected_pattern</span><span class="p">,</span>
            <span class="s1">'independent_array'</span><span class="p">:</span> <span class="n">independent_array</span><span class="p">,</span>
            <span class="s1">'best_accuracy'</span><span class="p">:</span> <span class="n">best_accuracy</span><span class="p">,</span>
            <span class="s1">'avg_accuracy'</span><span class="p">:</span> <span class="n">avg_accuracy</span><span class="p">,</span>
            <span class="s1">'days_out'</span><span class="p">:</span> <span class="n">day</span><span class="p">,</span>
            <span class="s1">'Total_observations'</span><span class="p">:</span> <span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">dependent_array</span><span class="p">,</span> <span class="n">return_counts</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">1</span><span class="p">]),</span>
            <span class="s1">'Negative_observations'</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">dependent_array</span><span class="p">,</span> <span class="n">return_counts</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
            <span class="s1">'Positive_observations'</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">dependent_array</span><span class="p">,</span> <span class="n">return_counts</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span>
            <span class="s1">'Percent_increase_parameter'</span><span class="p">:</span> <span class="n">percent</span>
        <span class="p">})</span>
    
        <span class="c1"># Concatenate the new data to the empty DataFrame</span>
        <span class="n">accuracy_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">accuracy_df</span><span class="p">,</span> <span class="n">df_new</span><span class="p">],</span> <span class="n">ignore_index</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 1; independent_array: independent_array15; K-fold: 1
Epoch 1/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">9s</span> 92ms/step - accuracy: 0.5199 - loss: 0.7155 - val_accuracy: 0.4521 - val_loss: 0.6954
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 72ms/step - accuracy: 0.5184 - loss: 0.6940 - val_accuracy: 0.5000 - val_loss: 0.6950
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">6s</span> 86ms/step - accuracy: 0.5173 - loss: 0.6976 - val_accuracy: 0.5297 - val_loss: 0.6909
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 80ms/step - accuracy: 0.5402 - loss: 0.6860 - val_accuracy: 0.5388 - val_loss: 0.6906
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 83ms/step - accuracy: 0.5505 - loss: 0.6877 - val_accuracy: 0.5342 - val_loss: 0.6907
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 62ms/step - accuracy: 0.5293 - loss: 0.6888 - val_accuracy: 0.5000 - val_loss: 0.6941
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 67ms/step - accuracy: 0.5610 - loss: 0.6899 - val_accuracy: 0.5000 - val_loss: 0.6938
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 62ms/step - accuracy: 0.5568 - loss: 0.6896 - val_accuracy: 0.5297 - val_loss: 0.6911
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 67ms/step - accuracy: 0.5274 - loss: 0.6893 - val_accuracy: 0.5411 - val_loss: 0.6920
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 66ms/step - accuracy: 0.5503 - loss: 0.6889 - val_accuracy: 0.5342 - val_loss: 0.6907
Now running, pct_increase: 1.0; days out: 1; independent_array: independent_array15; K-fold: 2
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 69ms/step - accuracy: 0.5395 - loss: 0.6903 - val_accuracy: 0.5479 - val_loss: 0.6874
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 65ms/step - accuracy: 0.5397 - loss: 0.6919 - val_accuracy: 0.5639 - val_loss: 0.6853
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 60ms/step - accuracy: 0.5661 - loss: 0.6847 - val_accuracy: 0.5662 - val_loss: 0.6863
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 65ms/step - accuracy: 0.5259 - loss: 0.6927 - val_accuracy: 0.5479 - val_loss: 0.6874
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 60ms/step - accuracy: 0.5561 - loss: 0.6895 - val_accuracy: 0.5548 - val_loss: 0.6875
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 69ms/step - accuracy: 0.5437 - loss: 0.6891 - val_accuracy: 0.5457 - val_loss: 0.6865
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 66ms/step - accuracy: 0.5510 - loss: 0.6863 - val_accuracy: 0.5502 - val_loss: 0.6878
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 55ms/step - accuracy: 0.5593 - loss: 0.6860 - val_accuracy: 0.5457 - val_loss: 0.6897
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 43ms/step - accuracy: 0.5328 - loss: 0.6902 - val_accuracy: 0.5457 - val_loss: 0.6913
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 42ms/step - accuracy: 0.5446 - loss: 0.6876 - val_accuracy: 0.5594 - val_loss: 0.6863
Now running, pct_increase: 1.0; days out: 1; independent_array: independent_array15; K-fold: 3
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 41ms/step - accuracy: 0.5588 - loss: 0.6864 - val_accuracy: 0.5639 - val_loss: 0.6857
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 41ms/step - accuracy: 0.5470 - loss: 0.6887 - val_accuracy: 0.5685 - val_loss: 0.6857
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 42ms/step - accuracy: 0.5473 - loss: 0.6906 - val_accuracy: 0.5639 - val_loss: 0.6856
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 42ms/step - accuracy: 0.5463 - loss: 0.6891 - val_accuracy: 0.5708 - val_loss: 0.6850
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 41ms/step - accuracy: 0.5521 - loss: 0.6868 - val_accuracy: 0.5594 - val_loss: 0.6860
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 44ms/step - accuracy: 0.5571 - loss: 0.6846 - val_accuracy: 0.5731 - val_loss: 0.6866
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 38ms/step - accuracy: 0.5580 - loss: 0.6871 - val_accuracy: 0.5479 - val_loss: 0.6873
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 43ms/step - accuracy: 0.5679 - loss: 0.6863 - val_accuracy: 0.5205 - val_loss: 0.6932
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 44ms/step - accuracy: 0.5226 - loss: 0.6900 - val_accuracy: 0.5616 - val_loss: 0.6873
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 46ms/step - accuracy: 0.5576 - loss: 0.6860 - val_accuracy: 0.5434 - val_loss: 0.6901
Now running, pct_increase: 1.0; days out: 1; independent_array: independent_array15; K-fold: 4
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 45ms/step - accuracy: 0.5471 - loss: 0.6885 - val_accuracy: 0.5365 - val_loss: 0.6884
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 45ms/step - accuracy: 0.5415 - loss: 0.6894 - val_accuracy: 0.5434 - val_loss: 0.6911
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 46ms/step - accuracy: 0.5488 - loss: 0.6882 - val_accuracy: 0.5434 - val_loss: 0.6911
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 44ms/step - accuracy: 0.5549 - loss: 0.6844 - val_accuracy: 0.5365 - val_loss: 0.6891
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 41ms/step - accuracy: 0.5691 - loss: 0.6840 - val_accuracy: 0.5342 - val_loss: 0.6914
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 42ms/step - accuracy: 0.5506 - loss: 0.6861 - val_accuracy: 0.5434 - val_loss: 0.6894
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 44ms/step - accuracy: 0.5669 - loss: 0.6836 - val_accuracy: 0.5411 - val_loss: 0.7003
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 44ms/step - accuracy: 0.5268 - loss: 0.6945 - val_accuracy: 0.5320 - val_loss: 0.6913
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 44ms/step - accuracy: 0.5474 - loss: 0.6867 - val_accuracy: 0.5297 - val_loss: 0.6916
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 44ms/step - accuracy: 0.5290 - loss: 0.6881 - val_accuracy: 0.5297 - val_loss: 0.6942
Now running, pct_increase: 1.0; days out: 1; independent_array: independent_array15; K-fold: 5
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 46ms/step - accuracy: 0.5571 - loss: 0.6860 - val_accuracy: 0.5309 - val_loss: 0.6882
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 50ms/step - accuracy: 0.5558 - loss: 0.6823 - val_accuracy: 0.5538 - val_loss: 0.6855
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 52ms/step - accuracy: 0.5469 - loss: 0.6901 - val_accuracy: 0.5263 - val_loss: 0.6861
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 49ms/step - accuracy: 0.5594 - loss: 0.6850 - val_accuracy: 0.5309 - val_loss: 0.6864
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 51ms/step - accuracy: 0.5576 - loss: 0.6885 - val_accuracy: 0.5332 - val_loss: 0.6890
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 50ms/step - accuracy: 0.5523 - loss: 0.6879 - val_accuracy: 0.5355 - val_loss: 0.6875
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 48ms/step - accuracy: 0.5435 - loss: 0.6881 - val_accuracy: 0.5309 - val_loss: 0.6893
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 53ms/step - accuracy: 0.5638 - loss: 0.6827 - val_accuracy: 0.5240 - val_loss: 0.6873
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 51ms/step - accuracy: 0.5676 - loss: 0.6827 - val_accuracy: 0.5378 - val_loss: 0.6865
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 51ms/step - accuracy: 0.5764 - loss: 0.6765 - val_accuracy: 0.5492 - val_loss: 0.6844
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\AppData\Local\Temp\ipykernel_27512\3723232288.py:311: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  accuracy_df = pd.concat([accuracy_df, df_new], ignore_index=True)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Now running, pct_increase: 1.0; days out: 3; independent_array: independent_array15; K-fold: 1
Epoch 1/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">7s</span> 70ms/step - accuracy: 0.5491 - loss: 0.7074 - val_accuracy: 0.5753 - val_loss: 0.6837
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 83ms/step - accuracy: 0.5599 - loss: 0.6887 - val_accuracy: 0.5753 - val_loss: 0.6815
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 81ms/step - accuracy: 0.5764 - loss: 0.6848 - val_accuracy: 0.5753 - val_loss: 0.6811
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 81ms/step - accuracy: 0.5442 - loss: 0.6902 - val_accuracy: 0.5753 - val_loss: 0.6847
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 81ms/step - accuracy: 0.5657 - loss: 0.6843 - val_accuracy: 0.5753 - val_loss: 0.6809
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 77ms/step - accuracy: 0.5926 - loss: 0.6824 - val_accuracy: 0.5753 - val_loss: 0.6806
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">6s</span> 95ms/step - accuracy: 0.5815 - loss: 0.6837 - val_accuracy: 0.5753 - val_loss: 0.6797
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">10s</span> 88ms/step - accuracy: 0.5824 - loss: 0.6799 - val_accuracy: 0.5776 - val_loss: 0.6816
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 82ms/step - accuracy: 0.5801 - loss: 0.6816 - val_accuracy: 0.5731 - val_loss: 0.6810
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 73ms/step - accuracy: 0.5704 - loss: 0.6838 - val_accuracy: 0.5799 - val_loss: 0.6796
Now running, pct_increase: 1.0; days out: 3; independent_array: independent_array15; K-fold: 2
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 83ms/step - accuracy: 0.5828 - loss: 0.6816 - val_accuracy: 0.5753 - val_loss: 0.6806
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 75ms/step - accuracy: 0.5717 - loss: 0.6827 - val_accuracy: 0.5776 - val_loss: 0.6805
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 69ms/step - accuracy: 0.5925 - loss: 0.6775 - val_accuracy: 0.5776 - val_loss: 0.6829
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 53ms/step - accuracy: 0.5927 - loss: 0.6743 - val_accuracy: 0.5731 - val_loss: 0.6820
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 62ms/step - accuracy: 0.5984 - loss: 0.6753 - val_accuracy: 0.5708 - val_loss: 0.6853
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 61ms/step - accuracy: 0.5858 - loss: 0.6807 - val_accuracy: 0.5731 - val_loss: 0.6823
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 65ms/step - accuracy: 0.5800 - loss: 0.6761 - val_accuracy: 0.5662 - val_loss: 0.6831
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 64ms/step - accuracy: 0.5831 - loss: 0.6769 - val_accuracy: 0.5662 - val_loss: 0.6837
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 66ms/step - accuracy: 0.5799 - loss: 0.6792 - val_accuracy: 0.5594 - val_loss: 0.6842
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 63ms/step - accuracy: 0.5971 - loss: 0.6728 - val_accuracy: 0.5753 - val_loss: 0.6827
Now running, pct_increase: 1.0; days out: 3; independent_array: independent_array15; K-fold: 3
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 60ms/step - accuracy: 0.5703 - loss: 0.6810 - val_accuracy: 0.5812 - val_loss: 0.6788
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 62ms/step - accuracy: 0.5829 - loss: 0.6806 - val_accuracy: 0.5858 - val_loss: 0.6767
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 55ms/step - accuracy: 0.5846 - loss: 0.6766 - val_accuracy: 0.5767 - val_loss: 0.6768
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 67ms/step - accuracy: 0.6079 - loss: 0.6717 - val_accuracy: 0.5835 - val_loss: 0.6799
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 66ms/step - accuracy: 0.5832 - loss: 0.6807 - val_accuracy: 0.5995 - val_loss: 0.6736
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 40ms/step - accuracy: 0.5722 - loss: 0.6838 - val_accuracy: 0.5835 - val_loss: 0.6748
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 43ms/step - accuracy: 0.5803 - loss: 0.6773 - val_accuracy: 0.5881 - val_loss: 0.6732
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 40ms/step - accuracy: 0.6105 - loss: 0.6678 - val_accuracy: 0.5789 - val_loss: 0.6756
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 42ms/step - accuracy: 0.5874 - loss: 0.6745 - val_accuracy: 0.5721 - val_loss: 0.6802
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 40ms/step - accuracy: 0.6055 - loss: 0.6680 - val_accuracy: 0.5858 - val_loss: 0.6789
Now running, pct_increase: 1.0; days out: 3; independent_array: independent_array15; K-fold: 4
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 39ms/step - accuracy: 0.5866 - loss: 0.6729 - val_accuracy: 0.5904 - val_loss: 0.6750
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 41ms/step - accuracy: 0.5660 - loss: 0.6766 - val_accuracy: 0.5858 - val_loss: 0.6800
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 40ms/step - accuracy: 0.5684 - loss: 0.6726 - val_accuracy: 0.5881 - val_loss: 0.6810
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 40ms/step - accuracy: 0.6143 - loss: 0.6594 - val_accuracy: 0.5858 - val_loss: 0.6741
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 47ms/step - accuracy: 0.5889 - loss: 0.6715 - val_accuracy: 0.5789 - val_loss: 0.6742
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 43ms/step - accuracy: 0.5968 - loss: 0.6670 - val_accuracy: 0.5904 - val_loss: 0.6778
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 44ms/step - accuracy: 0.5965 - loss: 0.6665 - val_accuracy: 0.5812 - val_loss: 0.6769
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 40ms/step - accuracy: 0.6024 - loss: 0.6616 - val_accuracy: 0.5995 - val_loss: 0.6804
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 40ms/step - accuracy: 0.5908 - loss: 0.6684 - val_accuracy: 0.5904 - val_loss: 0.6750
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 41ms/step - accuracy: 0.6140 - loss: 0.6628 - val_accuracy: 0.5904 - val_loss: 0.6764
Now running, pct_increase: 1.0; days out: 3; independent_array: independent_array15; K-fold: 5
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 42ms/step - accuracy: 0.5874 - loss: 0.6697 - val_accuracy: 0.5881 - val_loss: 0.6803
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 41ms/step - accuracy: 0.5764 - loss: 0.6800 - val_accuracy: 0.5858 - val_loss: 0.6871
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 40ms/step - accuracy: 0.5970 - loss: 0.6630 - val_accuracy: 0.5950 - val_loss: 0.6817
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 44ms/step - accuracy: 0.5824 - loss: 0.6696 - val_accuracy: 0.5881 - val_loss: 0.6843
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 42ms/step - accuracy: 0.6025 - loss: 0.6653 - val_accuracy: 0.5858 - val_loss: 0.6745
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 40ms/step - accuracy: 0.6089 - loss: 0.6576 - val_accuracy: 0.5721 - val_loss: 0.6842
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 43ms/step - accuracy: 0.5976 - loss: 0.6668 - val_accuracy: 0.5789 - val_loss: 0.6827
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 49ms/step - accuracy: 0.6008 - loss: 0.6612 - val_accuracy: 0.5812 - val_loss: 0.6906
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 52ms/step - accuracy: 0.6176 - loss: 0.6563 - val_accuracy: 0.5744 - val_loss: 0.6921
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 52ms/step - accuracy: 0.6010 - loss: 0.6614 - val_accuracy: 0.5881 - val_loss: 0.6916
Now running, pct_increase: 1.0; days out: 5; independent_array: independent_array15; K-fold: 1
Epoch 1/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">6s</span> 61ms/step - accuracy: 0.5584 - loss: 0.6965 - val_accuracy: 0.5936 - val_loss: 0.6938
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 53ms/step - accuracy: 0.5735 - loss: 0.6907 - val_accuracy: 0.5936 - val_loss: 0.6763
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 51ms/step - accuracy: 0.5904 - loss: 0.6795 - val_accuracy: 0.5936 - val_loss: 0.6760
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 53ms/step - accuracy: 0.6004 - loss: 0.6717 - val_accuracy: 0.5936 - val_loss: 0.6762
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 51ms/step - accuracy: 0.5798 - loss: 0.6759 - val_accuracy: 0.5936 - val_loss: 0.6775
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 53ms/step - accuracy: 0.5948 - loss: 0.6779 - val_accuracy: 0.5913 - val_loss: 0.6761
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 60ms/step - accuracy: 0.5943 - loss: 0.6731 - val_accuracy: 0.5936 - val_loss: 0.6752
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 76ms/step - accuracy: 0.5977 - loss: 0.6744 - val_accuracy: 0.5936 - val_loss: 0.6776
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">6s</span> 84ms/step - accuracy: 0.5824 - loss: 0.6846 - val_accuracy: 0.5845 - val_loss: 0.6774
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 86ms/step - accuracy: 0.5895 - loss: 0.6759 - val_accuracy: 0.5913 - val_loss: 0.6779
Now running, pct_increase: 1.0; days out: 5; independent_array: independent_array15; K-fold: 2
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 84ms/step - accuracy: 0.5778 - loss: 0.6792 - val_accuracy: 0.5950 - val_loss: 0.6727
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 76ms/step - accuracy: 0.5913 - loss: 0.6755 - val_accuracy: 0.5950 - val_loss: 0.6763
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 79ms/step - accuracy: 0.5768 - loss: 0.6796 - val_accuracy: 0.5950 - val_loss: 0.6736
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">6s</span> 85ms/step - accuracy: 0.5962 - loss: 0.6724 - val_accuracy: 0.5950 - val_loss: 0.6728
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 85ms/step - accuracy: 0.6096 - loss: 0.6681 - val_accuracy: 0.5973 - val_loss: 0.6768
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 79ms/step - accuracy: 0.6037 - loss: 0.6707 - val_accuracy: 0.5995 - val_loss: 0.6728
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 74ms/step - accuracy: 0.5856 - loss: 0.6726 - val_accuracy: 0.5950 - val_loss: 0.6755
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 84ms/step - accuracy: 0.6117 - loss: 0.6654 - val_accuracy: 0.5950 - val_loss: 0.6767
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 78ms/step - accuracy: 0.5945 - loss: 0.6728 - val_accuracy: 0.5973 - val_loss: 0.6749
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 57ms/step - accuracy: 0.5922 - loss: 0.6703 - val_accuracy: 0.5950 - val_loss: 0.6767
Now running, pct_increase: 1.0; days out: 5; independent_array: independent_array15; K-fold: 3
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 62ms/step - accuracy: 0.6148 - loss: 0.6651 - val_accuracy: 0.6018 - val_loss: 0.6708
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 62ms/step - accuracy: 0.6097 - loss: 0.6675 - val_accuracy: 0.6087 - val_loss: 0.6678
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 65ms/step - accuracy: 0.5990 - loss: 0.6704 - val_accuracy: 0.6041 - val_loss: 0.6679
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 67ms/step - accuracy: 0.6049 - loss: 0.6648 - val_accuracy: 0.6041 - val_loss: 0.6666
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 61ms/step - accuracy: 0.6062 - loss: 0.6672 - val_accuracy: 0.6064 - val_loss: 0.6691
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 64ms/step - accuracy: 0.5991 - loss: 0.6677 - val_accuracy: 0.5835 - val_loss: 0.6734
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 69ms/step - accuracy: 0.5730 - loss: 0.6827 - val_accuracy: 0.6041 - val_loss: 0.6693
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 66ms/step - accuracy: 0.5862 - loss: 0.6713 - val_accuracy: 0.5927 - val_loss: 0.6688
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 68ms/step - accuracy: 0.6173 - loss: 0.6622 - val_accuracy: 0.6064 - val_loss: 0.6616
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 65ms/step - accuracy: 0.5985 - loss: 0.6640 - val_accuracy: 0.6064 - val_loss: 0.6675
Now running, pct_increase: 1.0; days out: 5; independent_array: independent_array15; K-fold: 4
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 60ms/step - accuracy: 0.6040 - loss: 0.6655 - val_accuracy: 0.5904 - val_loss: 0.6715
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 43ms/step - accuracy: 0.6140 - loss: 0.6566 - val_accuracy: 0.5515 - val_loss: 0.6814
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 41ms/step - accuracy: 0.5805 - loss: 0.6720 - val_accuracy: 0.5629 - val_loss: 0.6678
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 44ms/step - accuracy: 0.6092 - loss: 0.6551 - val_accuracy: 0.5858 - val_loss: 0.6760
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 43ms/step - accuracy: 0.6138 - loss: 0.6622 - val_accuracy: 0.5675 - val_loss: 0.6708
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 44ms/step - accuracy: 0.6097 - loss: 0.6643 - val_accuracy: 0.5538 - val_loss: 0.6773
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 43ms/step - accuracy: 0.6007 - loss: 0.6665 - val_accuracy: 0.5927 - val_loss: 0.6756
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 46ms/step - accuracy: 0.6182 - loss: 0.6567 - val_accuracy: 0.5721 - val_loss: 0.6860
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 44ms/step - accuracy: 0.6004 - loss: 0.6505 - val_accuracy: 0.5995 - val_loss: 0.6650
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 42ms/step - accuracy: 0.6268 - loss: 0.6469 - val_accuracy: 0.5950 - val_loss: 0.6806
Now running, pct_increase: 1.0; days out: 5; independent_array: independent_array15; K-fold: 5
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 43ms/step - accuracy: 0.6157 - loss: 0.6498 - val_accuracy: 0.6407 - val_loss: 0.6347
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 40ms/step - accuracy: 0.6123 - loss: 0.6536 - val_accuracy: 0.6613 - val_loss: 0.6341
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 44ms/step - accuracy: 0.6294 - loss: 0.6443 - val_accuracy: 0.6270 - val_loss: 0.6390
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 45ms/step - accuracy: 0.6353 - loss: 0.6380 - val_accuracy: 0.6453 - val_loss: 0.6339
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 43ms/step - accuracy: 0.6107 - loss: 0.6469 - val_accuracy: 0.6270 - val_loss: 0.6357
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 42ms/step - accuracy: 0.6174 - loss: 0.6524 - val_accuracy: 0.6407 - val_loss: 0.6354
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 43ms/step - accuracy: 0.6008 - loss: 0.6489 - val_accuracy: 0.6339 - val_loss: 0.6401
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 42ms/step - accuracy: 0.6246 - loss: 0.6363 - val_accuracy: 0.6430 - val_loss: 0.6409
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 44ms/step - accuracy: 0.6075 - loss: 0.6515 - val_accuracy: 0.6316 - val_loss: 0.6402
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 43ms/step - accuracy: 0.6199 - loss: 0.6493 - val_accuracy: 0.6339 - val_loss: 0.6388
Now running, pct_increase: 1.0; days out: 10; independent_array: independent_array15; K-fold: 1
Epoch 1/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">6s</span> 57ms/step - accuracy: 0.5815 - loss: 0.6986 - val_accuracy: 0.6224 - val_loss: 0.6612
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 50ms/step - accuracy: 0.6310 - loss: 0.6637 - val_accuracy: 0.6224 - val_loss: 0.6590
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 51ms/step - accuracy: 0.6299 - loss: 0.6563 - val_accuracy: 0.6087 - val_loss: 0.6686
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 50ms/step - accuracy: 0.6116 - loss: 0.6708 - val_accuracy: 0.6224 - val_loss: 0.6652
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 51ms/step - accuracy: 0.6076 - loss: 0.6710 - val_accuracy: 0.6224 - val_loss: 0.6640
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 52ms/step - accuracy: 0.6364 - loss: 0.6571 - val_accuracy: 0.6201 - val_loss: 0.6635
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 52ms/step - accuracy: 0.6047 - loss: 0.6690 - val_accuracy: 0.6224 - val_loss: 0.6747
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 53ms/step - accuracy: 0.6291 - loss: 0.6607 - val_accuracy: 0.6224 - val_loss: 0.6642
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 49ms/step - accuracy: 0.6358 - loss: 0.6544 - val_accuracy: 0.6133 - val_loss: 0.6647
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 52ms/step - accuracy: 0.6447 - loss: 0.6536 - val_accuracy: 0.6224 - val_loss: 0.6631
Now running, pct_increase: 1.0; days out: 10; independent_array: independent_array15; K-fold: 2
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 52ms/step - accuracy: 0.6107 - loss: 0.6666 - val_accuracy: 0.6156 - val_loss: 0.6578
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">7s</span> 84ms/step - accuracy: 0.6164 - loss: 0.6640 - val_accuracy: 0.6224 - val_loss: 0.6568
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 85ms/step - accuracy: 0.6321 - loss: 0.6537 - val_accuracy: 0.6270 - val_loss: 0.6593
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 84ms/step - accuracy: 0.6177 - loss: 0.6654 - val_accuracy: 0.6224 - val_loss: 0.6550
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">6s</span> 90ms/step - accuracy: 0.6250 - loss: 0.6615 - val_accuracy: 0.6270 - val_loss: 0.6532
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 68ms/step - accuracy: 0.6523 - loss: 0.6405 - val_accuracy: 0.6270 - val_loss: 0.6600
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">6s</span> 90ms/step - accuracy: 0.6399 - loss: 0.6537 - val_accuracy: 0.6156 - val_loss: 0.6572
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 89ms/step - accuracy: 0.6405 - loss: 0.6473 - val_accuracy: 0.6224 - val_loss: 0.6575
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 94ms/step - accuracy: 0.6344 - loss: 0.6522 - val_accuracy: 0.6201 - val_loss: 0.6567
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 88ms/step - accuracy: 0.6316 - loss: 0.6530 - val_accuracy: 0.6384 - val_loss: 0.6514
Now running, pct_increase: 1.0; days out: 10; independent_array: independent_array15; K-fold: 3
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 90ms/step - accuracy: 0.6234 - loss: 0.6524 - val_accuracy: 0.6247 - val_loss: 0.6469
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 86ms/step - accuracy: 0.6231 - loss: 0.6495 - val_accuracy: 0.6339 - val_loss: 0.6436
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 70ms/step - accuracy: 0.6415 - loss: 0.6354 - val_accuracy: 0.6430 - val_loss: 0.6409
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 56ms/step - accuracy: 0.6479 - loss: 0.6339 - val_accuracy: 0.6270 - val_loss: 0.6495
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">6s</span> 73ms/step - accuracy: 0.6480 - loss: 0.6392 - val_accuracy: 0.6430 - val_loss: 0.6516
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 65ms/step - accuracy: 0.6366 - loss: 0.6476 - val_accuracy: 0.6384 - val_loss: 0.6430
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 66ms/step - accuracy: 0.6467 - loss: 0.6387 - val_accuracy: 0.6362 - val_loss: 0.6454
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 65ms/step - accuracy: 0.6170 - loss: 0.6488 - val_accuracy: 0.6293 - val_loss: 0.6492
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 64ms/step - accuracy: 0.6339 - loss: 0.6319 - val_accuracy: 0.6430 - val_loss: 0.6377
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 63ms/step - accuracy: 0.6345 - loss: 0.6354 - val_accuracy: 0.6178 - val_loss: 0.6499
Now running, pct_increase: 1.0; days out: 10; independent_array: independent_array15; K-fold: 4
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 67ms/step - accuracy: 0.6581 - loss: 0.6307 - val_accuracy: 0.6339 - val_loss: 0.6312
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 67ms/step - accuracy: 0.6390 - loss: 0.6295 - val_accuracy: 0.6499 - val_loss: 0.6180
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 64ms/step - accuracy: 0.6548 - loss: 0.6193 - val_accuracy: 0.6453 - val_loss: 0.6293
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 64ms/step - accuracy: 0.6568 - loss: 0.6169 - val_accuracy: 0.6430 - val_loss: 0.6180
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 45ms/step - accuracy: 0.6493 - loss: 0.6219 - val_accuracy: 0.6613 - val_loss: 0.6083
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 43ms/step - accuracy: 0.6547 - loss: 0.6166 - val_accuracy: 0.6453 - val_loss: 0.6221
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 40ms/step - accuracy: 0.6681 - loss: 0.6119 - val_accuracy: 0.6522 - val_loss: 0.6120
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 44ms/step - accuracy: 0.6676 - loss: 0.6105 - val_accuracy: 0.6407 - val_loss: 0.6236
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 41ms/step - accuracy: 0.6649 - loss: 0.6052 - val_accuracy: 0.6522 - val_loss: 0.6250
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 42ms/step - accuracy: 0.6454 - loss: 0.6140 - val_accuracy: 0.6499 - val_loss: 0.6273
Now running, pct_increase: 1.0; days out: 10; independent_array: independent_array15; K-fold: 5
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 42ms/step - accuracy: 0.6782 - loss: 0.5913 - val_accuracy: 0.6911 - val_loss: 0.6015
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 42ms/step - accuracy: 0.6664 - loss: 0.6101 - val_accuracy: 0.6888 - val_loss: 0.6037
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 42ms/step - accuracy: 0.6591 - loss: 0.6082 - val_accuracy: 0.6796 - val_loss: 0.6081
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 42ms/step - accuracy: 0.6572 - loss: 0.6000 - val_accuracy: 0.6911 - val_loss: 0.5951
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 41ms/step - accuracy: 0.6497 - loss: 0.6043 - val_accuracy: 0.6613 - val_loss: 0.6125
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 42ms/step - accuracy: 0.6581 - loss: 0.5974 - val_accuracy: 0.6865 - val_loss: 0.5978
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 44ms/step - accuracy: 0.6666 - loss: 0.5930 - val_accuracy: 0.6865 - val_loss: 0.5960
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 44ms/step - accuracy: 0.6882 - loss: 0.5791 - val_accuracy: 0.6728 - val_loss: 0.6231
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 43ms/step - accuracy: 0.6553 - loss: 0.5856 - val_accuracy: 0.6659 - val_loss: 0.6010
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 45ms/step - accuracy: 0.6695 - loss: 0.5855 - val_accuracy: 0.6728 - val_loss: 0.6109
Now running, pct_increase: 1.0; days out: 15; independent_array: independent_array15; K-fold: 1
Epoch 1/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">6s</span> 55ms/step - accuracy: 0.6381 - loss: 0.6637 - val_accuracy: 0.6362 - val_loss: 0.6484
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 49ms/step - accuracy: 0.6494 - loss: 0.6584 - val_accuracy: 0.6362 - val_loss: 0.6449
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 50ms/step - accuracy: 0.6283 - loss: 0.6575 - val_accuracy: 0.6362 - val_loss: 0.6496
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 50ms/step - accuracy: 0.6382 - loss: 0.6529 - val_accuracy: 0.6362 - val_loss: 0.6407
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 49ms/step - accuracy: 0.6261 - loss: 0.6523 - val_accuracy: 0.6362 - val_loss: 0.6644
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 50ms/step - accuracy: 0.6541 - loss: 0.6382 - val_accuracy: 0.6384 - val_loss: 0.6728
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 51ms/step - accuracy: 0.6254 - loss: 0.6579 - val_accuracy: 0.6362 - val_loss: 0.6401
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 51ms/step - accuracy: 0.6267 - loss: 0.6519 - val_accuracy: 0.6362 - val_loss: 0.6437
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 52ms/step - accuracy: 0.6524 - loss: 0.6336 - val_accuracy: 0.6362 - val_loss: 0.6319
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 51ms/step - accuracy: 0.6327 - loss: 0.6396 - val_accuracy: 0.6384 - val_loss: 0.6493
Now running, pct_increase: 1.0; days out: 15; independent_array: independent_array15; K-fold: 2
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 52ms/step - accuracy: 0.6399 - loss: 0.6459 - val_accuracy: 0.6362 - val_loss: 0.6504
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 52ms/step - accuracy: 0.6493 - loss: 0.6367 - val_accuracy: 0.6384 - val_loss: 0.6476
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">6s</span> 68ms/step - accuracy: 0.6438 - loss: 0.6356 - val_accuracy: 0.6430 - val_loss: 0.6360
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">6s</span> 91ms/step - accuracy: 0.6347 - loss: 0.6374 - val_accuracy: 0.6407 - val_loss: 0.6472
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 89ms/step - accuracy: 0.6654 - loss: 0.6312 - val_accuracy: 0.6407 - val_loss: 0.6393
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 88ms/step - accuracy: 0.6469 - loss: 0.6253 - val_accuracy: 0.6476 - val_loss: 0.6327
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 93ms/step - accuracy: 0.6315 - loss: 0.6214 - val_accuracy: 0.6362 - val_loss: 0.6404
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">10s</span> 95ms/step - accuracy: 0.6246 - loss: 0.6432 - val_accuracy: 0.6407 - val_loss: 0.6292
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">10s</span> 93ms/step - accuracy: 0.6443 - loss: 0.6271 - val_accuracy: 0.6339 - val_loss: 0.6218
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 88ms/step - accuracy: 0.6311 - loss: 0.6377 - val_accuracy: 0.6545 - val_loss: 0.6256
Now running, pct_increase: 1.0; days out: 15; independent_array: independent_array15; K-fold: 3
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 80ms/step - accuracy: 0.6344 - loss: 0.6365 - val_accuracy: 0.6522 - val_loss: 0.6136
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 88ms/step - accuracy: 0.6420 - loss: 0.6385 - val_accuracy: 0.6453 - val_loss: 0.6132
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 58ms/step - accuracy: 0.6344 - loss: 0.6271 - val_accuracy: 0.6384 - val_loss: 0.6238
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 59ms/step - accuracy: 0.6537 - loss: 0.6262 - val_accuracy: 0.6339 - val_loss: 0.6245
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 68ms/step - accuracy: 0.6352 - loss: 0.6322 - val_accuracy: 0.6407 - val_loss: 0.6099
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 58ms/step - accuracy: 0.6537 - loss: 0.6219 - val_accuracy: 0.6407 - val_loss: 0.6019
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">6s</span> 68ms/step - accuracy: 0.6561 - loss: 0.6218 - val_accuracy: 0.6476 - val_loss: 0.5998
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 59ms/step - accuracy: 0.6501 - loss: 0.6090 - val_accuracy: 0.6384 - val_loss: 0.6054
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 59ms/step - accuracy: 0.6885 - loss: 0.5969 - val_accuracy: 0.6156 - val_loss: 0.6125
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 69ms/step - accuracy: 0.6480 - loss: 0.6047 - val_accuracy: 0.6476 - val_loss: 0.6220
Now running, pct_increase: 1.0; days out: 15; independent_array: independent_array15; K-fold: 4
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 60ms/step - accuracy: 0.6707 - loss: 0.6044 - val_accuracy: 0.6568 - val_loss: 0.6025
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">6s</span> 67ms/step - accuracy: 0.6579 - loss: 0.6121 - val_accuracy: 0.6613 - val_loss: 0.5930
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 67ms/step - accuracy: 0.6508 - loss: 0.5973 - val_accuracy: 0.6682 - val_loss: 0.5954
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 69ms/step - accuracy: 0.6608 - loss: 0.6033 - val_accuracy: 0.6568 - val_loss: 0.6045
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 51ms/step - accuracy: 0.6916 - loss: 0.5710 - val_accuracy: 0.6568 - val_loss: 0.6047
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 43ms/step - accuracy: 0.6814 - loss: 0.5852 - val_accuracy: 0.6545 - val_loss: 0.6063
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 45ms/step - accuracy: 0.6678 - loss: 0.5943 - val_accuracy: 0.6613 - val_loss: 0.5972
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 45ms/step - accuracy: 0.6790 - loss: 0.5760 - val_accuracy: 0.6613 - val_loss: 0.5905
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 43ms/step - accuracy: 0.6750 - loss: 0.5851 - val_accuracy: 0.6362 - val_loss: 0.6191
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 44ms/step - accuracy: 0.6501 - loss: 0.6002 - val_accuracy: 0.6568 - val_loss: 0.6178
Now running, pct_increase: 1.0; days out: 15; independent_array: independent_array15; K-fold: 5
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 45ms/step - accuracy: 0.6573 - loss: 0.5968 - val_accuracy: 0.6934 - val_loss: 0.5876
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 45ms/step - accuracy: 0.6708 - loss: 0.5634 - val_accuracy: 0.6659 - val_loss: 0.6177
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 42ms/step - accuracy: 0.6720 - loss: 0.5786 - val_accuracy: 0.6705 - val_loss: 0.6017
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 43ms/step - accuracy: 0.6744 - loss: 0.5715 - val_accuracy: 0.6865 - val_loss: 0.6003
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 42ms/step - accuracy: 0.6811 - loss: 0.5725 - val_accuracy: 0.6796 - val_loss: 0.6172
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 41ms/step - accuracy: 0.6710 - loss: 0.5695 - val_accuracy: 0.6728 - val_loss: 0.6302
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 43ms/step - accuracy: 0.6442 - loss: 0.5953 - val_accuracy: 0.6842 - val_loss: 0.6104
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 41ms/step - accuracy: 0.6962 - loss: 0.5663 - val_accuracy: 0.6705 - val_loss: 0.5988
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 41ms/step - accuracy: 0.6953 - loss: 0.5494 - val_accuracy: 0.6705 - val_loss: 0.6160
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 45ms/step - accuracy: 0.6741 - loss: 0.5624 - val_accuracy: 0.6156 - val_loss: 0.6329
Now running, pct_increase: 1.01; days out: 1; independent_array: independent_array15; K-fold: 1
Epoch 1/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 50ms/step - accuracy: 0.7796 - loss: 0.4872 - val_accuracy: 0.8539 - val_loss: 0.4109
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 50ms/step - accuracy: 0.8573 - loss: 0.4031 - val_accuracy: 0.8539 - val_loss: 0.3968
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 50ms/step - accuracy: 0.8617 - loss: 0.3908 - val_accuracy: 0.8562 - val_loss: 0.3964
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 50ms/step - accuracy: 0.8634 - loss: 0.3899 - val_accuracy: 0.8562 - val_loss: 0.3921
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 52ms/step - accuracy: 0.8611 - loss: 0.3836 - val_accuracy: 0.8539 - val_loss: 0.4006
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 49ms/step - accuracy: 0.8469 - loss: 0.4110 - val_accuracy: 0.8539 - val_loss: 0.3916
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 50ms/step - accuracy: 0.8582 - loss: 0.3873 - val_accuracy: 0.8539 - val_loss: 0.3956
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 52ms/step - accuracy: 0.8432 - loss: 0.4152 - val_accuracy: 0.8539 - val_loss: 0.3906
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 50ms/step - accuracy: 0.8606 - loss: 0.3883 - val_accuracy: 0.8493 - val_loss: 0.3987
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 49ms/step - accuracy: 0.8475 - loss: 0.4062 - val_accuracy: 0.8493 - val_loss: 0.3874
Now running, pct_increase: 1.01; days out: 1; independent_array: independent_array15; K-fold: 2
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 51ms/step - accuracy: 0.8409 - loss: 0.4186 - val_accuracy: 0.8539 - val_loss: 0.4043
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 52ms/step - accuracy: 0.8515 - loss: 0.3949 - val_accuracy: 0.8562 - val_loss: 0.3987
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 50ms/step - accuracy: 0.8540 - loss: 0.3940 - val_accuracy: 0.8539 - val_loss: 0.4054
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 49ms/step - accuracy: 0.8596 - loss: 0.3876 - val_accuracy: 0.8539 - val_loss: 0.4045
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">8s</span> 94ms/step - accuracy: 0.8507 - loss: 0.3990 - val_accuracy: 0.8539 - val_loss: 0.4153
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 79ms/step - accuracy: 0.8523 - loss: 0.4061 - val_accuracy: 0.8562 - val_loss: 0.4140
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">6s</span> 90ms/step - accuracy: 0.8474 - loss: 0.3854 - val_accuracy: 0.8539 - val_loss: 0.4052
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 92ms/step - accuracy: 0.8660 - loss: 0.3609 - val_accuracy: 0.8562 - val_loss: 0.4048
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 89ms/step - accuracy: 0.8562 - loss: 0.3778 - val_accuracy: 0.8562 - val_loss: 0.4027
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 90ms/step - accuracy: 0.8502 - loss: 0.3841 - val_accuracy: 0.8539 - val_loss: 0.4000
Now running, pct_increase: 1.01; days out: 1; independent_array: independent_array15; K-fold: 3
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 93ms/step - accuracy: 0.8519 - loss: 0.3843 - val_accuracy: 0.8539 - val_loss: 0.3908
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 98ms/step - accuracy: 0.8586 - loss: 0.3800 - val_accuracy: 0.8539 - val_loss: 0.3888
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">10s</span> 92ms/step - accuracy: 0.8675 - loss: 0.3677 - val_accuracy: 0.8539 - val_loss: 0.3895
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 85ms/step - accuracy: 0.8618 - loss: 0.3765 - val_accuracy: 0.8539 - val_loss: 0.3889
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 74ms/step - accuracy: 0.8540 - loss: 0.3786 - val_accuracy: 0.8493 - val_loss: 0.3930
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 67ms/step - accuracy: 0.8620 - loss: 0.3728 - val_accuracy: 0.8539 - val_loss: 0.3905
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 68ms/step - accuracy: 0.8483 - loss: 0.3857 - val_accuracy: 0.8516 - val_loss: 0.3918
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 67ms/step - accuracy: 0.8511 - loss: 0.3955 - val_accuracy: 0.8562 - val_loss: 0.3922
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 66ms/step - accuracy: 0.8679 - loss: 0.3701 - val_accuracy: 0.8470 - val_loss: 0.3996
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 59ms/step - accuracy: 0.8516 - loss: 0.3777 - val_accuracy: 0.8539 - val_loss: 0.3915
Now running, pct_increase: 1.01; days out: 1; independent_array: independent_array15; K-fold: 4
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 56ms/step - accuracy: 0.8654 - loss: 0.3711 - val_accuracy: 0.8584 - val_loss: 0.3618
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">6s</span> 68ms/step - accuracy: 0.8450 - loss: 0.4180 - val_accuracy: 0.8630 - val_loss: 0.3634
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 68ms/step - accuracy: 0.8479 - loss: 0.4061 - val_accuracy: 0.8607 - val_loss: 0.3654
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 63ms/step - accuracy: 0.8568 - loss: 0.3749 - val_accuracy: 0.8584 - val_loss: 0.3691
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 68ms/step - accuracy: 0.8555 - loss: 0.3871 - val_accuracy: 0.8607 - val_loss: 0.3673
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 61ms/step - accuracy: 0.8572 - loss: 0.3733 - val_accuracy: 0.8584 - val_loss: 0.3709
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 53ms/step - accuracy: 0.8486 - loss: 0.3912 - val_accuracy: 0.8607 - val_loss: 0.3695
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 45ms/step - accuracy: 0.8473 - loss: 0.4044 - val_accuracy: 0.8607 - val_loss: 0.3679
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 41ms/step - accuracy: 0.8440 - loss: 0.3880 - val_accuracy: 0.8584 - val_loss: 0.3752
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 43ms/step - accuracy: 0.8672 - loss: 0.3705 - val_accuracy: 0.8562 - val_loss: 0.3712
Now running, pct_increase: 1.01; days out: 1; independent_array: independent_array15; K-fold: 5
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 41ms/step - accuracy: 0.8552 - loss: 0.3800 - val_accuracy: 0.8535 - val_loss: 0.3890
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 44ms/step - accuracy: 0.8643 - loss: 0.3660 - val_accuracy: 0.8558 - val_loss: 0.3834
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 42ms/step - accuracy: 0.8674 - loss: 0.3632 - val_accuracy: 0.8513 - val_loss: 0.3844
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 46ms/step - accuracy: 0.8599 - loss: 0.3664 - val_accuracy: 0.8513 - val_loss: 0.3879
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 43ms/step - accuracy: 0.8606 - loss: 0.3718 - val_accuracy: 0.8604 - val_loss: 0.3854
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 39ms/step - accuracy: 0.8587 - loss: 0.3852 - val_accuracy: 0.8604 - val_loss: 0.3862
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 43ms/step - accuracy: 0.8657 - loss: 0.3616 - val_accuracy: 0.8535 - val_loss: 0.3900
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 42ms/step - accuracy: 0.8625 - loss: 0.3737 - val_accuracy: 0.8513 - val_loss: 0.3931
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 43ms/step - accuracy: 0.8485 - loss: 0.3843 - val_accuracy: 0.8535 - val_loss: 0.3992
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 39ms/step - accuracy: 0.8523 - loss: 0.3814 - val_accuracy: 0.8490 - val_loss: 0.3978
Now running, pct_increase: 1.01; days out: 3; independent_array: independent_array15; K-fold: 1
Epoch 1/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 49ms/step - accuracy: 0.7132 - loss: 0.6077 - val_accuracy: 0.7215 - val_loss: 0.5856
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 45ms/step - accuracy: 0.7120 - loss: 0.5978 - val_accuracy: 0.7260 - val_loss: 0.5850
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 43ms/step - accuracy: 0.7046 - loss: 0.6058 - val_accuracy: 0.7215 - val_loss: 0.5827
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 47ms/step - accuracy: 0.7100 - loss: 0.6011 - val_accuracy: 0.7215 - val_loss: 0.5847
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 49ms/step - accuracy: 0.7077 - loss: 0.6101 - val_accuracy: 0.7215 - val_loss: 0.5834
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 48ms/step - accuracy: 0.7219 - loss: 0.5840 - val_accuracy: 0.7215 - val_loss: 0.5826
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 51ms/step - accuracy: 0.7215 - loss: 0.5781 - val_accuracy: 0.7215 - val_loss: 0.5832
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 52ms/step - accuracy: 0.7272 - loss: 0.5763 - val_accuracy: 0.7215 - val_loss: 0.5835
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 50ms/step - accuracy: 0.7365 - loss: 0.5712 - val_accuracy: 0.7215 - val_loss: 0.5828
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 49ms/step - accuracy: 0.7150 - loss: 0.5940 - val_accuracy: 0.7215 - val_loss: 0.5822
Now running, pct_increase: 1.01; days out: 3; independent_array: independent_array15; K-fold: 2
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 50ms/step - accuracy: 0.7107 - loss: 0.5892 - val_accuracy: 0.7215 - val_loss: 0.5781
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 51ms/step - accuracy: 0.7190 - loss: 0.5877 - val_accuracy: 0.7237 - val_loss: 0.5788
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 53ms/step - accuracy: 0.7287 - loss: 0.5712 - val_accuracy: 0.7215 - val_loss: 0.5860
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 50ms/step - accuracy: 0.7220 - loss: 0.5789 - val_accuracy: 0.7215 - val_loss: 0.5810
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 51ms/step - accuracy: 0.7200 - loss: 0.5887 - val_accuracy: 0.7237 - val_loss: 0.5816
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 51ms/step - accuracy: 0.7274 - loss: 0.5788 - val_accuracy: 0.7215 - val_loss: 0.5830
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">7s</span> 86ms/step - accuracy: 0.7218 - loss: 0.5731 - val_accuracy: 0.7215 - val_loss: 0.5830
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 89ms/step - accuracy: 0.7215 - loss: 0.5792 - val_accuracy: 0.7237 - val_loss: 0.5795
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">6s</span> 101ms/step - accuracy: 0.7221 - loss: 0.5740 - val_accuracy: 0.7237 - val_loss: 0.5779
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">10s</span> 90ms/step - accuracy: 0.7296 - loss: 0.5660 - val_accuracy: 0.7215 - val_loss: 0.5803
Now running, pct_increase: 1.01; days out: 3; independent_array: independent_array15; K-fold: 3
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 91ms/step - accuracy: 0.7398 - loss: 0.5520 - val_accuracy: 0.7208 - val_loss: 0.5784
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 82ms/step - accuracy: 0.7295 - loss: 0.5675 - val_accuracy: 0.7185 - val_loss: 0.5878
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">6s</span> 94ms/step - accuracy: 0.7312 - loss: 0.5686 - val_accuracy: 0.7185 - val_loss: 0.5812
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">10s</span> 85ms/step - accuracy: 0.7224 - loss: 0.5749 - val_accuracy: 0.7208 - val_loss: 0.5897
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">6s</span> 93ms/step - accuracy: 0.7288 - loss: 0.5654 - val_accuracy: 0.7208 - val_loss: 0.5819
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 85ms/step - accuracy: 0.7251 - loss: 0.5690 - val_accuracy: 0.6979 - val_loss: 0.6014
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 58ms/step - accuracy: 0.7229 - loss: 0.5703 - val_accuracy: 0.7208 - val_loss: 0.5871
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">6s</span> 66ms/step - accuracy: 0.7265 - loss: 0.5642 - val_accuracy: 0.7002 - val_loss: 0.6039
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 64ms/step - accuracy: 0.7271 - loss: 0.5708 - val_accuracy: 0.6957 - val_loss: 0.5982
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 66ms/step - accuracy: 0.7199 - loss: 0.5641 - val_accuracy: 0.7117 - val_loss: 0.6007
Now running, pct_increase: 1.01; days out: 3; independent_array: independent_array15; K-fold: 4
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 66ms/step - accuracy: 0.7062 - loss: 0.5867 - val_accuracy: 0.7208 - val_loss: 0.5605
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 68ms/step - accuracy: 0.7265 - loss: 0.5705 - val_accuracy: 0.7368 - val_loss: 0.5472
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 68ms/step - accuracy: 0.7100 - loss: 0.5830 - val_accuracy: 0.7391 - val_loss: 0.5407
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 60ms/step - accuracy: 0.7304 - loss: 0.5564 - val_accuracy: 0.7368 - val_loss: 0.5503
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 65ms/step - accuracy: 0.7442 - loss: 0.5544 - val_accuracy: 0.7391 - val_loss: 0.5510
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 63ms/step - accuracy: 0.7255 - loss: 0.5633 - val_accuracy: 0.7414 - val_loss: 0.5485
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 60ms/step - accuracy: 0.7203 - loss: 0.5730 - val_accuracy: 0.7437 - val_loss: 0.5503
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 51ms/step - accuracy: 0.7415 - loss: 0.5530 - val_accuracy: 0.7368 - val_loss: 0.5520
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 41ms/step - accuracy: 0.7353 - loss: 0.5599 - val_accuracy: 0.7208 - val_loss: 0.5619
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 43ms/step - accuracy: 0.7272 - loss: 0.5676 - val_accuracy: 0.7323 - val_loss: 0.5765
Now running, pct_increase: 1.01; days out: 3; independent_array: independent_array15; K-fold: 5
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 45ms/step - accuracy: 0.7090 - loss: 0.6003 - val_accuracy: 0.7277 - val_loss: 0.5534
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 45ms/step - accuracy: 0.7104 - loss: 0.5821 - val_accuracy: 0.7208 - val_loss: 0.5585
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 45ms/step - accuracy: 0.7320 - loss: 0.5624 - val_accuracy: 0.7185 - val_loss: 0.5675
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 42ms/step - accuracy: 0.7425 - loss: 0.5435 - val_accuracy: 0.7117 - val_loss: 0.5711
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 43ms/step - accuracy: 0.7353 - loss: 0.5384 - val_accuracy: 0.7140 - val_loss: 0.5671
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 44ms/step - accuracy: 0.7456 - loss: 0.5409 - val_accuracy: 0.7094 - val_loss: 0.5685
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 45ms/step - accuracy: 0.7428 - loss: 0.5487 - val_accuracy: 0.6865 - val_loss: 0.5967
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 44ms/step - accuracy: 0.7578 - loss: 0.5291 - val_accuracy: 0.7094 - val_loss: 0.5816
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 42ms/step - accuracy: 0.7482 - loss: 0.5298 - val_accuracy: 0.7162 - val_loss: 0.5612
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 45ms/step - accuracy: 0.7366 - loss: 0.5313 - val_accuracy: 0.6934 - val_loss: 0.6011
Now running, pct_increase: 1.01; days out: 5; independent_array: independent_array15; K-fold: 1
Epoch 1/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 49ms/step - accuracy: 0.6005 - loss: 0.6756 - val_accuracy: 0.6370 - val_loss: 0.6490
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 44ms/step - accuracy: 0.6172 - loss: 0.6624 - val_accuracy: 0.6393 - val_loss: 0.6546
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 44ms/step - accuracy: 0.6254 - loss: 0.6625 - val_accuracy: 0.6393 - val_loss: 0.6514
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 46ms/step - accuracy: 0.6432 - loss: 0.6473 - val_accuracy: 0.6370 - val_loss: 0.6465
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 48ms/step - accuracy: 0.6174 - loss: 0.6613 - val_accuracy: 0.6393 - val_loss: 0.6550
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 50ms/step - accuracy: 0.6498 - loss: 0.6416 - val_accuracy: 0.6393 - val_loss: 0.6472
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 48ms/step - accuracy: 0.6372 - loss: 0.6490 - val_accuracy: 0.6393 - val_loss: 0.6509
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 49ms/step - accuracy: 0.6512 - loss: 0.6413 - val_accuracy: 0.6370 - val_loss: 0.6486
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 51ms/step - accuracy: 0.6442 - loss: 0.6470 - val_accuracy: 0.6393 - val_loss: 0.6511
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 51ms/step - accuracy: 0.6463 - loss: 0.6441 - val_accuracy: 0.6370 - val_loss: 0.6504
Now running, pct_increase: 1.01; days out: 5; independent_array: independent_array15; K-fold: 2
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 49ms/step - accuracy: 0.6395 - loss: 0.6480 - val_accuracy: 0.6270 - val_loss: 0.6584
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 52ms/step - accuracy: 0.6431 - loss: 0.6453 - val_accuracy: 0.6339 - val_loss: 0.6501
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 52ms/step - accuracy: 0.6574 - loss: 0.6428 - val_accuracy: 0.6384 - val_loss: 0.6567
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 49ms/step - accuracy: 0.6605 - loss: 0.6373 - val_accuracy: 0.6178 - val_loss: 0.6523
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 52ms/step - accuracy: 0.6467 - loss: 0.6456 - val_accuracy: 0.6156 - val_loss: 0.6546
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 50ms/step - accuracy: 0.6492 - loss: 0.6375 - val_accuracy: 0.6270 - val_loss: 0.6555
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 56ms/step - accuracy: 0.6590 - loss: 0.6399 - val_accuracy: 0.6110 - val_loss: 0.6637
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">6s</span> 78ms/step - accuracy: 0.6472 - loss: 0.6453 - val_accuracy: 0.6201 - val_loss: 0.6662
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 82ms/step - accuracy: 0.6385 - loss: 0.6521 - val_accuracy: 0.6224 - val_loss: 0.6593
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 86ms/step - accuracy: 0.6575 - loss: 0.6416 - val_accuracy: 0.6407 - val_loss: 0.6497
Now running, pct_increase: 1.01; days out: 5; independent_array: independent_array15; K-fold: 3
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 84ms/step - accuracy: 0.6437 - loss: 0.6477 - val_accuracy: 0.6430 - val_loss: 0.6415
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 79ms/step - accuracy: 0.6794 - loss: 0.6241 - val_accuracy: 0.6728 - val_loss: 0.6440
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 80ms/step - accuracy: 0.6483 - loss: 0.6401 - val_accuracy: 0.6293 - val_loss: 0.6448
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 75ms/step - accuracy: 0.6526 - loss: 0.6428 - val_accuracy: 0.6590 - val_loss: 0.6415
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">6s</span> 84ms/step - accuracy: 0.6619 - loss: 0.6366 - val_accuracy: 0.6476 - val_loss: 0.6425
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 81ms/step - accuracy: 0.6568 - loss: 0.6332 - val_accuracy: 0.6590 - val_loss: 0.6423
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 81ms/step - accuracy: 0.6499 - loss: 0.6381 - val_accuracy: 0.6384 - val_loss: 0.6493
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 76ms/step - accuracy: 0.6372 - loss: 0.6472 - val_accuracy: 0.6522 - val_loss: 0.6476
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 74ms/step - accuracy: 0.6597 - loss: 0.6368 - val_accuracy: 0.6407 - val_loss: 0.6588
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 58ms/step - accuracy: 0.6514 - loss: 0.6389 - val_accuracy: 0.6568 - val_loss: 0.6487
Now running, pct_increase: 1.01; days out: 5; independent_array: independent_array15; K-fold: 4
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 66ms/step - accuracy: 0.6469 - loss: 0.6425 - val_accuracy: 0.7025 - val_loss: 0.6049
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 62ms/step - accuracy: 0.6459 - loss: 0.6425 - val_accuracy: 0.6796 - val_loss: 0.6146
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 60ms/step - accuracy: 0.6535 - loss: 0.6377 - val_accuracy: 0.6613 - val_loss: 0.6355
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 64ms/step - accuracy: 0.6739 - loss: 0.6217 - val_accuracy: 0.6728 - val_loss: 0.6186
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 57ms/step - accuracy: 0.6562 - loss: 0.6363 - val_accuracy: 0.6659 - val_loss: 0.6269
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 59ms/step - accuracy: 0.6703 - loss: 0.6263 - val_accuracy: 0.6613 - val_loss: 0.6364
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 60ms/step - accuracy: 0.6797 - loss: 0.6163 - val_accuracy: 0.6957 - val_loss: 0.6183
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 64ms/step - accuracy: 0.6799 - loss: 0.6169 - val_accuracy: 0.6979 - val_loss: 0.6154
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 62ms/step - accuracy: 0.6534 - loss: 0.6228 - val_accuracy: 0.6613 - val_loss: 0.6243
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 64ms/step - accuracy: 0.6627 - loss: 0.6260 - val_accuracy: 0.6682 - val_loss: 0.6230
Now running, pct_increase: 1.01; days out: 5; independent_array: independent_array15; K-fold: 5
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 55ms/step - accuracy: 0.6791 - loss: 0.6186 - val_accuracy: 0.6659 - val_loss: 0.6274
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 43ms/step - accuracy: 0.6839 - loss: 0.6036 - val_accuracy: 0.6545 - val_loss: 0.6264
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 39ms/step - accuracy: 0.6603 - loss: 0.6188 - val_accuracy: 0.6636 - val_loss: 0.6263
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 40ms/step - accuracy: 0.6764 - loss: 0.6087 - val_accuracy: 0.6453 - val_loss: 0.6466
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 42ms/step - accuracy: 0.6859 - loss: 0.6044 - val_accuracy: 0.6705 - val_loss: 0.6254
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 41ms/step - accuracy: 0.6749 - loss: 0.6080 - val_accuracy: 0.6453 - val_loss: 0.6362
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 46ms/step - accuracy: 0.6777 - loss: 0.6007 - val_accuracy: 0.6590 - val_loss: 0.6401
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 43ms/step - accuracy: 0.6783 - loss: 0.6071 - val_accuracy: 0.6613 - val_loss: 0.6222
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 42ms/step - accuracy: 0.6824 - loss: 0.5983 - val_accuracy: 0.6568 - val_loss: 0.6310
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 44ms/step - accuracy: 0.6995 - loss: 0.5878 - val_accuracy: 0.6522 - val_loss: 0.6336
Now running, pct_increase: 1.01; days out: 10; independent_array: independent_array15; K-fold: 1
Epoch 1/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 49ms/step - accuracy: 0.5340 - loss: 0.7088 - val_accuracy: 0.5515 - val_loss: 0.6919
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 42ms/step - accuracy: 0.5699 - loss: 0.6843 - val_accuracy: 0.5057 - val_loss: 0.6932
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 45ms/step - accuracy: 0.5442 - loss: 0.6878 - val_accuracy: 0.4943 - val_loss: 0.6917
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 45ms/step - accuracy: 0.5501 - loss: 0.6863 - val_accuracy: 0.5080 - val_loss: 0.6974
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 44ms/step - accuracy: 0.5396 - loss: 0.6860 - val_accuracy: 0.5011 - val_loss: 0.6994
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 41ms/step - accuracy: 0.5438 - loss: 0.6911 - val_accuracy: 0.5126 - val_loss: 0.6951
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 45ms/step - accuracy: 0.5881 - loss: 0.6781 - val_accuracy: 0.5149 - val_loss: 0.6903
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 49ms/step - accuracy: 0.5865 - loss: 0.6763 - val_accuracy: 0.5217 - val_loss: 0.6950
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 49ms/step - accuracy: 0.5679 - loss: 0.6785 - val_accuracy: 0.5149 - val_loss: 0.6908
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 49ms/step - accuracy: 0.5915 - loss: 0.6717 - val_accuracy: 0.5126 - val_loss: 0.6901
Now running, pct_increase: 1.01; days out: 10; independent_array: independent_array15; K-fold: 2
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 51ms/step - accuracy: 0.5992 - loss: 0.6716 - val_accuracy: 0.5767 - val_loss: 0.6673
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 48ms/step - accuracy: 0.5836 - loss: 0.6728 - val_accuracy: 0.5309 - val_loss: 0.6869
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 50ms/step - accuracy: 0.5622 - loss: 0.6802 - val_accuracy: 0.6201 - val_loss: 0.6715
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 47ms/step - accuracy: 0.6056 - loss: 0.6603 - val_accuracy: 0.5927 - val_loss: 0.6822
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 51ms/step - accuracy: 0.5742 - loss: 0.6763 - val_accuracy: 0.5561 - val_loss: 0.6782
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 51ms/step - accuracy: 0.6064 - loss: 0.6630 - val_accuracy: 0.5858 - val_loss: 0.6769
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 50ms/step - accuracy: 0.6074 - loss: 0.6562 - val_accuracy: 0.5927 - val_loss: 0.6706
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 50ms/step - accuracy: 0.6104 - loss: 0.6571 - val_accuracy: 0.6293 - val_loss: 0.6646
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">6s</span> 58ms/step - accuracy: 0.5869 - loss: 0.6646 - val_accuracy: 0.5881 - val_loss: 0.6798
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">6s</span> 80ms/step - accuracy: 0.6276 - loss: 0.6385 - val_accuracy: 0.6110 - val_loss: 0.6698
Now running, pct_increase: 1.01; days out: 10; independent_array: independent_array15; K-fold: 3
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 80ms/step - accuracy: 0.5990 - loss: 0.6574 - val_accuracy: 0.5973 - val_loss: 0.6441
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 63ms/step - accuracy: 0.6217 - loss: 0.6524 - val_accuracy: 0.5973 - val_loss: 0.6534
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 76ms/step - accuracy: 0.6048 - loss: 0.6596 - val_accuracy: 0.6156 - val_loss: 0.6458
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 81ms/step - accuracy: 0.6290 - loss: 0.6439 - val_accuracy: 0.5995 - val_loss: 0.6558
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 83ms/step - accuracy: 0.6387 - loss: 0.6375 - val_accuracy: 0.5835 - val_loss: 0.6592
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 78ms/step - accuracy: 0.6031 - loss: 0.6484 - val_accuracy: 0.6018 - val_loss: 0.6587
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 76ms/step - accuracy: 0.6163 - loss: 0.6546 - val_accuracy: 0.6133 - val_loss: 0.6565
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 78ms/step - accuracy: 0.6346 - loss: 0.6373 - val_accuracy: 0.6087 - val_loss: 0.6547
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 78ms/step - accuracy: 0.6320 - loss: 0.6424 - val_accuracy: 0.6110 - val_loss: 0.6545
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 77ms/step - accuracy: 0.6217 - loss: 0.6329 - val_accuracy: 0.6041 - val_loss: 0.6556
Now running, pct_increase: 1.01; days out: 10; independent_array: independent_array15; K-fold: 4
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 75ms/step - accuracy: 0.5972 - loss: 0.6533 - val_accuracy: 0.6339 - val_loss: 0.6316
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 59ms/step - accuracy: 0.6528 - loss: 0.6266 - val_accuracy: 0.6476 - val_loss: 0.6228
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 57ms/step - accuracy: 0.6444 - loss: 0.6311 - val_accuracy: 0.6201 - val_loss: 0.6400
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">6s</span> 67ms/step - accuracy: 0.6545 - loss: 0.6144 - val_accuracy: 0.6499 - val_loss: 0.6244
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 63ms/step - accuracy: 0.6316 - loss: 0.6387 - val_accuracy: 0.6362 - val_loss: 0.6341
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 68ms/step - accuracy: 0.6394 - loss: 0.6373 - val_accuracy: 0.6636 - val_loss: 0.6199
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 65ms/step - accuracy: 0.6589 - loss: 0.6089 - val_accuracy: 0.6178 - val_loss: 0.6388
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 58ms/step - accuracy: 0.6680 - loss: 0.6077 - val_accuracy: 0.6293 - val_loss: 0.6319
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 63ms/step - accuracy: 0.6781 - loss: 0.6079 - val_accuracy: 0.6476 - val_loss: 0.6245
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 60ms/step - accuracy: 0.6685 - loss: 0.6027 - val_accuracy: 0.6590 - val_loss: 0.6245
Now running, pct_increase: 1.01; days out: 10; independent_array: independent_array15; K-fold: 5
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 65ms/step - accuracy: 0.6570 - loss: 0.6216 - val_accuracy: 0.6476 - val_loss: 0.6126
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 60ms/step - accuracy: 0.6422 - loss: 0.6118 - val_accuracy: 0.6545 - val_loss: 0.6178
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 62ms/step - accuracy: 0.6676 - loss: 0.6126 - val_accuracy: 0.6636 - val_loss: 0.6070
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 55ms/step - accuracy: 0.6763 - loss: 0.5877 - val_accuracy: 0.5973 - val_loss: 0.6573
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 42ms/step - accuracy: 0.6418 - loss: 0.6030 - val_accuracy: 0.6407 - val_loss: 0.6275
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 41ms/step - accuracy: 0.6819 - loss: 0.5831 - val_accuracy: 0.6453 - val_loss: 0.6133
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 40ms/step - accuracy: 0.6763 - loss: 0.5745 - val_accuracy: 0.6430 - val_loss: 0.6194
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 40ms/step - accuracy: 0.6945 - loss: 0.5675 - val_accuracy: 0.6178 - val_loss: 0.6371
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 45ms/step - accuracy: 0.6683 - loss: 0.5832 - val_accuracy: 0.6407 - val_loss: 0.6248
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 40ms/step - accuracy: 0.6898 - loss: 0.5464 - val_accuracy: 0.6728 - val_loss: 0.6038
Now running, pct_increase: 1.01; days out: 15; independent_array: independent_array15; K-fold: 1
Epoch 1/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 48ms/step - accuracy: 0.5054 - loss: 0.7049 - val_accuracy: 0.5172 - val_loss: 0.6907
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 42ms/step - accuracy: 0.4985 - loss: 0.6967 - val_accuracy: 0.5080 - val_loss: 0.6913
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 43ms/step - accuracy: 0.5082 - loss: 0.6971 - val_accuracy: 0.5263 - val_loss: 0.6885
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 44ms/step - accuracy: 0.5215 - loss: 0.6908 - val_accuracy: 0.5400 - val_loss: 0.6886
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 44ms/step - accuracy: 0.5394 - loss: 0.6863 - val_accuracy: 0.5149 - val_loss: 0.6922
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 41ms/step - accuracy: 0.5293 - loss: 0.6925 - val_accuracy: 0.5584 - val_loss: 0.6885
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 42ms/step - accuracy: 0.5110 - loss: 0.6903 - val_accuracy: 0.5103 - val_loss: 0.6869
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 45ms/step - accuracy: 0.5376 - loss: 0.6805 - val_accuracy: 0.5309 - val_loss: 0.6844
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 45ms/step - accuracy: 0.5259 - loss: 0.6809 - val_accuracy: 0.5309 - val_loss: 0.6857
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 45ms/step - accuracy: 0.5545 - loss: 0.6788 - val_accuracy: 0.5698 - val_loss: 0.6789
Now running, pct_increase: 1.01; days out: 15; independent_array: independent_array15; K-fold: 2
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 46ms/step - accuracy: 0.5609 - loss: 0.6767 - val_accuracy: 0.5858 - val_loss: 0.6650
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 51ms/step - accuracy: 0.5376 - loss: 0.6753 - val_accuracy: 0.5950 - val_loss: 0.6679
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 50ms/step - accuracy: 0.5688 - loss: 0.6723 - val_accuracy: 0.5423 - val_loss: 0.6786
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 50ms/step - accuracy: 0.5626 - loss: 0.6694 - val_accuracy: 0.5789 - val_loss: 0.6696
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 50ms/step - accuracy: 0.5500 - loss: 0.6746 - val_accuracy: 0.5789 - val_loss: 0.6562
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 51ms/step - accuracy: 0.5923 - loss: 0.6653 - val_accuracy: 0.5675 - val_loss: 0.6754
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 49ms/step - accuracy: 0.5740 - loss: 0.6620 - val_accuracy: 0.6018 - val_loss: 0.6651
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 50ms/step - accuracy: 0.5858 - loss: 0.6574 - val_accuracy: 0.5309 - val_loss: 0.6912
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 50ms/step - accuracy: 0.5299 - loss: 0.6715 - val_accuracy: 0.6018 - val_loss: 0.6704
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 50ms/step - accuracy: 0.5651 - loss: 0.6626 - val_accuracy: 0.5629 - val_loss: 0.6701
Now running, pct_increase: 1.01; days out: 15; independent_array: independent_array15; K-fold: 3
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 50ms/step - accuracy: 0.5687 - loss: 0.6610 - val_accuracy: 0.6201 - val_loss: 0.6389
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 50ms/step - accuracy: 0.5880 - loss: 0.6555 - val_accuracy: 0.6087 - val_loss: 0.6405
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 52ms/step - accuracy: 0.5838 - loss: 0.6490 - val_accuracy: 0.5698 - val_loss: 0.6691
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">7s</span> 83ms/step - accuracy: 0.5820 - loss: 0.6677 - val_accuracy: 0.6133 - val_loss: 0.6448
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 77ms/step - accuracy: 0.5932 - loss: 0.6516 - val_accuracy: 0.5767 - val_loss: 0.6631
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">6s</span> 84ms/step - accuracy: 0.5955 - loss: 0.6572 - val_accuracy: 0.6224 - val_loss: 0.6439
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 82ms/step - accuracy: 0.6109 - loss: 0.6465 - val_accuracy: 0.6156 - val_loss: 0.6364
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 85ms/step - accuracy: 0.6309 - loss: 0.6373 - val_accuracy: 0.6064 - val_loss: 0.6426
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 76ms/step - accuracy: 0.6082 - loss: 0.6430 - val_accuracy: 0.6247 - val_loss: 0.6394
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">6s</span> 93ms/step - accuracy: 0.6391 - loss: 0.6236 - val_accuracy: 0.6041 - val_loss: 0.6488
Now running, pct_increase: 1.01; days out: 15; independent_array: independent_array15; K-fold: 4
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 86ms/step - accuracy: 0.6044 - loss: 0.6399 - val_accuracy: 0.6293 - val_loss: 0.6204
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 91ms/step - accuracy: 0.6279 - loss: 0.6243 - val_accuracy: 0.6178 - val_loss: 0.6330
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 90ms/step - accuracy: 0.6276 - loss: 0.6291 - val_accuracy: 0.5973 - val_loss: 0.6327
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 90ms/step - accuracy: 0.6448 - loss: 0.6149 - val_accuracy: 0.6407 - val_loss: 0.6248
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 88ms/step - accuracy: 0.6397 - loss: 0.6194 - val_accuracy: 0.6247 - val_loss: 0.6180
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 66ms/step - accuracy: 0.6419 - loss: 0.6102 - val_accuracy: 0.6522 - val_loss: 0.6142
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 61ms/step - accuracy: 0.6458 - loss: 0.6123 - val_accuracy: 0.5995 - val_loss: 0.6540
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 62ms/step - accuracy: 0.6366 - loss: 0.6242 - val_accuracy: 0.6362 - val_loss: 0.6212
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 65ms/step - accuracy: 0.6402 - loss: 0.6106 - val_accuracy: 0.6293 - val_loss: 0.6287
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 64ms/step - accuracy: 0.6617 - loss: 0.5890 - val_accuracy: 0.6110 - val_loss: 0.6426
Now running, pct_increase: 1.01; days out: 15; independent_array: independent_array15; K-fold: 5
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 63ms/step - accuracy: 0.6587 - loss: 0.6051 - val_accuracy: 0.6751 - val_loss: 0.5907
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 62ms/step - accuracy: 0.6548 - loss: 0.6019 - val_accuracy: 0.6522 - val_loss: 0.5857
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 62ms/step - accuracy: 0.6678 - loss: 0.5888 - val_accuracy: 0.6453 - val_loss: 0.5842
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 65ms/step - accuracy: 0.6473 - loss: 0.5820 - val_accuracy: 0.6751 - val_loss: 0.5929
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 56ms/step - accuracy: 0.6704 - loss: 0.5844 - val_accuracy: 0.6773 - val_loss: 0.5985
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 62ms/step - accuracy: 0.6867 - loss: 0.5778 - val_accuracy: 0.6934 - val_loss: 0.5770
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 63ms/step - accuracy: 0.6640 - loss: 0.5721 - val_accuracy: 0.6705 - val_loss: 0.5927
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 49ms/step - accuracy: 0.6780 - loss: 0.5789 - val_accuracy: 0.6819 - val_loss: 0.5704
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 44ms/step - accuracy: 0.6607 - loss: 0.5874 - val_accuracy: 0.6476 - val_loss: 0.5783
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 43ms/step - accuracy: 0.6873 - loss: 0.5726 - val_accuracy: 0.6568 - val_loss: 0.5750
Now running, pct_increase: 1.02; days out: 1; independent_array: independent_array15; K-fold: 1
Epoch 1/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">6s</span> 52ms/step - accuracy: 0.8981 - loss: 0.2653 - val_accuracy: 0.9703 - val_loss: 0.1295
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 43ms/step - accuracy: 0.9720 - loss: 0.1297 - val_accuracy: 0.9703 - val_loss: 0.1181
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 43ms/step - accuracy: 0.9703 - loss: 0.1282 - val_accuracy: 0.9703 - val_loss: 0.1107
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 42ms/step - accuracy: 0.9730 - loss: 0.1163 - val_accuracy: 0.9703 - val_loss: 0.1097
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 44ms/step - accuracy: 0.9674 - loss: 0.1371 - val_accuracy: 0.9703 - val_loss: 0.1138
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 46ms/step - accuracy: 0.9671 - loss: 0.1298 - val_accuracy: 0.9703 - val_loss: 0.1090
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 44ms/step - accuracy: 0.9684 - loss: 0.1298 - val_accuracy: 0.9703 - val_loss: 0.1058
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 45ms/step - accuracy: 0.9661 - loss: 0.1338 - val_accuracy: 0.9703 - val_loss: 0.1085
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 41ms/step - accuracy: 0.9693 - loss: 0.1241 - val_accuracy: 0.9703 - val_loss: 0.1180
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 47ms/step - accuracy: 0.9741 - loss: 0.1076 - val_accuracy: 0.9703 - val_loss: 0.1086
Now running, pct_increase: 1.02; days out: 1; independent_array: independent_array15; K-fold: 2
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 43ms/step - accuracy: 0.9687 - loss: 0.1207 - val_accuracy: 0.9703 - val_loss: 0.1205
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 46ms/step - accuracy: 0.9661 - loss: 0.1219 - val_accuracy: 0.9703 - val_loss: 0.1275
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 42ms/step - accuracy: 0.9676 - loss: 0.1251 - val_accuracy: 0.9703 - val_loss: 0.1184
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 43ms/step - accuracy: 0.9757 - loss: 0.1044 - val_accuracy: 0.9703 - val_loss: 0.1205
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 45ms/step - accuracy: 0.9636 - loss: 0.1296 - val_accuracy: 0.9703 - val_loss: 0.1206
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 48ms/step - accuracy: 0.9705 - loss: 0.1122 - val_accuracy: 0.9703 - val_loss: 0.1208
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 47ms/step - accuracy: 0.9689 - loss: 0.1188 - val_accuracy: 0.9703 - val_loss: 0.1311
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 50ms/step - accuracy: 0.9696 - loss: 0.1205 - val_accuracy: 0.9703 - val_loss: 0.1192
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 50ms/step - accuracy: 0.9685 - loss: 0.1078 - val_accuracy: 0.9703 - val_loss: 0.1172
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 50ms/step - accuracy: 0.9712 - loss: 0.1078 - val_accuracy: 0.9703 - val_loss: 0.1183
Now running, pct_increase: 1.02; days out: 1; independent_array: independent_array15; K-fold: 3
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 50ms/step - accuracy: 0.9699 - loss: 0.1118 - val_accuracy: 0.9703 - val_loss: 0.1040
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 50ms/step - accuracy: 0.9630 - loss: 0.1294 - val_accuracy: 0.9703 - val_loss: 0.1083
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 51ms/step - accuracy: 0.9718 - loss: 0.1062 - val_accuracy: 0.9703 - val_loss: 0.1047
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 51ms/step - accuracy: 0.9702 - loss: 0.1064 - val_accuracy: 0.9703 - val_loss: 0.1052
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 51ms/step - accuracy: 0.9686 - loss: 0.1120 - val_accuracy: 0.9703 - val_loss: 0.1062
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 51ms/step - accuracy: 0.9722 - loss: 0.1048 - val_accuracy: 0.9703 - val_loss: 0.1079
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 50ms/step - accuracy: 0.9592 - loss: 0.1340 - val_accuracy: 0.9703 - val_loss: 0.1060
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">7s</span> 79ms/step - accuracy: 0.9716 - loss: 0.1060 - val_accuracy: 0.9703 - val_loss: 0.1069
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 81ms/step - accuracy: 0.9679 - loss: 0.1195 - val_accuracy: 0.9703 - val_loss: 0.1061
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 83ms/step - accuracy: 0.9674 - loss: 0.1131 - val_accuracy: 0.9703 - val_loss: 0.1049
Now running, pct_increase: 1.02; days out: 1; independent_array: independent_array15; K-fold: 4
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 83ms/step - accuracy: 0.9728 - loss: 0.0941 - val_accuracy: 0.9680 - val_loss: 0.1193
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 80ms/step - accuracy: 0.9663 - loss: 0.1033 - val_accuracy: 0.9658 - val_loss: 0.1308
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 83ms/step - accuracy: 0.9647 - loss: 0.1197 - val_accuracy: 0.9658 - val_loss: 0.1273
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 80ms/step - accuracy: 0.9743 - loss: 0.0863 - val_accuracy: 0.9658 - val_loss: 0.1259
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 78ms/step - accuracy: 0.9674 - loss: 0.1027 - val_accuracy: 0.9658 - val_loss: 0.1326
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 79ms/step - accuracy: 0.9730 - loss: 0.0958 - val_accuracy: 0.9658 - val_loss: 0.1274
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 84ms/step - accuracy: 0.9674 - loss: 0.1036 - val_accuracy: 0.9680 - val_loss: 0.1288
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 83ms/step - accuracy: 0.9720 - loss: 0.0974 - val_accuracy: 0.9658 - val_loss: 0.1366
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 80ms/step - accuracy: 0.9742 - loss: 0.0958 - val_accuracy: 0.9680 - val_loss: 0.1322
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 62ms/step - accuracy: 0.9686 - loss: 0.0959 - val_accuracy: 0.9658 - val_loss: 0.1310
Now running, pct_increase: 1.02; days out: 1; independent_array: independent_array15; K-fold: 5
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 64ms/step - accuracy: 0.9760 - loss: 0.0929 - val_accuracy: 0.9725 - val_loss: 0.0979
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 64ms/step - accuracy: 0.9689 - loss: 0.1082 - val_accuracy: 0.9725 - val_loss: 0.0929
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 61ms/step - accuracy: 0.9700 - loss: 0.1024 - val_accuracy: 0.9725 - val_loss: 0.0947
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 61ms/step - accuracy: 0.9657 - loss: 0.1052 - val_accuracy: 0.9703 - val_loss: 0.0911
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 61ms/step - accuracy: 0.9703 - loss: 0.0943 - val_accuracy: 0.9703 - val_loss: 0.0921
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 65ms/step - accuracy: 0.9678 - loss: 0.1085 - val_accuracy: 0.9703 - val_loss: 0.0993
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 66ms/step - accuracy: 0.9710 - loss: 0.0906 - val_accuracy: 0.9703 - val_loss: 0.0916
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 64ms/step - accuracy: 0.9770 - loss: 0.0827 - val_accuracy: 0.9703 - val_loss: 0.0938
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 63ms/step - accuracy: 0.9737 - loss: 0.0873 - val_accuracy: 0.9680 - val_loss: 0.0963
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">6s</span> 69ms/step - accuracy: 0.9738 - loss: 0.0923 - val_accuracy: 0.9703 - val_loss: 0.0920
Now running, pct_increase: 1.02; days out: 3; independent_array: independent_array15; K-fold: 1
Epoch 1/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 49ms/step - accuracy: 0.8883 - loss: 0.3802 - val_accuracy: 0.8881 - val_loss: 0.3406
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 40ms/step - accuracy: 0.8957 - loss: 0.3192 - val_accuracy: 0.8881 - val_loss: 0.3387
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 42ms/step - accuracy: 0.8772 - loss: 0.3444 - val_accuracy: 0.8881 - val_loss: 0.3409
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 43ms/step - accuracy: 0.8918 - loss: 0.3181 - val_accuracy: 0.8881 - val_loss: 0.3466
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 44ms/step - accuracy: 0.8809 - loss: 0.3351 - val_accuracy: 0.8881 - val_loss: 0.3409
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 43ms/step - accuracy: 0.8918 - loss: 0.3279 - val_accuracy: 0.8881 - val_loss: 0.3477
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 44ms/step - accuracy: 0.8881 - loss: 0.3301 - val_accuracy: 0.8881 - val_loss: 0.3437
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 46ms/step - accuracy: 0.8889 - loss: 0.3224 - val_accuracy: 0.8881 - val_loss: 0.3519
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 45ms/step - accuracy: 0.8809 - loss: 0.3439 - val_accuracy: 0.8881 - val_loss: 0.3391
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 43ms/step - accuracy: 0.8876 - loss: 0.3342 - val_accuracy: 0.8881 - val_loss: 0.3448
Now running, pct_increase: 1.02; days out: 3; independent_array: independent_array15; K-fold: 2
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 43ms/step - accuracy: 0.8865 - loss: 0.3231 - val_accuracy: 0.8881 - val_loss: 0.3323
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 47ms/step - accuracy: 0.8780 - loss: 0.3497 - val_accuracy: 0.8881 - val_loss: 0.3300
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 44ms/step - accuracy: 0.8863 - loss: 0.3195 - val_accuracy: 0.8881 - val_loss: 0.3239
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 42ms/step - accuracy: 0.8872 - loss: 0.3285 - val_accuracy: 0.8881 - val_loss: 0.3336
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 45ms/step - accuracy: 0.8935 - loss: 0.3196 - val_accuracy: 0.8881 - val_loss: 0.3296
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 43ms/step - accuracy: 0.8864 - loss: 0.3357 - val_accuracy: 0.8881 - val_loss: 0.3237
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 45ms/step - accuracy: 0.8804 - loss: 0.3344 - val_accuracy: 0.8881 - val_loss: 0.3355
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 46ms/step - accuracy: 0.8933 - loss: 0.3227 - val_accuracy: 0.8881 - val_loss: 0.3332
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 47ms/step - accuracy: 0.8897 - loss: 0.3115 - val_accuracy: 0.8881 - val_loss: 0.3214
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 49ms/step - accuracy: 0.8961 - loss: 0.2912 - val_accuracy: 0.8881 - val_loss: 0.3204
Now running, pct_increase: 1.02; days out: 3; independent_array: independent_array15; K-fold: 3
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 51ms/step - accuracy: 0.8903 - loss: 0.3113 - val_accuracy: 0.8902 - val_loss: 0.3320
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 49ms/step - accuracy: 0.8906 - loss: 0.3009 - val_accuracy: 0.8902 - val_loss: 0.3382
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 50ms/step - accuracy: 0.8863 - loss: 0.3255 - val_accuracy: 0.8902 - val_loss: 0.3318
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 50ms/step - accuracy: 0.8965 - loss: 0.2907 - val_accuracy: 0.8902 - val_loss: 0.3308
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 51ms/step - accuracy: 0.8992 - loss: 0.2955 - val_accuracy: 0.8902 - val_loss: 0.3320
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 50ms/step - accuracy: 0.8840 - loss: 0.3113 - val_accuracy: 0.8902 - val_loss: 0.3342
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 50ms/step - accuracy: 0.8975 - loss: 0.2791 - val_accuracy: 0.8902 - val_loss: 0.3354
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 51ms/step - accuracy: 0.8913 - loss: 0.3023 - val_accuracy: 0.8902 - val_loss: 0.3359
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 50ms/step - accuracy: 0.8916 - loss: 0.3063 - val_accuracy: 0.8902 - val_loss: 0.3308
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 52ms/step - accuracy: 0.8989 - loss: 0.2763 - val_accuracy: 0.8902 - val_loss: 0.3282
Now running, pct_increase: 1.02; days out: 3; independent_array: independent_array15; K-fold: 4
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 50ms/step - accuracy: 0.8924 - loss: 0.3081 - val_accuracy: 0.8924 - val_loss: 0.2864
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 80ms/step - accuracy: 0.8809 - loss: 0.3307 - val_accuracy: 0.8902 - val_loss: 0.2840
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 81ms/step - accuracy: 0.8849 - loss: 0.3245 - val_accuracy: 0.8947 - val_loss: 0.2818
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 82ms/step - accuracy: 0.8998 - loss: 0.3052 - val_accuracy: 0.8970 - val_loss: 0.2905
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 80ms/step - accuracy: 0.8894 - loss: 0.3199 - val_accuracy: 0.8947 - val_loss: 0.2879
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 82ms/step - accuracy: 0.8904 - loss: 0.3168 - val_accuracy: 0.8993 - val_loss: 0.2883
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 83ms/step - accuracy: 0.8954 - loss: 0.2895 - val_accuracy: 0.8924 - val_loss: 0.2912
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 78ms/step - accuracy: 0.8855 - loss: 0.3111 - val_accuracy: 0.8970 - val_loss: 0.2837
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 78ms/step - accuracy: 0.8905 - loss: 0.3078 - val_accuracy: 0.8902 - val_loss: 0.2968
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 81ms/step - accuracy: 0.8959 - loss: 0.2936 - val_accuracy: 0.9039 - val_loss: 0.2976
Now running, pct_increase: 1.02; days out: 3; independent_array: independent_array15; K-fold: 5
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 82ms/step - accuracy: 0.8942 - loss: 0.3084 - val_accuracy: 0.8970 - val_loss: 0.2635
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 85ms/step - accuracy: 0.9051 - loss: 0.2812 - val_accuracy: 0.9016 - val_loss: 0.2694
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 86ms/step - accuracy: 0.9023 - loss: 0.2941 - val_accuracy: 0.9016 - val_loss: 0.2665
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 63ms/step - accuracy: 0.9164 - loss: 0.2634 - val_accuracy: 0.9062 - val_loss: 0.2624
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 63ms/step - accuracy: 0.9055 - loss: 0.2905 - val_accuracy: 0.9062 - val_loss: 0.2639
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 67ms/step - accuracy: 0.8777 - loss: 0.3289 - val_accuracy: 0.8993 - val_loss: 0.2809
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 66ms/step - accuracy: 0.8843 - loss: 0.3143 - val_accuracy: 0.9062 - val_loss: 0.2731
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 65ms/step - accuracy: 0.8868 - loss: 0.3157 - val_accuracy: 0.8993 - val_loss: 0.2820
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 64ms/step - accuracy: 0.9045 - loss: 0.2893 - val_accuracy: 0.8970 - val_loss: 0.2750
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 61ms/step - accuracy: 0.9096 - loss: 0.2716 - val_accuracy: 0.8993 - val_loss: 0.2806
Now running, pct_increase: 1.02; days out: 5; independent_array: independent_array15; K-fold: 1
Epoch 1/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 48ms/step - accuracy: 0.8427 - loss: 0.4823 - val_accuracy: 0.8333 - val_loss: 0.4301
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 41ms/step - accuracy: 0.8293 - loss: 0.4393 - val_accuracy: 0.8333 - val_loss: 0.4267
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 41ms/step - accuracy: 0.8364 - loss: 0.4388 - val_accuracy: 0.8333 - val_loss: 0.4273
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 41ms/step - accuracy: 0.8482 - loss: 0.3977 - val_accuracy: 0.8333 - val_loss: 0.4291
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 44ms/step - accuracy: 0.8257 - loss: 0.4405 - val_accuracy: 0.8333 - val_loss: 0.4274
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 41ms/step - accuracy: 0.8384 - loss: 0.4257 - val_accuracy: 0.8333 - val_loss: 0.4419
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 41ms/step - accuracy: 0.8248 - loss: 0.4401 - val_accuracy: 0.8333 - val_loss: 0.4305
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 39ms/step - accuracy: 0.8371 - loss: 0.4254 - val_accuracy: 0.8333 - val_loss: 0.4303
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 43ms/step - accuracy: 0.8287 - loss: 0.4393 - val_accuracy: 0.8311 - val_loss: 0.4262
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 44ms/step - accuracy: 0.8345 - loss: 0.4249 - val_accuracy: 0.8333 - val_loss: 0.4368
Now running, pct_increase: 1.02; days out: 5; independent_array: independent_array15; K-fold: 2
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 44ms/step - accuracy: 0.8383 - loss: 0.4211 - val_accuracy: 0.8398 - val_loss: 0.4267
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 41ms/step - accuracy: 0.8481 - loss: 0.3990 - val_accuracy: 0.8375 - val_loss: 0.4205
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 43ms/step - accuracy: 0.8358 - loss: 0.4212 - val_accuracy: 0.8398 - val_loss: 0.4119
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 41ms/step - accuracy: 0.8311 - loss: 0.4269 - val_accuracy: 0.8398 - val_loss: 0.4220
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 48ms/step - accuracy: 0.8137 - loss: 0.4461 - val_accuracy: 0.8375 - val_loss: 0.4180
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 45ms/step - accuracy: 0.8485 - loss: 0.3929 - val_accuracy: 0.8375 - val_loss: 0.4139
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 43ms/step - accuracy: 0.8207 - loss: 0.4315 - val_accuracy: 0.8375 - val_loss: 0.4196
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 44ms/step - accuracy: 0.8286 - loss: 0.4121 - val_accuracy: 0.8375 - val_loss: 0.4126
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 45ms/step - accuracy: 0.8361 - loss: 0.3974 - val_accuracy: 0.8375 - val_loss: 0.4236
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 44ms/step - accuracy: 0.8354 - loss: 0.4090 - val_accuracy: 0.8352 - val_loss: 0.4142
Now running, pct_increase: 1.02; days out: 5; independent_array: independent_array15; K-fold: 3
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 50ms/step - accuracy: 0.8538 - loss: 0.3817 - val_accuracy: 0.8375 - val_loss: 0.4296
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 50ms/step - accuracy: 0.8314 - loss: 0.4232 - val_accuracy: 0.8352 - val_loss: 0.4290
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 51ms/step - accuracy: 0.8472 - loss: 0.3904 - val_accuracy: 0.8375 - val_loss: 0.4169
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 48ms/step - accuracy: 0.8346 - loss: 0.4020 - val_accuracy: 0.8375 - val_loss: 0.4219
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 51ms/step - accuracy: 0.8419 - loss: 0.3924 - val_accuracy: 0.8101 - val_loss: 0.4394
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 47ms/step - accuracy: 0.8325 - loss: 0.4082 - val_accuracy: 0.8352 - val_loss: 0.4134
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 48ms/step - accuracy: 0.8385 - loss: 0.4057 - val_accuracy: 0.8375 - val_loss: 0.4278
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 50ms/step - accuracy: 0.8349 - loss: 0.4104 - val_accuracy: 0.8284 - val_loss: 0.4243
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 51ms/step - accuracy: 0.8409 - loss: 0.3880 - val_accuracy: 0.8352 - val_loss: 0.4173
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 51ms/step - accuracy: 0.8481 - loss: 0.3927 - val_accuracy: 0.8261 - val_loss: 0.4297
Now running, pct_increase: 1.02; days out: 5; independent_array: independent_array15; K-fold: 4
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 52ms/step - accuracy: 0.8320 - loss: 0.4242 - val_accuracy: 0.8444 - val_loss: 0.3728
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 52ms/step - accuracy: 0.8439 - loss: 0.3930 - val_accuracy: 0.8513 - val_loss: 0.3672
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 50ms/step - accuracy: 0.8577 - loss: 0.3728 - val_accuracy: 0.8490 - val_loss: 0.3748
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">7s</span> 85ms/step - accuracy: 0.8478 - loss: 0.3953 - val_accuracy: 0.8535 - val_loss: 0.3767
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 81ms/step - accuracy: 0.8530 - loss: 0.3654 - val_accuracy: 0.8444 - val_loss: 0.3840
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 84ms/step - accuracy: 0.8359 - loss: 0.4054 - val_accuracy: 0.8467 - val_loss: 0.3813
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 82ms/step - accuracy: 0.8481 - loss: 0.3849 - val_accuracy: 0.8513 - val_loss: 0.3919
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 80ms/step - accuracy: 0.8382 - loss: 0.4072 - val_accuracy: 0.8444 - val_loss: 0.3764
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 83ms/step - accuracy: 0.8444 - loss: 0.3981 - val_accuracy: 0.8467 - val_loss: 0.3849
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 76ms/step - accuracy: 0.8290 - loss: 0.4261 - val_accuracy: 0.8444 - val_loss: 0.4011
Now running, pct_increase: 1.02; days out: 5; independent_array: independent_array15; K-fold: 5
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 83ms/step - accuracy: 0.8394 - loss: 0.4026 - val_accuracy: 0.8444 - val_loss: 0.3821
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 78ms/step - accuracy: 0.8383 - loss: 0.4016 - val_accuracy: 0.8490 - val_loss: 0.3757
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 76ms/step - accuracy: 0.8507 - loss: 0.3907 - val_accuracy: 0.8467 - val_loss: 0.3845
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 79ms/step - accuracy: 0.8559 - loss: 0.3545 - val_accuracy: 0.8513 - val_loss: 0.3908
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">6s</span> 85ms/step - accuracy: 0.8483 - loss: 0.3816 - val_accuracy: 0.8513 - val_loss: 0.3723
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 56ms/step - accuracy: 0.8581 - loss: 0.3640 - val_accuracy: 0.8627 - val_loss: 0.3766
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 56ms/step - accuracy: 0.8644 - loss: 0.3566 - val_accuracy: 0.8604 - val_loss: 0.3846
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">6s</span> 65ms/step - accuracy: 0.8392 - loss: 0.4244 - val_accuracy: 0.8513 - val_loss: 0.3759
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 60ms/step - accuracy: 0.8554 - loss: 0.3572 - val_accuracy: 0.8467 - val_loss: 0.3861
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">6s</span> 68ms/step - accuracy: 0.8450 - loss: 0.3823 - val_accuracy: 0.8581 - val_loss: 0.3748
Now running, pct_increase: 1.02; days out: 10; independent_array: independent_array15; K-fold: 1
Epoch 1/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">8s</span> 72ms/step - accuracy: 0.6856 - loss: 0.6356 - val_accuracy: 0.7323 - val_loss: 0.5729
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 60ms/step - accuracy: 0.7426 - loss: 0.5660 - val_accuracy: 0.7323 - val_loss: 0.5691
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 44ms/step - accuracy: 0.7261 - loss: 0.5779 - val_accuracy: 0.7323 - val_loss: 0.5682
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 43ms/step - accuracy: 0.7295 - loss: 0.5750 - val_accuracy: 0.7323 - val_loss: 0.5640
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 42ms/step - accuracy: 0.7196 - loss: 0.5886 - val_accuracy: 0.7300 - val_loss: 0.5683
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 38ms/step - accuracy: 0.7472 - loss: 0.5589 - val_accuracy: 0.7323 - val_loss: 0.5690
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 45ms/step - accuracy: 0.7154 - loss: 0.5867 - val_accuracy: 0.7323 - val_loss: 0.5606
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 42ms/step - accuracy: 0.7490 - loss: 0.5349 - val_accuracy: 0.7346 - val_loss: 0.5653
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 42ms/step - accuracy: 0.7212 - loss: 0.5649 - val_accuracy: 0.7323 - val_loss: 0.5586
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 39ms/step - accuracy: 0.7443 - loss: 0.5437 - val_accuracy: 0.7391 - val_loss: 0.5660
Now running, pct_increase: 1.02; days out: 10; independent_array: independent_array15; K-fold: 2
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 43ms/step - accuracy: 0.7495 - loss: 0.5364 - val_accuracy: 0.7529 - val_loss: 0.5438
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 45ms/step - accuracy: 0.7229 - loss: 0.5627 - val_accuracy: 0.7460 - val_loss: 0.5442
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 42ms/step - accuracy: 0.7232 - loss: 0.5613 - val_accuracy: 0.7414 - val_loss: 0.5583
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 44ms/step - accuracy: 0.7392 - loss: 0.5585 - val_accuracy: 0.7551 - val_loss: 0.5465
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 40ms/step - accuracy: 0.7375 - loss: 0.5498 - val_accuracy: 0.7437 - val_loss: 0.5414
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 44ms/step - accuracy: 0.7570 - loss: 0.5246 - val_accuracy: 0.7231 - val_loss: 0.5477
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 43ms/step - accuracy: 0.7463 - loss: 0.5440 - val_accuracy: 0.7323 - val_loss: 0.5475
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 43ms/step - accuracy: 0.7405 - loss: 0.5479 - val_accuracy: 0.7254 - val_loss: 0.5558
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 41ms/step - accuracy: 0.7475 - loss: 0.5339 - val_accuracy: 0.7323 - val_loss: 0.5500
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 43ms/step - accuracy: 0.7679 - loss: 0.5220 - val_accuracy: 0.7368 - val_loss: 0.5505
Now running, pct_increase: 1.02; days out: 10; independent_array: independent_array15; K-fold: 3
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 41ms/step - accuracy: 0.7504 - loss: 0.5346 - val_accuracy: 0.7666 - val_loss: 0.5028
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 45ms/step - accuracy: 0.7736 - loss: 0.5060 - val_accuracy: 0.7620 - val_loss: 0.5111
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 43ms/step - accuracy: 0.7763 - loss: 0.5073 - val_accuracy: 0.7666 - val_loss: 0.5014
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 44ms/step - accuracy: 0.7605 - loss: 0.5219 - val_accuracy: 0.7689 - val_loss: 0.5088
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 44ms/step - accuracy: 0.7554 - loss: 0.5181 - val_accuracy: 0.7666 - val_loss: 0.5041
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 46ms/step - accuracy: 0.7714 - loss: 0.5072 - val_accuracy: 0.7574 - val_loss: 0.5245
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 50ms/step - accuracy: 0.7782 - loss: 0.5079 - val_accuracy: 0.7460 - val_loss: 0.5321
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 49ms/step - accuracy: 0.7736 - loss: 0.4973 - val_accuracy: 0.7597 - val_loss: 0.5023
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 49ms/step - accuracy: 0.7830 - loss: 0.4897 - val_accuracy: 0.7529 - val_loss: 0.5184
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 48ms/step - accuracy: 0.7662 - loss: 0.5220 - val_accuracy: 0.7597 - val_loss: 0.5150
Now running, pct_increase: 1.02; days out: 10; independent_array: independent_array15; K-fold: 4
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 50ms/step - accuracy: 0.7761 - loss: 0.5023 - val_accuracy: 0.7803 - val_loss: 0.4857
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 55ms/step - accuracy: 0.7652 - loss: 0.5068 - val_accuracy: 0.7849 - val_loss: 0.4913
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 52ms/step - accuracy: 0.7796 - loss: 0.4975 - val_accuracy: 0.7986 - val_loss: 0.4788
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 51ms/step - accuracy: 0.7723 - loss: 0.5020 - val_accuracy: 0.7574 - val_loss: 0.5196
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 52ms/step - accuracy: 0.7740 - loss: 0.5055 - val_accuracy: 0.7780 - val_loss: 0.4823
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 49ms/step - accuracy: 0.7696 - loss: 0.4980 - val_accuracy: 0.7574 - val_loss: 0.5082
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 51ms/step - accuracy: 0.7691 - loss: 0.4998 - val_accuracy: 0.7712 - val_loss: 0.4975
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">6s</span> 61ms/step - accuracy: 0.7860 - loss: 0.4849 - val_accuracy: 0.7895 - val_loss: 0.4963
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">6s</span> 81ms/step - accuracy: 0.7784 - loss: 0.4779 - val_accuracy: 0.7757 - val_loss: 0.4975
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 81ms/step - accuracy: 0.7861 - loss: 0.4827 - val_accuracy: 0.7689 - val_loss: 0.4926
Now running, pct_increase: 1.02; days out: 10; independent_array: independent_array15; K-fold: 5
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 79ms/step - accuracy: 0.8000 - loss: 0.4595 - val_accuracy: 0.7574 - val_loss: 0.5160
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 78ms/step - accuracy: 0.7861 - loss: 0.4773 - val_accuracy: 0.7735 - val_loss: 0.4993
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 82ms/step - accuracy: 0.7809 - loss: 0.4782 - val_accuracy: 0.7712 - val_loss: 0.5085
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 83ms/step - accuracy: 0.8177 - loss: 0.4300 - val_accuracy: 0.7597 - val_loss: 0.5137
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 82ms/step - accuracy: 0.7921 - loss: 0.4719 - val_accuracy: 0.7620 - val_loss: 0.5283
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 82ms/step - accuracy: 0.7950 - loss: 0.4420 - val_accuracy: 0.7735 - val_loss: 0.5513
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 81ms/step - accuracy: 0.8264 - loss: 0.4024 - val_accuracy: 0.7483 - val_loss: 0.5458
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 81ms/step - accuracy: 0.8040 - loss: 0.4595 - val_accuracy: 0.7597 - val_loss: 0.5451
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 82ms/step - accuracy: 0.7895 - loss: 0.4597 - val_accuracy: 0.7574 - val_loss: 0.5784
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 70ms/step - accuracy: 0.7911 - loss: 0.4581 - val_accuracy: 0.7689 - val_loss: 0.5548
Now running, pct_increase: 1.02; days out: 15; independent_array: independent_array15; K-fold: 1
Epoch 1/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">9s</span> 74ms/step - accuracy: 0.6253 - loss: 0.6607 - val_accuracy: 0.6384 - val_loss: 0.6517
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 61ms/step - accuracy: 0.6276 - loss: 0.6496 - val_accuracy: 0.6293 - val_loss: 0.6449
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 64ms/step - accuracy: 0.6516 - loss: 0.6359 - val_accuracy: 0.6201 - val_loss: 0.6469
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 62ms/step - accuracy: 0.6398 - loss: 0.6461 - val_accuracy: 0.6339 - val_loss: 0.6533
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 67ms/step - accuracy: 0.6677 - loss: 0.6314 - val_accuracy: 0.6384 - val_loss: 0.6494
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 70ms/step - accuracy: 0.6525 - loss: 0.6318 - val_accuracy: 0.6247 - val_loss: 0.6463
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">4s</span> 43ms/step - accuracy: 0.6372 - loss: 0.6463 - val_accuracy: 0.6430 - val_loss: 0.6377
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 42ms/step - accuracy: 0.6548 - loss: 0.6353 - val_accuracy: 0.6682 - val_loss: 0.6338
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 42ms/step - accuracy: 0.6583 - loss: 0.6241 - val_accuracy: 0.6362 - val_loss: 0.6386
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 43ms/step - accuracy: 0.6394 - loss: 0.6394 - val_accuracy: 0.6659 - val_loss: 0.6322
Now running, pct_increase: 1.02; days out: 15; independent_array: independent_array15; K-fold: 2
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 44ms/step - accuracy: 0.6521 - loss: 0.6397 - val_accuracy: 0.6407 - val_loss: 0.6434
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 40ms/step - accuracy: 0.6702 - loss: 0.6193 - val_accuracy: 0.6201 - val_loss: 0.6522
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 40ms/step - accuracy: 0.6250 - loss: 0.6476 - val_accuracy: 0.6590 - val_loss: 0.6383
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 43ms/step - accuracy: 0.6677 - loss: 0.6284 - val_accuracy: 0.6751 - val_loss: 0.6147
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 42ms/step - accuracy: 0.6560 - loss: 0.6189 - val_accuracy: 0.6888 - val_loss: 0.6062
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 46ms/step - accuracy: 0.6472 - loss: 0.6354 - val_accuracy: 0.6911 - val_loss: 0.6101
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 43ms/step - accuracy: 0.6582 - loss: 0.6290 - val_accuracy: 0.6819 - val_loss: 0.6123
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 45ms/step - accuracy: 0.6945 - loss: 0.5945 - val_accuracy: 0.6934 - val_loss: 0.6163
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 45ms/step - accuracy: 0.6866 - loss: 0.5995 - val_accuracy: 0.6590 - val_loss: 0.6448
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 45ms/step - accuracy: 0.6900 - loss: 0.5987 - val_accuracy: 0.6957 - val_loss: 0.6097
Now running, pct_increase: 1.02; days out: 15; independent_array: independent_array15; K-fold: 3
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 46ms/step - accuracy: 0.7037 - loss: 0.6009 - val_accuracy: 0.6979 - val_loss: 0.5840
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 45ms/step - accuracy: 0.6900 - loss: 0.6170 - val_accuracy: 0.6865 - val_loss: 0.5971
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 43ms/step - accuracy: 0.7029 - loss: 0.5944 - val_accuracy: 0.6842 - val_loss: 0.6071
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 43ms/step - accuracy: 0.7037 - loss: 0.5787 - val_accuracy: 0.7048 - val_loss: 0.5798
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 44ms/step - accuracy: 0.7120 - loss: 0.5709 - val_accuracy: 0.7025 - val_loss: 0.5806
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 42ms/step - accuracy: 0.7086 - loss: 0.5785 - val_accuracy: 0.6911 - val_loss: 0.5937
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">2s</span> 44ms/step - accuracy: 0.6988 - loss: 0.5892 - val_accuracy: 0.6888 - val_loss: 0.6055
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 44ms/step - accuracy: 0.6873 - loss: 0.6092 - val_accuracy: 0.7048 - val_loss: 0.5904
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 45ms/step - accuracy: 0.6835 - loss: 0.5910 - val_accuracy: 0.7002 - val_loss: 0.5778
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 47ms/step - accuracy: 0.7307 - loss: 0.5539 - val_accuracy: 0.7162 - val_loss: 0.5742
Now running, pct_increase: 1.02; days out: 15; independent_array: independent_array15; K-fold: 4
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 51ms/step - accuracy: 0.6871 - loss: 0.5917 - val_accuracy: 0.6888 - val_loss: 0.6001
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 49ms/step - accuracy: 0.7019 - loss: 0.5978 - val_accuracy: 0.7048 - val_loss: 0.5915
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 51ms/step - accuracy: 0.7070 - loss: 0.5768 - val_accuracy: 0.6934 - val_loss: 0.5851
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 49ms/step - accuracy: 0.7224 - loss: 0.5662 - val_accuracy: 0.7162 - val_loss: 0.5893
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 52ms/step - accuracy: 0.7144 - loss: 0.5574 - val_accuracy: 0.7025 - val_loss: 0.6003
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 51ms/step - accuracy: 0.7094 - loss: 0.5617 - val_accuracy: 0.6682 - val_loss: 0.6495
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 53ms/step - accuracy: 0.7321 - loss: 0.5538 - val_accuracy: 0.7025 - val_loss: 0.5868
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 50ms/step - accuracy: 0.7199 - loss: 0.5569 - val_accuracy: 0.7117 - val_loss: 0.5860
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 51ms/step - accuracy: 0.7188 - loss: 0.5622 - val_accuracy: 0.6957 - val_loss: 0.6012
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 50ms/step - accuracy: 0.7281 - loss: 0.5508 - val_accuracy: 0.6819 - val_loss: 0.6221
Now running, pct_increase: 1.02; days out: 15; independent_array: independent_array15; K-fold: 5
Epoch 1/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 49ms/step - accuracy: 0.7174 - loss: 0.5534 - val_accuracy: 0.6957 - val_loss: 0.5887
Epoch 2/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 51ms/step - accuracy: 0.7093 - loss: 0.5661 - val_accuracy: 0.7346 - val_loss: 0.5567
Epoch 3/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">3s</span> 56ms/step - accuracy: 0.7444 - loss: 0.5319 - val_accuracy: 0.7048 - val_loss: 0.5701
Epoch 4/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">7s</span> 85ms/step - accuracy: 0.7296 - loss: 0.5535 - val_accuracy: 0.6911 - val_loss: 0.5831
Epoch 5/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 81ms/step - accuracy: 0.7223 - loss: 0.5451 - val_accuracy: 0.6773 - val_loss: 0.6037
Epoch 6/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">6s</span> 98ms/step - accuracy: 0.7333 - loss: 0.5458 - val_accuracy: 0.7346 - val_loss: 0.5636
Epoch 7/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">10s</span> 83ms/step - accuracy: 0.7588 - loss: 0.5240 - val_accuracy: 0.6888 - val_loss: 0.6254
Epoch 8/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 84ms/step - accuracy: 0.7515 - loss: 0.5231 - val_accuracy: 0.7185 - val_loss: 0.5670
Epoch 9/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 85ms/step - accuracy: 0.7434 - loss: 0.5213 - val_accuracy: 0.7254 - val_loss: 0.5809
Epoch 10/10
<span class="ansi-bold">55/55</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">5s</span> 76ms/step - accuracy: 0.7547 - loss: 0.4993 - val_accuracy: 0.6957 - val_loss: 0.6050
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=d4ca4428-0b62-4d10-b997-4479cc7e54ab">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[144]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1">#will output one CSV files with training results for classification model</span>
<span class="n">accuracy_df</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="sa">f</span><span class="s1">'</span><span class="si">{</span><span class="n">ticker_symbol</span><span class="si">}</span><span class="s1">_</span><span class="si">{</span><span class="n">selected_pattern</span><span class="si">}</span><span class="s1">_classification_2200_Observations_output.csv'</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># `index=False` avoids writing the index column</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=1e5c94c9-e690-4894-8a9a-d417ad519b31">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="Final-LSTM-Classification-Model-with:-Increased-Observations,-Stratified-K-fold-Cross-Validation,-Ensemble-Learning-Approach">Final LSTM Classification Model with: Increased Observations, Stratified K-fold Cross Validation, Ensemble Learning Approach<a class="anchor-link" href="#Final-LSTM-Classification-Model-with:-Increased-Observations,-Stratified-K-fold-Cross-Validation,-Ensemble-Learning-Approach"></a></h4><p>To build off from the previous code, I will now implement ensemble methods. Below, I am going to create and train 9 models, all of which have the same architecture and train them all on my best performing set of independent variables, independent array #15.  Each of these 9 models will undergo Stratified 5-fold cross validation as we did previously, and I will save the best weights for each model during training using a ModelCheckpoint callback - the epoch with the highest validation accuracy across all five folds will have its model weights saved.</p>
<p>After the models have been trained and their best weights saved, I will load these models and combine their predictions using an ensemble approach. Specifically, I will take each model and output its predictions; I will use a voting-based system where if any 5 of the 9 models (the majority) predict the class as a positive class, then its final prediction will be classified as positive, and if less than 5 models predict it as positive, it will be classified as negative. This approach ensures that the final decision is based on the collective judgment of the majority of the models, reducing the likelihood of errors from individual models.</p>
<p>Lastly, the accuracy of the ensemble model will be evaluated on a separate test dataset to assess the effectiveness of this combined approach. This ensemble method aims to leverage the strengths of each individual model and provide a more robust prediction.</p>
<p>For this example below, I will just set the 'days_out' parameter to 10 and the 'pct_increase' parameter to 1.01. I am currently choosing this parameter combination because when analyzing the model results from the last step, there appeared to be a noticeable increase in the accuracy score from my model (67.3%) when compared to always guessing the majority class (55.3%).</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=93d6607c-77a8-45c6-877a-352f121a2269">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[145]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1">#Subset data frame for desired pattern</span>
<span class="n">pattern_df</span> <span class="o">=</span> <span class="n">finance_df</span><span class="p">[</span><span class="n">finance_df</span><span class="p">[</span><span class="s1">'Random_Yes_No_2'</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"Yes"</span><span class="p">]</span>

<span class="c1">#How many days after the pattern is identified to use for the dependent variable</span>
<span class="n">days_out</span> <span class="o">=</span> <span class="mi">10</span>

<span class="c1">#What percent increase from the current price is considered a positive class. For example 1.01 = 1% increase; 100 * 1.01 = 101. So if original</span>
<span class="c1">#price is $100, anything greater than $101 is considered a positive class.</span>
<span class="n">pct_increase</span> <span class="o">=</span> <span class="mf">1.01</span>

<span class="c1">#Gather independent variables</span>
<span class="n">independent_list15</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1">#gather dependent variables</span>
<span class="n">dependent_list</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">pattern_index</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">pattern_df</span><span class="p">[</span><span class="s2">"Row_index"</span><span class="p">])</span>
<span class="c1">#pattern_index = [60, 62]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">pattern_index</span><span class="p">:</span>
    <span class="c1">#if (i == 62):</span>
    <span class="c1">#    break</span>
    
    <span class="c1">#unable to get 30 days worth of data if index is less than 56, because previously removed first 26 observations</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="mi">56</span><span class="p">):</span>
        <span class="k">continue</span>

    <span class="c1">#get 30 days worth of data to gather data for indpendent variables</span>
    <span class="n">subset_df</span> <span class="o">=</span> <span class="n">finance_df</span><span class="p">[(</span><span class="n">finance_df</span><span class="p">[</span><span class="s2">"Row_index"</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="n">i</span> <span class="o">-</span> <span class="mi">29</span><span class="p">))</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">finance_df</span><span class="p">[</span><span class="s2">"Row_index"</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="p">(</span><span class="n">i</span><span class="p">))]</span>
    <span class="c1">#subset_df = finance_df[(finance_df["Row_index"] &gt;= (i - 13)) &amp; (finance_df["Row_index"] &lt;= (i))]</span>
    
    <span class="c1">#Get day after data to gather closing price for dependent variable</span>
    <span class="n">dependent_df</span> <span class="o">=</span> <span class="n">finance_df</span><span class="p">[</span><span class="n">finance_df</span><span class="p">[</span><span class="s2">"Row_index"</span><span class="p">]</span> <span class="o">==</span> <span class="p">(</span><span class="n">i</span><span class="p">)]</span>
    <span class="n">dependent2_df</span> <span class="o">=</span> <span class="n">finance_df</span><span class="p">[</span><span class="n">finance_df</span><span class="p">[</span><span class="s2">"Row_index"</span><span class="p">]</span> <span class="o">==</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="n">days_out</span><span class="p">)]</span>
    
    <span class="n">temp_list15</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1">#append temp_list to independent_list</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">dependent2_df</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span> <span class="c1">#dependent2_df may have length of zero as it is a future date, data may not be available</span>
    

        <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">subset_df</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
                
                <span class="n">test_array15</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'RSI'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MFI'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MACD'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Signal_Line'</span><span class="p">]])</span>
            
                <span class="n">temp_list15</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array15</span><span class="p">)</span>
                
        <span class="n">independent_list15</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list15</span><span class="p">)</span>
    
        <span class="k">if</span> <span class="p">(</span><span class="n">dependent2_df</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">dependent_df</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">pct_increase</span><span class="p">):</span>
            <span class="n">dependent_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">dependent_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">independent_array15</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list15</span><span class="p">)</span>
<span class="n">dependent_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dependent_list</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=c92bea2c-9dd1-48b9-84aa-fb73eeb08ab5">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>The resulting shape of the independent variable dataset is: 30 time-steps and 8 features per time-step; with a total of 2185 observations.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=2f4bd8f5-2479-466a-9d0d-83cd3252a987">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[146]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">independent_array15</span><span class="p">))</span> <span class="c1"># 30 time-steps and 8 features per time-step; with a total of 2185 observations</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>(2185, 30, 8)
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=8277a31b-bfa6-4b9d-898d-a870f532c27b">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>Now, after my independent variables associated with independent array #15 has been gathered above and my dependent array has also been gathered above, I will train 9 models with the same architecture on the data, applying 5-fold stratified cross-validation to each model. For each of the 9 models, the epoch with the best performance (highest validation accuracy) will have its weights saved.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=b550720a-4acd-4969-a9e6-7cd35f7849da">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[147]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tensorflow.keras.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tensorflow.keras.layers</span><span class="w"> </span><span class="kn">import</span> <span class="n">LSTM</span><span class="p">,</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Dropout</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tensorflow.keras.callbacks</span><span class="w"> </span><span class="kn">import</span> <span class="n">ModelCheckpoint</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">StratifiedKFold</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="c1"># Define the LSTM classification model</span>
<span class="k">def</span><span class="w"> </span><span class="nf">create_lstm_classification</span><span class="p">(</span><span class="n">input_shape</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
    
    <span class="c1"># LSTM layers</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'tanh'</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">))</span>  <span class="c1"># Dropout to reduce overfitting</span>
    
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'tanh'</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>  <span class="c1"># Final LSTM layer</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">))</span>
    
    <span class="c1"># Dense output layer for binary classification</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'sigmoid'</span><span class="p">))</span>  <span class="c1"># Sigmoid for binary classification (probability)</span>
    
    <span class="c1"># Compile the model</span>
    <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">'adam'</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s1">'binary_crossentropy'</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'accuracy'</span><span class="p">])</span>  <span class="c1"># Binary cross-entropy for classification</span>
    
    <span class="k">return</span> <span class="n">model</span>

<span class="c1"># Define independent and dependent variables</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">independent_array15</span>  <span class="c1"># Shape: (890, 30, [4, 5, 6, or 8]) </span>
<span class="n">y</span> <span class="o">=</span> <span class="n">dependent_array</span>  <span class="c1"># Shape: (890,) (binary labels, 0 or 1)</span>

<span class="c1"># Initialize list to hold models and their weights</span>
<span class="n">models</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Loop for training 9 identical models</span>
<span class="k">for</span> <span class="n">model_num</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">9</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Training model </span><span class="si">{</span><span class="n">model_num</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">/9..."</span><span class="p">)</span>

    <span class="c1"># Initialize the KFold</span>
    <span class="n">kf</span> <span class="o">=</span> <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="p">(</span><span class="n">model_num</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>

    <span class="c1"># Create and compile the model once per model_num (outside of the KFold loop)</span>
    <span class="c1"># This model will be reused for each fold</span>
    <span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>  <span class="c1"># Define input shape based on your data</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">create_lstm_classification</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>

    <span class="c1"># Initialize an empty list for storing fold accuracies</span>
    <span class="n">fold_accuracies</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># Set up a ModelCheckpoint callback to save the model's weights when validation accuracy is improved</span>
    <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="sa">f</span><span class="s1">'model_</span><span class="si">{</span><span class="n">model_num</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s1">_best_weights.keras'</span><span class="p">,</span> 
                                 <span class="n">save_best_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
                                 <span class="n">monitor</span><span class="o">=</span><span class="s1">'val_accuracy'</span><span class="p">,</span> 
                                 <span class="n">mode</span><span class="o">=</span><span class="s1">'max'</span><span class="p">,</span> 
                                 <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">K_fold_counter</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="c1"># Perform stratified K-fold cross-validation</span>
    <span class="k">for</span> <span class="n">train_index</span><span class="p">,</span> <span class="n">val_index</span> <span class="ow">in</span> <span class="n">kf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Model: </span><span class="si">{</span><span class="n">model_num</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s1">; K-fold: </span><span class="si">{</span><span class="p">(</span><span class="n">K_fold_counter</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
        <span class="n">K_fold_counter</span> <span class="o">=</span> <span class="n">K_fold_counter</span> <span class="o">+</span> <span class="mi">1</span>
        
        <span class="n">X_train</span><span class="p">,</span> <span class="n">X_val</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">val_index</span><span class="p">]</span>
        <span class="n">y_train</span><span class="p">,</span> <span class="n">y_val</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">val_index</span><span class="p">]</span>

        <span class="c1"># Train the model</span>
        <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">),</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">checkpoint</span><span class="p">])</span>

    
    <span class="c1"># After training all folds, load the best weights into the model</span>
    <span class="c1">#best_model = create_lstm_classification(input_shape)</span>
    <span class="c1">#best_model.load_weights(f'model_{model_num + 1}_best_weights.keras')</span>

    <span class="c1"># Append the model to the list of trained models</span>
    <span class="c1">#models.append(best_model)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Training model 1/9...
Model: 1; K-fold: 1
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>
Epoch 1: val_accuracy improved from -inf to 0.55149, saving model to model_1_best_weights.keras

Epoch 2: val_accuracy did not improve from 0.55149

Epoch 3: val_accuracy improved from 0.55149 to 0.55378, saving model to model_1_best_weights.keras

Epoch 4: val_accuracy improved from 0.55378 to 0.56293, saving model to model_1_best_weights.keras

Epoch 5: val_accuracy did not improve from 0.56293

Epoch 6: val_accuracy did not improve from 0.56293

Epoch 7: val_accuracy improved from 0.56293 to 0.56979, saving model to model_1_best_weights.keras

Epoch 8: val_accuracy improved from 0.56979 to 0.57895, saving model to model_1_best_weights.keras

Epoch 9: val_accuracy did not improve from 0.57895

Epoch 10: val_accuracy did not improve from 0.57895
Model: 1; K-fold: 2

Epoch 1: val_accuracy did not improve from 0.57895

Epoch 2: val_accuracy did not improve from 0.57895

Epoch 3: val_accuracy did not improve from 0.57895

Epoch 4: val_accuracy did not improve from 0.57895

Epoch 5: val_accuracy improved from 0.57895 to 0.60870, saving model to model_1_best_weights.keras

Epoch 6: val_accuracy did not improve from 0.60870

Epoch 7: val_accuracy did not improve from 0.60870

Epoch 8: val_accuracy did not improve from 0.60870

Epoch 9: val_accuracy did not improve from 0.60870

Epoch 10: val_accuracy did not improve from 0.60870
Model: 1; K-fold: 3

Epoch 1: val_accuracy did not improve from 0.60870

Epoch 2: val_accuracy did not improve from 0.60870

Epoch 3: val_accuracy did not improve from 0.60870

Epoch 4: val_accuracy improved from 0.60870 to 0.61556, saving model to model_1_best_weights.keras

Epoch 5: val_accuracy improved from 0.61556 to 0.62700, saving model to model_1_best_weights.keras

Epoch 6: val_accuracy did not improve from 0.62700

Epoch 7: val_accuracy did not improve from 0.62700

Epoch 8: val_accuracy did not improve from 0.62700

Epoch 9: val_accuracy did not improve from 0.62700

Epoch 10: val_accuracy improved from 0.62700 to 0.63387, saving model to model_1_best_weights.keras
Model: 1; K-fold: 4

Epoch 1: val_accuracy improved from 0.63387 to 0.67048, saving model to model_1_best_weights.keras

Epoch 2: val_accuracy improved from 0.67048 to 0.67506, saving model to model_1_best_weights.keras

Epoch 3: val_accuracy improved from 0.67506 to 0.68192, saving model to model_1_best_weights.keras

Epoch 4: val_accuracy did not improve from 0.68192

Epoch 5: val_accuracy did not improve from 0.68192

Epoch 6: val_accuracy did not improve from 0.68192

Epoch 7: val_accuracy did not improve from 0.68192

Epoch 8: val_accuracy did not improve from 0.68192

Epoch 9: val_accuracy did not improve from 0.68192

Epoch 10: val_accuracy did not improve from 0.68192
Model: 1; K-fold: 5

Epoch 1: val_accuracy did not improve from 0.68192

Epoch 2: val_accuracy did not improve from 0.68192

Epoch 3: val_accuracy did not improve from 0.68192

Epoch 4: val_accuracy did not improve from 0.68192

Epoch 5: val_accuracy did not improve from 0.68192

Epoch 6: val_accuracy did not improve from 0.68192

Epoch 7: val_accuracy did not improve from 0.68192

Epoch 8: val_accuracy did not improve from 0.68192

Epoch 9: val_accuracy did not improve from 0.68192

Epoch 10: val_accuracy did not improve from 0.68192
Training model 2/9...
Model: 2; K-fold: 1
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>
Epoch 1: val_accuracy improved from -inf to 0.55835, saving model to model_2_best_weights.keras

Epoch 2: val_accuracy did not improve from 0.55835

Epoch 3: val_accuracy did not improve from 0.55835

Epoch 4: val_accuracy improved from 0.55835 to 0.56979, saving model to model_2_best_weights.keras

Epoch 5: val_accuracy did not improve from 0.56979

Epoch 6: val_accuracy did not improve from 0.56979

Epoch 7: val_accuracy did not improve from 0.56979

Epoch 8: val_accuracy did not improve from 0.56979

Epoch 9: val_accuracy did not improve from 0.56979

Epoch 10: val_accuracy did not improve from 0.56979
Model: 2; K-fold: 2

Epoch 1: val_accuracy improved from 0.56979 to 0.57895, saving model to model_2_best_weights.keras

Epoch 2: val_accuracy did not improve from 0.57895

Epoch 3: val_accuracy did not improve from 0.57895

Epoch 4: val_accuracy did not improve from 0.57895

Epoch 5: val_accuracy did not improve from 0.57895

Epoch 6: val_accuracy improved from 0.57895 to 0.59954, saving model to model_2_best_weights.keras

Epoch 7: val_accuracy did not improve from 0.59954

Epoch 8: val_accuracy did not improve from 0.59954

Epoch 9: val_accuracy did not improve from 0.59954

Epoch 10: val_accuracy did not improve from 0.59954
Model: 2; K-fold: 3

Epoch 1: val_accuracy improved from 0.59954 to 0.60183, saving model to model_2_best_weights.keras

Epoch 2: val_accuracy did not improve from 0.60183

Epoch 3: val_accuracy improved from 0.60183 to 0.60412, saving model to model_2_best_weights.keras

Epoch 4: val_accuracy did not improve from 0.60412

Epoch 5: val_accuracy did not improve from 0.60412

Epoch 6: val_accuracy did not improve from 0.60412

Epoch 7: val_accuracy did not improve from 0.60412

Epoch 8: val_accuracy improved from 0.60412 to 0.60870, saving model to model_2_best_weights.keras

Epoch 9: val_accuracy improved from 0.60870 to 0.61098, saving model to model_2_best_weights.keras

Epoch 10: val_accuracy did not improve from 0.61098
Model: 2; K-fold: 4

Epoch 1: val_accuracy did not improve from 0.61098

Epoch 2: val_accuracy did not improve from 0.61098

Epoch 3: val_accuracy did not improve from 0.61098

Epoch 4: val_accuracy did not improve from 0.61098

Epoch 5: val_accuracy did not improve from 0.61098

Epoch 6: val_accuracy did not improve from 0.61098

Epoch 7: val_accuracy did not improve from 0.61098

Epoch 8: val_accuracy improved from 0.61098 to 0.61785, saving model to model_2_best_weights.keras

Epoch 9: val_accuracy did not improve from 0.61785

Epoch 10: val_accuracy did not improve from 0.61785
Model: 2; K-fold: 5

Epoch 1: val_accuracy did not improve from 0.61785

Epoch 2: val_accuracy did not improve from 0.61785

Epoch 3: val_accuracy did not improve from 0.61785

Epoch 4: val_accuracy did not improve from 0.61785

Epoch 5: val_accuracy improved from 0.61785 to 0.62243, saving model to model_2_best_weights.keras

Epoch 6: val_accuracy did not improve from 0.62243

Epoch 7: val_accuracy did not improve from 0.62243

Epoch 8: val_accuracy did not improve from 0.62243

Epoch 9: val_accuracy did not improve from 0.62243

Epoch 10: val_accuracy did not improve from 0.62243
Training model 3/9...
Model: 3; K-fold: 1
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>
Epoch 1: val_accuracy improved from -inf to 0.46453, saving model to model_3_best_weights.keras

Epoch 2: val_accuracy improved from 0.46453 to 0.55378, saving model to model_3_best_weights.keras

Epoch 3: val_accuracy did not improve from 0.55378

Epoch 4: val_accuracy did not improve from 0.55378

Epoch 5: val_accuracy did not improve from 0.55378

Epoch 6: val_accuracy did not improve from 0.55378

Epoch 7: val_accuracy did not improve from 0.55378

Epoch 8: val_accuracy did not improve from 0.55378

Epoch 9: val_accuracy improved from 0.55378 to 0.55606, saving model to model_3_best_weights.keras

Epoch 10: val_accuracy did not improve from 0.55606
Model: 3; K-fold: 2

Epoch 1: val_accuracy improved from 0.55606 to 0.58352, saving model to model_3_best_weights.keras

Epoch 2: val_accuracy did not improve from 0.58352

Epoch 3: val_accuracy did not improve from 0.58352

Epoch 4: val_accuracy improved from 0.58352 to 0.59039, saving model to model_3_best_weights.keras

Epoch 5: val_accuracy improved from 0.59039 to 0.59725, saving model to model_3_best_weights.keras

Epoch 6: val_accuracy did not improve from 0.59725

Epoch 7: val_accuracy did not improve from 0.59725

Epoch 8: val_accuracy did not improve from 0.59725

Epoch 9: val_accuracy improved from 0.59725 to 0.61556, saving model to model_3_best_weights.keras

Epoch 10: val_accuracy did not improve from 0.61556
Model: 3; K-fold: 3

Epoch 1: val_accuracy did not improve from 0.61556

Epoch 2: val_accuracy did not improve from 0.61556

Epoch 3: val_accuracy did not improve from 0.61556

Epoch 4: val_accuracy did not improve from 0.61556

Epoch 5: val_accuracy did not improve from 0.61556

Epoch 6: val_accuracy did not improve from 0.61556

Epoch 7: val_accuracy did not improve from 0.61556

Epoch 8: val_accuracy did not improve from 0.61556

Epoch 9: val_accuracy did not improve from 0.61556

Epoch 10: val_accuracy did not improve from 0.61556
Model: 3; K-fold: 4

Epoch 1: val_accuracy improved from 0.61556 to 0.66590, saving model to model_3_best_weights.keras

Epoch 2: val_accuracy did not improve from 0.66590

Epoch 3: val_accuracy did not improve from 0.66590

Epoch 4: val_accuracy did not improve from 0.66590

Epoch 5: val_accuracy did not improve from 0.66590

Epoch 6: val_accuracy did not improve from 0.66590

Epoch 7: val_accuracy did not improve from 0.66590

Epoch 8: val_accuracy did not improve from 0.66590

Epoch 9: val_accuracy did not improve from 0.66590

Epoch 10: val_accuracy did not improve from 0.66590
Model: 3; K-fold: 5

Epoch 1: val_accuracy improved from 0.66590 to 0.68192, saving model to model_3_best_weights.keras

Epoch 2: val_accuracy improved from 0.68192 to 0.68879, saving model to model_3_best_weights.keras

Epoch 3: val_accuracy did not improve from 0.68879

Epoch 4: val_accuracy did not improve from 0.68879

Epoch 5: val_accuracy did not improve from 0.68879

Epoch 6: val_accuracy did not improve from 0.68879

Epoch 7: val_accuracy did not improve from 0.68879

Epoch 8: val_accuracy did not improve from 0.68879

Epoch 9: val_accuracy did not improve from 0.68879

Epoch 10: val_accuracy did not improve from 0.68879
Training model 4/9...
Model: 4; K-fold: 1
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>
Epoch 1: val_accuracy improved from -inf to 0.57437, saving model to model_4_best_weights.keras

Epoch 2: val_accuracy did not improve from 0.57437

Epoch 3: val_accuracy did not improve from 0.57437

Epoch 4: val_accuracy improved from 0.57437 to 0.58810, saving model to model_4_best_weights.keras

Epoch 5: val_accuracy did not improve from 0.58810

Epoch 6: val_accuracy did not improve from 0.58810

Epoch 7: val_accuracy did not improve from 0.58810

Epoch 8: val_accuracy did not improve from 0.58810

Epoch 9: val_accuracy did not improve from 0.58810

Epoch 10: val_accuracy did not improve from 0.58810
Model: 4; K-fold: 2

Epoch 1: val_accuracy did not improve from 0.58810

Epoch 2: val_accuracy did not improve from 0.58810

Epoch 3: val_accuracy did not improve from 0.58810

Epoch 4: val_accuracy did not improve from 0.58810

Epoch 5: val_accuracy did not improve from 0.58810

Epoch 6: val_accuracy improved from 0.58810 to 0.60641, saving model to model_4_best_weights.keras

Epoch 7: val_accuracy did not improve from 0.60641

Epoch 8: val_accuracy did not improve from 0.60641

Epoch 9: val_accuracy did not improve from 0.60641

Epoch 10: val_accuracy did not improve from 0.60641
Model: 4; K-fold: 3

Epoch 1: val_accuracy did not improve from 0.60641

Epoch 2: val_accuracy did not improve from 0.60641

Epoch 3: val_accuracy did not improve from 0.60641

Epoch 4: val_accuracy did not improve from 0.60641

Epoch 5: val_accuracy did not improve from 0.60641

Epoch 6: val_accuracy did not improve from 0.60641

Epoch 7: val_accuracy did not improve from 0.60641

Epoch 8: val_accuracy did not improve from 0.60641

Epoch 9: val_accuracy did not improve from 0.60641

Epoch 10: val_accuracy did not improve from 0.60641
Model: 4; K-fold: 4

Epoch 1: val_accuracy did not improve from 0.60641

Epoch 2: val_accuracy improved from 0.60641 to 0.61327, saving model to model_4_best_weights.keras

Epoch 3: val_accuracy improved from 0.61327 to 0.62014, saving model to model_4_best_weights.keras

Epoch 4: val_accuracy did not improve from 0.62014

Epoch 5: val_accuracy did not improve from 0.62014

Epoch 6: val_accuracy did not improve from 0.62014

Epoch 7: val_accuracy did not improve from 0.62014

Epoch 8: val_accuracy did not improve from 0.62014

Epoch 9: val_accuracy did not improve from 0.62014

Epoch 10: val_accuracy did not improve from 0.62014
Model: 4; K-fold: 5

Epoch 1: val_accuracy did not improve from 0.62014

Epoch 2: val_accuracy improved from 0.62014 to 0.66133, saving model to model_4_best_weights.keras

Epoch 3: val_accuracy did not improve from 0.66133

Epoch 4: val_accuracy did not improve from 0.66133

Epoch 5: val_accuracy did not improve from 0.66133

Epoch 6: val_accuracy did not improve from 0.66133

Epoch 7: val_accuracy did not improve from 0.66133

Epoch 8: val_accuracy did not improve from 0.66133

Epoch 9: val_accuracy did not improve from 0.66133

Epoch 10: val_accuracy did not improve from 0.66133
Training model 5/9...
Model: 5; K-fold: 1
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>
Epoch 1: val_accuracy improved from -inf to 0.56751, saving model to model_5_best_weights.keras

Epoch 2: val_accuracy did not improve from 0.56751

Epoch 3: val_accuracy did not improve from 0.56751

Epoch 4: val_accuracy did not improve from 0.56751

Epoch 5: val_accuracy did not improve from 0.56751

Epoch 6: val_accuracy did not improve from 0.56751

Epoch 7: val_accuracy did not improve from 0.56751

Epoch 8: val_accuracy did not improve from 0.56751

Epoch 9: val_accuracy did not improve from 0.56751

Epoch 10: val_accuracy did not improve from 0.56751
Model: 5; K-fold: 2

Epoch 1: val_accuracy did not improve from 0.56751

Epoch 2: val_accuracy did not improve from 0.56751

Epoch 3: val_accuracy did not improve from 0.56751

Epoch 4: val_accuracy did not improve from 0.56751

Epoch 5: val_accuracy did not improve from 0.56751

Epoch 6: val_accuracy did not improve from 0.56751

Epoch 7: val_accuracy did not improve from 0.56751

Epoch 8: val_accuracy did not improve from 0.56751

Epoch 9: val_accuracy did not improve from 0.56751

Epoch 10: val_accuracy did not improve from 0.56751
Model: 5; K-fold: 3

Epoch 1: val_accuracy improved from 0.56751 to 0.57666, saving model to model_5_best_weights.keras

Epoch 2: val_accuracy improved from 0.57666 to 0.57895, saving model to model_5_best_weights.keras

Epoch 3: val_accuracy improved from 0.57895 to 0.58352, saving model to model_5_best_weights.keras

Epoch 4: val_accuracy did not improve from 0.58352

Epoch 5: val_accuracy did not improve from 0.58352

Epoch 6: val_accuracy did not improve from 0.58352

Epoch 7: val_accuracy did not improve from 0.58352

Epoch 8: val_accuracy did not improve from 0.58352

Epoch 9: val_accuracy improved from 0.58352 to 0.59497, saving model to model_5_best_weights.keras

Epoch 10: val_accuracy did not improve from 0.59497
Model: 5; K-fold: 4

Epoch 1: val_accuracy did not improve from 0.59497

Epoch 2: val_accuracy did not improve from 0.59497

Epoch 3: val_accuracy did not improve from 0.59497

Epoch 4: val_accuracy did not improve from 0.59497

Epoch 5: val_accuracy did not improve from 0.59497

Epoch 6: val_accuracy did not improve from 0.59497

Epoch 7: val_accuracy improved from 0.59497 to 0.60183, saving model to model_5_best_weights.keras

Epoch 8: val_accuracy improved from 0.60183 to 0.61098, saving model to model_5_best_weights.keras

Epoch 9: val_accuracy did not improve from 0.61098

Epoch 10: val_accuracy did not improve from 0.61098
Model: 5; K-fold: 5

Epoch 1: val_accuracy did not improve from 0.61098

Epoch 2: val_accuracy improved from 0.61098 to 0.63158, saving model to model_5_best_weights.keras

Epoch 3: val_accuracy did not improve from 0.63158

Epoch 4: val_accuracy did not improve from 0.63158

Epoch 5: val_accuracy improved from 0.63158 to 0.63616, saving model to model_5_best_weights.keras

Epoch 6: val_accuracy did not improve from 0.63616

Epoch 7: val_accuracy did not improve from 0.63616

Epoch 8: val_accuracy did not improve from 0.63616

Epoch 9: val_accuracy did not improve from 0.63616

Epoch 10: val_accuracy did not improve from 0.63616
Training model 6/9...
Model: 6; K-fold: 1
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>
Epoch 1: val_accuracy improved from -inf to 0.50114, saving model to model_6_best_weights.keras

Epoch 2: val_accuracy improved from 0.50114 to 0.54233, saving model to model_6_best_weights.keras

Epoch 3: val_accuracy did not improve from 0.54233

Epoch 4: val_accuracy did not improve from 0.54233

Epoch 5: val_accuracy did not improve from 0.54233

Epoch 6: val_accuracy did not improve from 0.54233

Epoch 7: val_accuracy did not improve from 0.54233

Epoch 8: val_accuracy did not improve from 0.54233

Epoch 9: val_accuracy did not improve from 0.54233

Epoch 10: val_accuracy did not improve from 0.54233
Model: 6; K-fold: 2

Epoch 1: val_accuracy improved from 0.54233 to 0.60412, saving model to model_6_best_weights.keras

Epoch 2: val_accuracy did not improve from 0.60412

Epoch 3: val_accuracy did not improve from 0.60412

Epoch 4: val_accuracy did not improve from 0.60412

Epoch 5: val_accuracy did not improve from 0.60412

Epoch 6: val_accuracy improved from 0.60412 to 0.60870, saving model to model_6_best_weights.keras

Epoch 7: val_accuracy improved from 0.60870 to 0.62014, saving model to model_6_best_weights.keras

Epoch 8: val_accuracy improved from 0.62014 to 0.62471, saving model to model_6_best_weights.keras

Epoch 9: val_accuracy did not improve from 0.62471

Epoch 10: val_accuracy did not improve from 0.62471
Model: 6; K-fold: 3

Epoch 1: val_accuracy did not improve from 0.62471

Epoch 2: val_accuracy did not improve from 0.62471

Epoch 3: val_accuracy did not improve from 0.62471

Epoch 4: val_accuracy did not improve from 0.62471

Epoch 5: val_accuracy did not improve from 0.62471

Epoch 6: val_accuracy did not improve from 0.62471

Epoch 7: val_accuracy did not improve from 0.62471

Epoch 8: val_accuracy did not improve from 0.62471

Epoch 9: val_accuracy did not improve from 0.62471

Epoch 10: val_accuracy did not improve from 0.62471
Model: 6; K-fold: 4

Epoch 1: val_accuracy improved from 0.62471 to 0.62929, saving model to model_6_best_weights.keras

Epoch 2: val_accuracy improved from 0.62929 to 0.65675, saving model to model_6_best_weights.keras

Epoch 3: val_accuracy did not improve from 0.65675

Epoch 4: val_accuracy did not improve from 0.65675

Epoch 5: val_accuracy did not improve from 0.65675

Epoch 6: val_accuracy did not improve from 0.65675

Epoch 7: val_accuracy did not improve from 0.65675

Epoch 8: val_accuracy did not improve from 0.65675

Epoch 9: val_accuracy did not improve from 0.65675

Epoch 10: val_accuracy did not improve from 0.65675
Model: 6; K-fold: 5

Epoch 1: val_accuracy did not improve from 0.65675

Epoch 2: val_accuracy did not improve from 0.65675

Epoch 3: val_accuracy did not improve from 0.65675

Epoch 4: val_accuracy did not improve from 0.65675

Epoch 5: val_accuracy did not improve from 0.65675

Epoch 6: val_accuracy did not improve from 0.65675

Epoch 7: val_accuracy did not improve from 0.65675

Epoch 8: val_accuracy did not improve from 0.65675

Epoch 9: val_accuracy did not improve from 0.65675

Epoch 10: val_accuracy did not improve from 0.65675
Training model 7/9...
Model: 7; K-fold: 1
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>
Epoch 1: val_accuracy improved from -inf to 0.54920, saving model to model_7_best_weights.keras

Epoch 2: val_accuracy improved from 0.54920 to 0.55378, saving model to model_7_best_weights.keras

Epoch 3: val_accuracy improved from 0.55378 to 0.57895, saving model to model_7_best_weights.keras

Epoch 4: val_accuracy did not improve from 0.57895

Epoch 5: val_accuracy did not improve from 0.57895

Epoch 6: val_accuracy did not improve from 0.57895

Epoch 7: val_accuracy did not improve from 0.57895

Epoch 8: val_accuracy did not improve from 0.57895

Epoch 9: val_accuracy did not improve from 0.57895

Epoch 10: val_accuracy did not improve from 0.57895
Model: 7; K-fold: 2

Epoch 1: val_accuracy improved from 0.57895 to 0.58810, saving model to model_7_best_weights.keras

Epoch 2: val_accuracy did not improve from 0.58810

Epoch 3: val_accuracy did not improve from 0.58810

Epoch 4: val_accuracy improved from 0.58810 to 0.59039, saving model to model_7_best_weights.keras

Epoch 5: val_accuracy did not improve from 0.59039

Epoch 6: val_accuracy improved from 0.59039 to 0.59954, saving model to model_7_best_weights.keras

Epoch 7: val_accuracy did not improve from 0.59954

Epoch 8: val_accuracy did not improve from 0.59954

Epoch 9: val_accuracy did not improve from 0.59954

Epoch 10: val_accuracy did not improve from 0.59954
Model: 7; K-fold: 3

Epoch 1: val_accuracy did not improve from 0.59954

Epoch 2: val_accuracy improved from 0.59954 to 0.64073, saving model to model_7_best_weights.keras

Epoch 3: val_accuracy did not improve from 0.64073

Epoch 4: val_accuracy did not improve from 0.64073

Epoch 5: val_accuracy did not improve from 0.64073

Epoch 6: val_accuracy did not improve from 0.64073

Epoch 7: val_accuracy did not improve from 0.64073

Epoch 8: val_accuracy did not improve from 0.64073

Epoch 9: val_accuracy did not improve from 0.64073

Epoch 10: val_accuracy did not improve from 0.64073
Model: 7; K-fold: 4

Epoch 1: val_accuracy improved from 0.64073 to 0.64531, saving model to model_7_best_weights.keras

Epoch 2: val_accuracy did not improve from 0.64531

Epoch 3: val_accuracy did not improve from 0.64531

Epoch 4: val_accuracy improved from 0.64531 to 0.65446, saving model to model_7_best_weights.keras

Epoch 5: val_accuracy improved from 0.65446 to 0.66133, saving model to model_7_best_weights.keras

Epoch 6: val_accuracy did not improve from 0.66133

Epoch 7: val_accuracy improved from 0.66133 to 0.67048, saving model to model_7_best_weights.keras

Epoch 8: val_accuracy did not improve from 0.67048

Epoch 9: val_accuracy did not improve from 0.67048

Epoch 10: val_accuracy did not improve from 0.67048
Model: 7; K-fold: 5

Epoch 1: val_accuracy did not improve from 0.67048

Epoch 2: val_accuracy improved from 0.67048 to 0.68879, saving model to model_7_best_weights.keras

Epoch 3: val_accuracy did not improve from 0.68879

Epoch 4: val_accuracy did not improve from 0.68879

Epoch 5: val_accuracy did not improve from 0.68879

Epoch 6: val_accuracy did not improve from 0.68879

Epoch 7: val_accuracy did not improve from 0.68879

Epoch 8: val_accuracy did not improve from 0.68879

Epoch 9: val_accuracy did not improve from 0.68879

Epoch 10: val_accuracy did not improve from 0.68879
Training model 8/9...
Model: 8; K-fold: 1
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>
Epoch 1: val_accuracy improved from -inf to 0.53547, saving model to model_8_best_weights.keras

Epoch 2: val_accuracy improved from 0.53547 to 0.54233, saving model to model_8_best_weights.keras

Epoch 3: val_accuracy did not improve from 0.54233

Epoch 4: val_accuracy improved from 0.54233 to 0.55835, saving model to model_8_best_weights.keras

Epoch 5: val_accuracy improved from 0.55835 to 0.58352, saving model to model_8_best_weights.keras

Epoch 6: val_accuracy did not improve from 0.58352

Epoch 7: val_accuracy did not improve from 0.58352

Epoch 8: val_accuracy did not improve from 0.58352

Epoch 9: val_accuracy did not improve from 0.58352

Epoch 10: val_accuracy did not improve from 0.58352
Model: 8; K-fold: 2

Epoch 1: val_accuracy improved from 0.58352 to 0.59725, saving model to model_8_best_weights.keras

Epoch 2: val_accuracy did not improve from 0.59725

Epoch 3: val_accuracy did not improve from 0.59725

Epoch 4: val_accuracy did not improve from 0.59725

Epoch 5: val_accuracy did not improve from 0.59725

Epoch 6: val_accuracy did not improve from 0.59725

Epoch 7: val_accuracy did not improve from 0.59725

Epoch 8: val_accuracy did not improve from 0.59725

Epoch 9: val_accuracy did not improve from 0.59725

Epoch 10: val_accuracy improved from 0.59725 to 0.60641, saving model to model_8_best_weights.keras
Model: 8; K-fold: 3

Epoch 1: val_accuracy did not improve from 0.60641

Epoch 2: val_accuracy did not improve from 0.60641

Epoch 3: val_accuracy did not improve from 0.60641

Epoch 4: val_accuracy did not improve from 0.60641

Epoch 5: val_accuracy did not improve from 0.60641

Epoch 6: val_accuracy did not improve from 0.60641

Epoch 7: val_accuracy did not improve from 0.60641

Epoch 8: val_accuracy did not improve from 0.60641

Epoch 9: val_accuracy did not improve from 0.60641

Epoch 10: val_accuracy did not improve from 0.60641
Model: 8; K-fold: 4

Epoch 1: val_accuracy improved from 0.60641 to 0.62014, saving model to model_8_best_weights.keras

Epoch 2: val_accuracy improved from 0.62014 to 0.63616, saving model to model_8_best_weights.keras

Epoch 3: val_accuracy improved from 0.63616 to 0.63844, saving model to model_8_best_weights.keras

Epoch 4: val_accuracy did not improve from 0.63844

Epoch 5: val_accuracy did not improve from 0.63844

Epoch 6: val_accuracy improved from 0.63844 to 0.64073, saving model to model_8_best_weights.keras

Epoch 7: val_accuracy improved from 0.64073 to 0.66362, saving model to model_8_best_weights.keras

Epoch 8: val_accuracy did not improve from 0.66362

Epoch 9: val_accuracy did not improve from 0.66362

Epoch 10: val_accuracy improved from 0.66362 to 0.66819, saving model to model_8_best_weights.keras
Model: 8; K-fold: 5

Epoch 1: val_accuracy improved from 0.66819 to 0.68650, saving model to model_8_best_weights.keras

Epoch 2: val_accuracy improved from 0.68650 to 0.69108, saving model to model_8_best_weights.keras

Epoch 3: val_accuracy did not improve from 0.69108

Epoch 4: val_accuracy improved from 0.69108 to 0.69565, saving model to model_8_best_weights.keras

Epoch 5: val_accuracy did not improve from 0.69565

Epoch 6: val_accuracy did not improve from 0.69565

Epoch 7: val_accuracy did not improve from 0.69565

Epoch 8: val_accuracy did not improve from 0.69565

Epoch 9: val_accuracy did not improve from 0.69565

Epoch 10: val_accuracy did not improve from 0.69565
Training model 9/9...
Model: 9; K-fold: 1
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>
Epoch 1: val_accuracy improved from -inf to 0.55606, saving model to model_9_best_weights.keras

Epoch 2: val_accuracy improved from 0.55606 to 0.57437, saving model to model_9_best_weights.keras

Epoch 3: val_accuracy did not improve from 0.57437

Epoch 4: val_accuracy did not improve from 0.57437

Epoch 5: val_accuracy did not improve from 0.57437

Epoch 6: val_accuracy did not improve from 0.57437

Epoch 7: val_accuracy did not improve from 0.57437

Epoch 8: val_accuracy did not improve from 0.57437

Epoch 9: val_accuracy improved from 0.57437 to 0.57895, saving model to model_9_best_weights.keras

Epoch 10: val_accuracy did not improve from 0.57895
Model: 9; K-fold: 2

Epoch 1: val_accuracy did not improve from 0.57895

Epoch 2: val_accuracy did not improve from 0.57895

Epoch 3: val_accuracy did not improve from 0.57895

Epoch 4: val_accuracy did not improve from 0.57895

Epoch 5: val_accuracy did not improve from 0.57895

Epoch 6: val_accuracy did not improve from 0.57895

Epoch 7: val_accuracy did not improve from 0.57895

Epoch 8: val_accuracy did not improve from 0.57895

Epoch 9: val_accuracy did not improve from 0.57895

Epoch 10: val_accuracy did not improve from 0.57895
Model: 9; K-fold: 3

Epoch 1: val_accuracy improved from 0.57895 to 0.60641, saving model to model_9_best_weights.keras

Epoch 2: val_accuracy did not improve from 0.60641

Epoch 3: val_accuracy did not improve from 0.60641

Epoch 4: val_accuracy did not improve from 0.60641

Epoch 5: val_accuracy did not improve from 0.60641

Epoch 6: val_accuracy did not improve from 0.60641

Epoch 7: val_accuracy did not improve from 0.60641

Epoch 8: val_accuracy did not improve from 0.60641

Epoch 9: val_accuracy did not improve from 0.60641

Epoch 10: val_accuracy improved from 0.60641 to 0.61098, saving model to model_9_best_weights.keras
Model: 9; K-fold: 4

Epoch 1: val_accuracy improved from 0.61098 to 0.62243, saving model to model_9_best_weights.keras

Epoch 2: val_accuracy did not improve from 0.62243

Epoch 3: val_accuracy did not improve from 0.62243

Epoch 4: val_accuracy did not improve from 0.62243

Epoch 5: val_accuracy did not improve from 0.62243

Epoch 6: val_accuracy did not improve from 0.62243

Epoch 7: val_accuracy improved from 0.62243 to 0.62700, saving model to model_9_best_weights.keras

Epoch 8: val_accuracy did not improve from 0.62700

Epoch 9: val_accuracy did not improve from 0.62700

Epoch 10: val_accuracy did not improve from 0.62700
Model: 9; K-fold: 5

Epoch 1: val_accuracy improved from 0.62700 to 0.72540, saving model to model_9_best_weights.keras

Epoch 2: val_accuracy did not improve from 0.72540

Epoch 3: val_accuracy did not improve from 0.72540

Epoch 4: val_accuracy did not improve from 0.72540

Epoch 5: val_accuracy did not improve from 0.72540

Epoch 6: val_accuracy did not improve from 0.72540

Epoch 7: val_accuracy did not improve from 0.72540

Epoch 8: val_accuracy did not improve from 0.72540

Epoch 9: val_accuracy did not improve from 0.72540

Epoch 10: val_accuracy did not improve from 0.72540
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=c1ab6cb1-2cc2-4bff-9ea5-935f9d794d03">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>Now that I have trained my 9 models of the same architecture, using the best performing set of independent variables (independent array #15), I will now evaluate the accuracy of the ensemble model on a separate test dataset to assess the effectiveness of this combined approach. This ensemble method aims to leverage the strengths of each individual model and provide a more robust prediction.</p>
<p>I am going to create a test dataset by creating a list of new random values to evaluate the ensemble model on. I will do this by changing the random seed. Previously it was 6, now it is 7. What this does is ensure that the new dataset is generated deterministically with a different sequence of random numbers, providing a new set of test data. This change in the random seed helps evaluate how the ensemble model performs on different variations of the test data, enabling better validation of its generalization capability.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=f89c6bd5-d68d-42f6-9faa-781d1f673ae5">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[22]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Specify the number of "Yes" values you want, may show up as less during training due to location of the "Yes" value, as need at least 30 days</span>
<span class="c1">#of data for the 30-day sequence, or if the future closing price is not available (only have data to 2/14)</span>
<span class="n">num_yes</span> <span class="o">=</span> <span class="mi">2200</span>

<span class="c1"># Create a list of "Yes" and "No" values</span>
<span class="n">yes_no_list</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"Yes"</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_yes</span> <span class="o">+</span> <span class="p">[</span><span class="s2">"No"</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">finance_df</span><span class="p">)</span> <span class="o">-</span> <span class="n">num_yes</span><span class="p">)</span>

<span class="c1">#set seed for reproducibility; previously this seed was set to 6, so it is going to create an entirely new list of random "Yes" and "No" values</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span> 

<span class="c1"># Shuffle the list to randomize the order</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">yes_no_list</span><span class="p">)</span>

<span class="c1"># Create a new column in the dataframe; we already have a column 'Random_Yes_No_2' which was used to train the occurence of a random pattern</span>
<span class="n">finance_df</span><span class="p">[</span><span class="s1">'Random_Yes_No_3'</span><span class="p">]</span> <span class="o">=</span> <span class="n">yes_no_list</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=bfa5a777-c54a-44ab-bc34-0ea8725738d0">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[23]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1">#Subset data frame for desired pattern</span>
<span class="n">pattern_df</span> <span class="o">=</span> <span class="n">finance_df</span><span class="p">[</span><span class="n">finance_df</span><span class="p">[</span><span class="s1">'Random_Yes_No_3'</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"Yes"</span><span class="p">]</span>

<span class="c1">#How many days after the pattern is identified to use for the dependent variable</span>
<span class="n">days_out</span> <span class="o">=</span> <span class="mi">10</span>

<span class="c1">#What percent increase from the current price is considered a positive class. For example 1.01 = 1% increase; 100 * 1.01 = 101. So if original</span>
<span class="c1">#price is $100, anything greater than $101 is considered a positive class.</span>
<span class="n">pct_increase</span> <span class="o">=</span> <span class="mf">1.01</span>

<span class="c1">#Gather independent variables</span>
<span class="n">independent_list15</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1">#gather dependent variables</span>
<span class="n">dependent_list</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">pattern_index</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">pattern_df</span><span class="p">[</span><span class="s2">"Row_index"</span><span class="p">])</span>
<span class="c1">#pattern_index = [60, 62]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">pattern_index</span><span class="p">:</span>
    <span class="c1">#if (i == 62):</span>
    <span class="c1">#    break</span>
    
    <span class="c1">#unable to get 30 days worth of data if index is less than 56, because previously removed first 26 observations</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="mi">56</span><span class="p">):</span>
        <span class="k">continue</span>

    <span class="c1">#get 30 days worth of data to gather data for indpendent variables</span>
    <span class="n">subset_df</span> <span class="o">=</span> <span class="n">finance_df</span><span class="p">[(</span><span class="n">finance_df</span><span class="p">[</span><span class="s2">"Row_index"</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="n">i</span> <span class="o">-</span> <span class="mi">29</span><span class="p">))</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">finance_df</span><span class="p">[</span><span class="s2">"Row_index"</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="p">(</span><span class="n">i</span><span class="p">))]</span>
    <span class="c1">#subset_df = finance_df[(finance_df["Row_index"] &gt;= (i - 13)) &amp; (finance_df["Row_index"] &lt;= (i))]</span>
    
    <span class="c1">#Get day after data to gather closing price for dependent variable</span>
    <span class="n">dependent_df</span> <span class="o">=</span> <span class="n">finance_df</span><span class="p">[</span><span class="n">finance_df</span><span class="p">[</span><span class="s2">"Row_index"</span><span class="p">]</span> <span class="o">==</span> <span class="p">(</span><span class="n">i</span><span class="p">)]</span>
    <span class="n">dependent2_df</span> <span class="o">=</span> <span class="n">finance_df</span><span class="p">[</span><span class="n">finance_df</span><span class="p">[</span><span class="s2">"Row_index"</span><span class="p">]</span> <span class="o">==</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="n">days_out</span><span class="p">)]</span>
    
    <span class="n">temp_list15</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1">#append temp_list to independent_list</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">dependent2_df</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span> <span class="c1">#dependent2_df may have length of zero as it is a future date, data may not be available</span>
    

        <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">subset_df</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
                
                <span class="n">test_array15</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Open'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Close'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_High'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Normalized_Low'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'RSI'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MFI'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'MACD'</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">'Signal_Line'</span><span class="p">]])</span>
            
                <span class="n">temp_list15</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_array15</span><span class="p">)</span>
                
        <span class="n">independent_list15</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_list15</span><span class="p">)</span>
    
        <span class="k">if</span> <span class="p">(</span><span class="n">dependent2_df</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">dependent_df</span><span class="p">[</span><span class="s1">'Close'</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">pct_increase</span><span class="p">):</span>
            <span class="n">dependent_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">dependent_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">independent_array15</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">independent_list15</span><span class="p">)</span>
<span class="n">dependent_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dependent_list</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">independent_array15</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">dependent_array</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=fe1ff11c-d86b-4a72-8997-f02f2494250e">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>After collecting my new set of independent and dependent variables above based on the new random split, I am ready to load my previously trained models and perform ensemble learning using those 9 models as coded below.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=6d9ebc27-9cd9-410d-bb6d-96afddff2181">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[29]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tensorflow.keras.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tensorflow.keras.layers</span><span class="w"> </span><span class="kn">import</span> <span class="n">LSTM</span><span class="p">,</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Dropout</span>

<span class="c1"># Assuming you already have your trained models and their weights</span>
<span class="n">models</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1">#Define the LSTM classification model; already defined it previously, so don't have to but for testing purposes if I skip chunks, I will include it here</span>
<span class="k">def</span><span class="w"> </span><span class="nf">create_lstm_classification</span><span class="p">(</span><span class="n">input_shape</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
    
    <span class="c1"># LSTM layers</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'tanh'</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">))</span>  <span class="c1"># Dropout to reduce overfitting</span>
    
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'tanh'</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>  <span class="c1"># Final LSTM layer</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">))</span>
    
    <span class="c1"># Dense output layer for binary classification</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'sigmoid'</span><span class="p">))</span>  <span class="c1"># Sigmoid for binary classification (probability)</span>
    
    <span class="c1"># Compile the model</span>
    <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">'adam'</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s1">'binary_crossentropy'</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'accuracy'</span><span class="p">])</span>  <span class="c1"># Binary cross-entropy for classification</span>
    
    <span class="k">return</span> <span class="n">model</span>

<span class="c1">#Specify input shape</span>
<span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span> 

<span class="c1"># Load models and weights (adjust the range to include all your models)</span>
<span class="k">for</span> <span class="n">model_num</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">9</span><span class="p">):</span>  <span class="c1"># Change this range to include more models if necessary</span>
    <span class="n">best_model</span> <span class="o">=</span> <span class="n">create_lstm_classification</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
    <span class="n">best_model</span><span class="o">.</span><span class="n">load_weights</span><span class="p">(</span><span class="sa">f</span><span class="s1">'model_</span><span class="si">{</span><span class="n">model_num</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s1">_best_weights.keras'</span><span class="p">)</span>
    <span class="n">models</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">best_model</span><span class="p">)</span>

<span class="c1"># Split data for final training/testing; only using X_test_final for predicting and y_test_final for comparing my predictions</span>
<span class="n">X_train_final</span><span class="p">,</span> <span class="n">X_test_final</span><span class="p">,</span> <span class="n">y_train_final</span><span class="p">,</span> <span class="n">y_test_final</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>

<span class="c1"># Initialize a list to store binary predictions from each model</span>
<span class="n">ensemble_predictions</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1">#initialize to get votes from each model</span>
<span class="n">votes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_test_final</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>

<span class="c1"># Generate predictions from each model</span>
<span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">models</span><span class="p">:</span>
    <span class="c1"># Get the predictions from the model (probabilities)</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_final</span><span class="p">)</span>
    
    <span class="c1"># Convert predictions to binary (0 or 1)</span>
    <span class="n">binary_predictions</span> <span class="o">=</span> <span class="p">(</span><span class="n">predictions</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>  <span class="c1"># Convert to 0 or 1</span>

    <span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">binary_predictions</span><span class="p">:</span> <span class="c1">#for each value in the model's predictions, if the value is predicted 0, then no votes added, if 1, then add 1 vote</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">value</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
            <span class="n">votes</span><span class="p">[</span><span class="n">counter</span><span class="p">]</span> <span class="o">=</span> <span class="n">votes</span><span class="p">[</span><span class="n">counter</span><span class="p">]</span> <span class="o">+</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">votes</span><span class="p">[</span><span class="n">counter</span><span class="p">]</span> <span class="o">=</span> <span class="n">votes</span><span class="p">[</span><span class="n">counter</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="n">counter</span> <span class="o">=</span> <span class="n">counter</span> <span class="o">+</span> <span class="mi">1</span>

<span class="c1">#because there are 9 different models, if the vote is greater than 5, then it is a positive class</span>
<span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">vote</span> <span class="ow">in</span> <span class="n">votes</span><span class="p">:</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">vote</span> <span class="o">&gt;=</span> <span class="mi">5</span><span class="p">):</span>
        <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Create a pandas Series for comparison</span>
<span class="n">comparison</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">results</span> <span class="o">==</span> <span class="n">y_test_final</span><span class="p">)</span>

<span class="c1"># Use value_counts() to count True/False occurrences</span>
<span class="n">counts</span> <span class="o">=</span> <span class="n">comparison</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
<span class="n">num_true</span> <span class="o">=</span> <span class="n">counts</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># 0 if True is not found</span>
<span class="n">num_false</span> <span class="o">=</span> <span class="n">counts</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="kc">False</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># 0 if False is not found</span>

<span class="c1">#Get total amount of observations in training/validation dataset</span>
<span class="n">counts_2</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">dependent_array</span><span class="p">)</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
<span class="n">total_true_labels</span> <span class="o">=</span> <span class="n">counts_2</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># Will return 0 if no '1' exists</span>
<span class="n">total_false_labels</span> <span class="o">=</span> <span class="n">counts_2</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># Will return 0 if no '0' exists</span>

<span class="c1"># Convert results and y_test_final to numpy arrays for easier handling if they aren't already</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
<span class="n">y_test_final</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y_test_final</span><span class="p">)</span>

<span class="c1"># Count the number of true (1) and false (0) labels</span>
<span class="n">label_counts</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">y_test_final</span><span class="p">)</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>

<span class="c1"># Get counts of True (1) and False (0) specifically for the test set</span>
<span class="n">total_true_labels_test</span> <span class="o">=</span> <span class="n">label_counts</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># Will return 0 if no '1' exists</span>
<span class="n">total_false_labels_test</span> <span class="o">=</span> <span class="n">label_counts</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># Will return 0 if no '0' exists</span>

<span class="c1">#compute accuracy and most_frequent_class_test_pct</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">num_true</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_test_final</span><span class="p">))</span>
<span class="k">if</span> <span class="p">(</span><span class="n">total_true_labels_test</span> <span class="o">&gt;</span> <span class="n">total_false_labels_test</span><span class="p">):</span>
    <span class="n">most_frequent_class_test_pct</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">total_true_labels_test</span><span class="p">)</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_test_final</span><span class="p">))</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">most_frequent_class_test_pct</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">total_false_labels_test</span><span class="p">)</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_test_final</span><span class="p">))</span>


<span class="c1">#print out final results statements</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Number of total observations in entire dependent array dataset: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">dependent_array</span><span class="p">)</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Number of actual false labels in entire dependent array dataset: </span><span class="si">{</span><span class="n">total_false_labels</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Number of actual true labels in entire dependent array dataset: </span><span class="si">{</span><span class="n">total_true_labels</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Number of total observations in test dataset: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">y_test_final</span><span class="p">)</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Number of incorrect predictions on test dataset: </span><span class="si">{</span><span class="n">num_false</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Number of correct predictions on test dataset: </span><span class="si">{</span><span class="n">num_true</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Prediction accuracy on test dataset: </span><span class="si">{</span><span class="n">accuracy</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Number of actual false labels in test dataset: </span><span class="si">{</span><span class="n">total_false_labels_test</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Number of actual true labels test dataset: </span><span class="si">{</span><span class="n">total_true_labels_test</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Majority class label percent in test dataset: </span><span class="si">{</span><span class="n">most_frequent_class_test_pct</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\layers\rnn\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
C:\Users\james\Python Environments\myflaskenv\Lib\site-packages\keras\src\saving\saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 18 variables. 
  saveable.load_own_variables(weights_store.get(inner_path))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre><span class="ansi-bold">14/14</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 20ms/step 
<span class="ansi-bold">14/14</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 19ms/step 
<span class="ansi-bold">14/14</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 18ms/step 
<span class="ansi-bold">14/14</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 18ms/step 
<span class="ansi-bold">14/14</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 19ms/step 
<span class="ansi-bold">14/14</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 26ms/step 
<span class="ansi-bold">14/14</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 18ms/step 
<span class="ansi-bold">14/14</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">0s</span> 18ms/step 
<span class="ansi-bold">14/14</span> <span class="ansi-green-fg"></span> <span class="ansi-bold">1s</span> 18ms/step 
Number of total observations in entire dependent array dataset: 2188
Number of actual false labels in entire dependent array dataset: 1230
Number of actual true labels in entire dependent array dataset: 958
Number of total observations in test dataset: 438
Number of incorrect predictions on test dataset: 137
Number of correct predictions on test dataset: 301
Prediction accuracy on test dataset: 0.6872146118721462
Number of actual false labels in test dataset: 256
Number of actual true labels test dataset: 182
Majority class label percent in test dataset: 0.5844748858447488
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=912f40d5-1c7f-4aed-98d1-6fa40dfca689">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>The results of using ensemble methods are significant as we can see the accuracy scores. Based on this data sample, if we always predicted that the future closing price would be false, we would be correct 1230 out of 2188 times or 56.22% of the time. However, our model performs better, as our model has 301 correct predictions out of 438 observations or a correct prediction rate of 68.72%. Also, when looking at the actual false labels in the test dataset, which is the majority class, we see 256 observations; 256 out of 438 is 58.45%. Our model clearly performs better when compared against the actual labels in the entire dataset and when compared against the actual labels in the test dataset.</p>
<p>The results are significant, as we can outperform the expected market outcome (which assumes predicting the future price as false) by around 10% with this parameter combination when comparing against the actual values of the test set labels. This demonstrates the effectiveness of the model in identifying patterns and making predictions that exceed a baseline strategy of predicting no price increase of greater than 1% over the next ten days (which was the parameter combination I used for this ensemble model (pct_increase=1.01)).</p>
<p>I have demonstrated the implementation of the ensemble learning method (with stratified 5-fold cross-validation used to train these models) using the following parameters: The dependent variable represents the closing price 10 days in the future. The label for the dependent variable is assigned as 0 (false) if the future closing price is less than or equal to a 1% increase from the closing price of the last identified candle. It is labeled as 1 (true) if the future closing price is greater than a 1% increase from the closing price of the last identified candle.</p>
<p>I will run this implementation multiple times through multiple parameter combinations using independent array #15 as mentioned before, I found it to be the best performing combination of independent variables. I will run this separately outside of this document, for the submission for the next part of this project.</p>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=8679d2e2-3626-40aa-8b66-789288024684">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h1 id="Reporting---E">Reporting - <em>E</em><a class="anchor-link" href="#Reporting---E"></a></h1><p>To restate my research question:</p>
<p><em>Using historical market stock prices and technical indicators (RSI, MACD, MFI), how accurately can a neural network model, specifically an LSTM-based Recurrent Neural Network, predict stock price movements after the occurrence of a bullish candlestick pattern (e.g., 1 day, 3 days, 5 days, 10 days, and 15 days afterwards)? Additionally, how does the performance differ when using binary classification versus regression for price prediction?</em></p>
<p>To directly answer the first question, my LSTM-based Recurrent Neural Network can accurately predict stock price movements following the occurrence of a bullish candlestick pattern. Specifically, it performs about 9-10% better in predicting the outcome compared to when a random occurrence is used. By "random occurrence," I mean a randomly generated set of 30-day sequences that do not depend on the presence of a candlestick pattern on the 30th day of the sequence.</p>
<p>To directly answer the second question, the performance is more consistent and predictable while using a classification model in predicting price fluctuations.</p>
<p>Below, I will answer in high detail the first and second part of the research question and how I came to each conclusion.</p>
<h4 id="Answering-the-second-part-of-my-research-question">Answering the second part of my research question<a class="anchor-link" href="#Answering-the-second-part-of-my-research-question"></a></h4><p>To answer the second part of my research question, I conclude that the LSTM classification model was the better model overall for my purposes of this research project. The reason for this is as follows:</p>
<p>When comparing the results from the classification and regression models, its easier to identify the most effective independent variables for the classification model. It seems that the top-performing independent variables for the regression models were somewhat random. This is because, for the classification model, independent array #15 yielded the best results, showing the highest average and maximum accuracy scores across different combinations of model parameters.</p>
<p>These model parameters were used to define the dependent variable. Specifically, I had two key parameters when training the models: the number of days ahead from the last closing price in the 30-day sequence that I am predicting, and the percentage change from the last closing price of the 30-day sequence (e.g., whether the closing price on the specified future day is greater than, 1% higher than, or 2% higher than the last closing price on the 30th day of the sequence).</p>
<p>The classification results indicate that when the candlestick pattern is set to "Random" (<strong>I am using the "Random" pattern because the actual candlestick patterns have too small a data size</strong>)  meaning that random 30-day sequences are generated without relying on a specific candlestick pattern  independent array #15 produces the best outcomes, achieving the top spot 7 times. Independent array #14 follows closely in second place, with 6 occurrences at the top for yielding the highest average of maximum and average accuracy scores. I selected the candlestick pattern to "Random" because I wanted to analyze a larger sample size. Both arrays use all available independent variables (open, close, low, high, RSI, MFI, MACD, Signal Line), but there is a difference in how they process the data. Independent array #14 applies a log transformation to the open, close, low, and high prices, while independent array #15 uses Sklearns scaler function to normalize these prices. The classification results are as shown below:</p>
<ul>
<li>The classification results table is self-explanatory, but to clarify:<ul>
<li>"Most Frequent Class": This represents the class (negative or positive) that occurs more frequently within the entire dependent variable dataset.</li>
<li>"Highest Frequency": This is calculated as the ratio of the "Most Frequent Class" frequency to the "Total Observations" (i.e., Most Frequent Class / Total Observations). This metric is used to evaluate whether the model's accuracy scores are better than simply predicting the most frequent class.</li>
<li>"Weighted Score": The weighted score is calculated as the average of the "Best Accuracy" and "Avg Accuracy" (i.e., (Best Accuracy + Avg Accuracy) / 2). The purpose of this metric is to account for the possibility that the "Best Accuracy" might be an outlier or lucky result, and by combining it with the "Avg Accuracy," we get a more balanced measure.</li>
</ul>
</li>
</ul>
<table>
<thead>
<tr>
<th>Ticker</th>
<th>Pattern</th>
<th>Independent Array</th>
<th>Best Accuracy</th>
<th>Avg Accuracy</th>
<th>Days Out</th>
<th>Total Observations</th>
<th>Negative Observations</th>
<th>Positive Observations</th>
<th>Percent Increase Parameter</th>
<th>Most Frequent Class</th>
<th>Highest Frequency</th>
<th>Weighted Score</th>
</tr>
</thead>
<tbody>
<tr>
<td>SPY</td>
<td>Random</td>
<td>independent_array12</td>
<td>0.850000024</td>
<td>0.586346157</td>
<td>1</td>
<td>199</td>
<td>89</td>
<td>110</td>
<td>1</td>
<td>110</td>
<td>0.552763819</td>
<td>0.71817309</td>
</tr>
<tr>
<td>SPY</td>
<td>Random</td>
<td>independent_array15</td>
<td>0.899999976</td>
<td>0.832910252</td>
<td>1</td>
<td>199</td>
<td>165</td>
<td>34</td>
<td>1.01</td>
<td>165</td>
<td>0.829145729</td>
<td>0.866455114</td>
</tr>
<tr>
<td>SPY</td>
<td>Random</td>
<td>independent_array1</td>
<td>0.974358976</td>
<td>0.954871786</td>
<td>1</td>
<td>199</td>
<td>190</td>
<td>9</td>
<td>1.02</td>
<td>190</td>
<td>0.954773869</td>
<td>0.964615381</td>
</tr>
<tr>
<td>SPY</td>
<td>Random</td>
<td>independent_array14</td>
<td>0.824999988</td>
<td>0.614487181</td>
<td>3</td>
<td>199</td>
<td>89</td>
<td>110</td>
<td>1</td>
<td>110</td>
<td>0.552763819</td>
<td>0.719743585</td>
</tr>
<tr>
<td>SPY</td>
<td>Random</td>
<td>independent_array14</td>
<td>0.820512831</td>
<td>0.699025648</td>
<td>3</td>
<td>199</td>
<td>138</td>
<td>61</td>
<td>1.01</td>
<td>138</td>
<td>0.693467337</td>
<td>0.759769239</td>
</tr>
<tr>
<td>SPY</td>
<td>Random</td>
<td>independent_array15</td>
<td>0.925000012</td>
<td>0.890512816</td>
<td>3</td>
<td>199</td>
<td>176</td>
<td>23</td>
<td>1.02</td>
<td>176</td>
<td>0.884422111</td>
<td>0.907756414</td>
</tr>
<tr>
<td>SPY</td>
<td>Random</td>
<td>independent_array15</td>
<td>0.824999988</td>
<td>0.647256423</td>
<td>5</td>
<td>199</td>
<td>76</td>
<td>123</td>
<td>1</td>
<td>123</td>
<td>0.618090452</td>
<td>0.736128206</td>
</tr>
<tr>
<td>SPY</td>
<td>Random</td>
<td>independent_array15</td>
<td>0.794871807</td>
<td>0.651641024</td>
<td>5</td>
<td>199</td>
<td>122</td>
<td>77</td>
<td>1.01</td>
<td>122</td>
<td>0.613065327</td>
<td>0.723256416</td>
</tr>
<tr>
<td>SPY</td>
<td>Random</td>
<td>independent_array14</td>
<td>0.948717952</td>
<td>0.852961555</td>
<td>5</td>
<td>199</td>
<td>169</td>
<td>30</td>
<td>1.02</td>
<td>169</td>
<td>0.849246231</td>
<td>0.900839753</td>
</tr>
<tr>
<td>SPY</td>
<td>Random</td>
<td>independent_array15</td>
<td>0.850000024</td>
<td>0.662666665</td>
<td>10</td>
<td>199</td>
<td>71</td>
<td>128</td>
<td>1</td>
<td>128</td>
<td>0.64321608</td>
<td>0.756333345</td>
</tr>
<tr>
<td>SPY</td>
<td>Random</td>
<td>independent_array14</td>
<td>0.800000012</td>
<td>0.632756413</td>
<td>10</td>
<td>199</td>
<td>105</td>
<td>94</td>
<td>1.01</td>
<td>105</td>
<td>0.527638191</td>
<td>0.716378213</td>
</tr>
<tr>
<td>SPY</td>
<td>Random</td>
<td>independent_array15</td>
<td>0.820512831</td>
<td>0.739974356</td>
<td>10</td>
<td>199</td>
<td>138</td>
<td>61</td>
<td>1.02</td>
<td>138</td>
<td>0.693467337</td>
<td>0.780243593</td>
</tr>
<tr>
<td>SPY</td>
<td>Random</td>
<td>independent_array15</td>
<td>0.820512831</td>
<td>0.64825641</td>
<td>15</td>
<td>199</td>
<td>73</td>
<td>126</td>
<td>1</td>
<td>126</td>
<td>0.633165829</td>
<td>0.734384621</td>
</tr>
<tr>
<td>SPY</td>
<td>Random</td>
<td>independent_array14</td>
<td>0.800000012</td>
<td>0.613717952</td>
<td>15</td>
<td>199</td>
<td>100</td>
<td>99</td>
<td>1.01</td>
<td>100</td>
<td>0.502512563</td>
<td>0.706858982</td>
</tr>
<tr>
<td>SPY</td>
<td>Random</td>
<td>independent_array14</td>
<td>0.846153855</td>
<td>0.680448722</td>
<td>15</td>
<td>199</td>
<td>127</td>
<td>72</td>
<td>1.02</td>
<td>127</td>
<td>0.638190955</td>
<td>0.763301288</td>
</tr>
</tbody>
</table>
<br/>
<p>The regression results indicate that when the candlestick pattern is set to "Random"  meaning that random 30-day sequences are generated without relying on a specific candlestick pattern  independent array #1 produces the best outcomes, achieving the top spot 5 times. This is interesting because independent array #1 solely includes price action (open, close, high, low) independent variables. These variables are also not normalized. The classification results are as shown below:</p>
<ul>
<li>The regression results table is as follows:</li>
</ul>
<table>
<thead>
<tr>
<th>Ticker</th>
<th>Pattern</th>
<th>Independent Array</th>
<th>Best Accuracy</th>
<th>Avg Accuracy</th>
<th>Days Out</th>
<th>Total Observations</th>
<th>Negative Observations</th>
<th>Positive Observations</th>
<th>Percent Increase Parameter</th>
<th>Most Frequent Class</th>
<th>Highest Frequency</th>
<th>Weighted Score</th>
</tr>
</thead>
<tbody>
<tr>
<td>SPY</td>
<td>Random</td>
<td>independent_array15</td>
<td>0.846153846</td>
<td>0.619230769</td>
<td>1</td>
<td>199</td>
<td>89</td>
<td>110</td>
<td>1</td>
<td>110</td>
<td>0.552763819</td>
<td>0.732692308</td>
</tr>
<tr>
<td>SPY</td>
<td>Random</td>
<td>independent_array1</td>
<td>0.925</td>
<td>0.828461538</td>
<td>1</td>
<td>199</td>
<td>165</td>
<td>34</td>
<td>1.01</td>
<td>165</td>
<td>0.829145729</td>
<td>0.876730769</td>
</tr>
<tr>
<td>SPY</td>
<td>Random</td>
<td>independent_array1</td>
<td>1.0</td>
<td>0.954487179</td>
<td>1</td>
<td>199</td>
<td>190</td>
<td>9</td>
<td>1.02</td>
<td>190</td>
<td>0.954773869</td>
<td>0.97724359</td>
</tr>
<tr>
<td>SPY</td>
<td>Random</td>
<td>independent_array3</td>
<td>0.675</td>
<td>0.542564103</td>
<td>3</td>
<td>199</td>
<td>89</td>
<td>110</td>
<td>1</td>
<td>110</td>
<td>0.552763819</td>
<td>0.608782051</td>
</tr>
<tr>
<td>SPY</td>
<td>Random</td>
<td>independent_array11</td>
<td>0.923076923</td>
<td>0.709615385</td>
<td>3</td>
<td>199</td>
<td>138</td>
<td>61</td>
<td>1.01</td>
<td>138</td>
<td>0.693467337</td>
<td>0.816346154</td>
</tr>
<tr>
<td>SPY</td>
<td>Random</td>
<td>independent_array1</td>
<td>0.95</td>
<td>0.884102564</td>
<td>3</td>
<td>199</td>
<td>176</td>
<td>23</td>
<td>1.02</td>
<td>176</td>
<td>0.884422111</td>
<td>0.917051282</td>
</tr>
<tr>
<td>SPY</td>
<td>Random</td>
<td>independent_array6</td>
<td>0.775</td>
<td>0.582692308</td>
<td>5</td>
<td>199</td>
<td>76</td>
<td>123</td>
<td>1</td>
<td>123</td>
<td>0.618090452</td>
<td>0.678846154</td>
</tr>
<tr>
<td>SPY</td>
<td>Random</td>
<td>independent_array1</td>
<td>0.7</td>
<td>0.613076923</td>
<td>5</td>
<td>199</td>
<td>122</td>
<td>77</td>
<td>1.01</td>
<td>122</td>
<td>0.613065327</td>
<td>0.656538462</td>
</tr>
<tr>
<td>SPY</td>
<td>Random</td>
<td>independent_array1</td>
<td>0.925</td>
<td>0.849102564</td>
<td>5</td>
<td>199</td>
<td>169</td>
<td>30</td>
<td>1.02</td>
<td>169</td>
<td>0.849246231</td>
<td>0.887051282</td>
</tr>
<tr>
<td>SPY</td>
<td>Random</td>
<td>independent_array3</td>
<td>0.7</td>
<td>0.607948718</td>
<td>10</td>
<td>199</td>
<td>71</td>
<td>128</td>
<td>1</td>
<td>128</td>
<td>0.64321608</td>
<td>0.653974359</td>
</tr>
<tr>
<td>SPY</td>
<td>Random</td>
<td>independent_array14</td>
<td>0.775</td>
<td>0.558461538</td>
<td>10</td>
<td>199</td>
<td>105</td>
<td>94</td>
<td>1.01</td>
<td>105</td>
<td>0.527638191</td>
<td>0.666730769</td>
</tr>
<tr>
<td>SPY</td>
<td>Random</td>
<td>independent_array2</td>
<td>0.948717949</td>
<td>0.74474359</td>
<td>10</td>
<td>199</td>
<td>138</td>
<td>61</td>
<td>1.02</td>
<td>138</td>
<td>0.693467337</td>
<td>0.846730769</td>
</tr>
<tr>
<td>SPY</td>
<td>Random</td>
<td>independent_array6</td>
<td>0.675</td>
<td>0.573205128</td>
<td>15</td>
<td>199</td>
<td>73</td>
<td>126</td>
<td>1</td>
<td>126</td>
<td>0.633165829</td>
<td>0.624102564</td>
</tr>
<tr>
<td>SPY</td>
<td>Random</td>
<td>independent_array2</td>
<td>0.95</td>
<td>0.643846154</td>
<td>15</td>
<td>199</td>
<td>100</td>
<td>99</td>
<td>1.01</td>
<td>100</td>
<td>0.502512563</td>
<td>0.796923077</td>
</tr>
<tr>
<td>SPY</td>
<td>Random</td>
<td>independent_array11</td>
<td>0.95</td>
<td>0.794487179</td>
<td>15</td>
<td>199</td>
<td>127</td>
<td>72</td>
<td>1.02</td>
<td>127</td>
<td>0.638190955</td>
<td>0.87224359</td>
</tr>
</tbody>
</table>
<br/>
<p>Looking at the results, it seems that both the classification and regression models outperform simply predicting the most frequently occurring class. To determine which model performs betterclassification or regressionI will calculate the difference between the 'Weighted Score' and 'Highest Frequency' for each entry in the table, and then sum these differences across all observations. The model with the highest total of these differences will be considered the best performing, as it indicates that the model is yielding a greater improvement over the baseline of predicting the most frequent class, effectively demonstrating stronger overall performance.</p>
<table>
<thead>
<tr>
<th>Classification_Highest_frequency</th>
<th>Classification_Weighted_score</th>
<th>Classification_Differences</th>
</tr>
</thead>
<tbody>
<tr>
<td>0.552763819</td>
<td>0.71817309</td>
<td>0.165409271</td>
</tr>
<tr>
<td>0.829145729</td>
<td>0.866455114</td>
<td>0.037309385</td>
</tr>
<tr>
<td>0.954773869</td>
<td>0.964615381</td>
<td>0.009841511</td>
</tr>
<tr>
<td>0.552763819</td>
<td>0.719743585</td>
<td>0.166979766</td>
</tr>
<tr>
<td>0.693467337</td>
<td>0.759769239</td>
<td>0.066301903</td>
</tr>
<tr>
<td>0.884422111</td>
<td>0.907756414</td>
<td>0.023334303</td>
</tr>
<tr>
<td>0.618090452</td>
<td>0.736128206</td>
<td>0.118037753</td>
</tr>
<tr>
<td>0.613065327</td>
<td>0.723256416</td>
<td>0.110191089</td>
</tr>
<tr>
<td>0.849246231</td>
<td>0.900839753</td>
<td>0.051593522</td>
</tr>
<tr>
<td>0.64321608</td>
<td>0.756333345</td>
<td>0.113117264</td>
</tr>
<tr>
<td>0.527638191</td>
<td>0.716378213</td>
<td>0.188740022</td>
</tr>
<tr>
<td>0.693467337</td>
<td>0.780243593</td>
<td>0.086776257</td>
</tr>
<tr>
<td>0.633165829</td>
<td>0.734384621</td>
<td>0.101218792</td>
</tr>
<tr>
<td>0.502512563</td>
<td>0.706858982</td>
<td>0.204346419</td>
</tr>
<tr>
<td>0.638190955</td>
<td>0.763301288</td>
<td>0.125110334</td>
</tr>
</tbody>
</table>
<ul>
<li>The resulting sum of the differences for the classification table is: 1.568307591
<br/></li>
</ul>
<table>
<thead>
<tr>
<th>Regression_Highest_frequency</th>
<th>Regression_Weighted_score</th>
<th>Regression_Differences</th>
</tr>
</thead>
<tbody>
<tr>
<td>0.552763819</td>
<td>0.732692308</td>
<td>0.179928489</td>
</tr>
<tr>
<td>0.829145729</td>
<td>0.876730769</td>
<td>0.047585041</td>
</tr>
<tr>
<td>0.954773869</td>
<td>0.97724359</td>
<td>0.02246972</td>
</tr>
<tr>
<td>0.552763819</td>
<td>0.608782051</td>
<td>0.056018232</td>
</tr>
<tr>
<td>0.693467337</td>
<td>0.816346154</td>
<td>0.122878817</td>
</tr>
<tr>
<td>0.884422111</td>
<td>0.917051282</td>
<td>0.032629171</td>
</tr>
<tr>
<td>0.618090452</td>
<td>0.678846154</td>
<td>0.060755702</td>
</tr>
<tr>
<td>0.613065327</td>
<td>0.656538462</td>
<td>0.043473135</td>
</tr>
<tr>
<td>0.849246231</td>
<td>0.887051282</td>
<td>0.037805051</td>
</tr>
<tr>
<td>0.64321608</td>
<td>0.653974359</td>
<td>0.010758279</td>
</tr>
<tr>
<td>0.527638191</td>
<td>0.666730769</td>
<td>0.139092578</td>
</tr>
<tr>
<td>0.693467337</td>
<td>0.846730769</td>
<td>0.153263433</td>
</tr>
<tr>
<td>0.633165829</td>
<td>0.624102564</td>
<td>-0.009063265</td>
</tr>
<tr>
<td>0.502512563</td>
<td>0.796923077</td>
<td>0.294410514</td>
</tr>
<tr>
<td>0.638190955</td>
<td>0.87224359</td>
<td>0.234052635</td>
</tr>
</tbody>
</table>
<ul>
<li>The resulting sum of the differences for the regression table is: 1.426057531
<br/></li>
</ul>
<p>To conclude, the classification model emerged as the best performing model overall. However, it is important to acknowledge that the regression model outperformed the classification model in several specific parameter combinations. Despite this, I believe the classification model is the superior choice for this task due to its greater consistency. The best performing independent variables for the classification model consistently yield high accuracy scores (independent array #15), regardless of the parameter combinations (e.g., days out and percent increase).</p>
<p>In contrast, it is more challenging to identify which independent variables in the regression model contribute to its accuracy, as the performance tends to fluctuate more significantly based on different parameter settings. This variability in the regression models performance makes it less predictable and harder to interpret, whereas the classification models stability provides a more reliable foundation for decision-making.</p>
<p><strong>Lastly, and perhaps most importantly, while we are not analyzing a specific candlestick pattern here, but instead working with a randomly generated occurrence that simulates the presence of random 30-day sequences, we can clearly observe that our trained modelswhether for classification or regressionoutperform the "Highest Frequency" column. This metric is used to evaluate the accuracy of predicting the most frequent class. When compared to the "Weighted Score" columnrepresenting the accuracy score produced by our modelwe consistently find that the "Weighted Score" is higher in nearly all cases (and sometimes a lot more), regardless of the parameter combinations used. This shows us the efficacy of our model.</strong></p>
<h4 id="Answering-the-first-part-of-my-research-question">Answering the first part of my research question<a class="anchor-link" href="#Answering-the-first-part-of-my-research-question"></a></h4><p>Since I have decided that classification was the better performing model with independent array #15 being the best combination of indepdendent variables, I am going to use that model with that set of independent variables to answer this question. I have decided to not use the bullish engulfing, bullish harami, and three white soldiers patterns for this analysis because of their small sample size. Although the hammer and inverted patterns had small sample sizes as well, they were a bit larger of a sample size than the other three.</p>
<p>Below I am going to compare all three results: for a randomly generated occurrence, the hammer pattern occurrence, and the inverted hammer occurrence.</p>
<ul>
<li>Random Occurrence</li>
</ul>
<table>
<thead>
<tr>
<th>Ticker</th>
<th>Pattern</th>
<th>Independent Array</th>
<th>Best Accuracy</th>
<th>Avg Accuracy</th>
<th>Days Out</th>
<th>Total Observations</th>
<th>Negative Observations</th>
<th>Positive Observations</th>
<th>Percent Increase Parameter</th>
<th>Most Frequent Class</th>
<th>Classification Highest Frequency</th>
<th>Classification Weighted Score</th>
</tr>
</thead>
<tbody>
<tr>
<td>SPY</td>
<td>Random</td>
<td>independent_array15</td>
<td>0.75</td>
<td>0.569756417</td>
<td>1</td>
<td>199</td>
<td>89</td>
<td>110</td>
<td>1</td>
<td>110</td>
<td>0.552763819</td>
<td>0.659878208</td>
</tr>
<tr>
<td>SPY</td>
<td>Random</td>
<td>independent_array15</td>
<td>0.794871807</td>
<td>0.59938462</td>
<td>3</td>
<td>199</td>
<td>89</td>
<td>110</td>
<td>1</td>
<td>110</td>
<td>0.552763819</td>
<td>0.697128213</td>
</tr>
<tr>
<td>SPY</td>
<td>Random</td>
<td>independent_array15</td>
<td>0.824999988</td>
<td>0.647256423</td>
<td>5</td>
<td>199</td>
<td>76</td>
<td>123</td>
<td>1</td>
<td>123</td>
<td>0.618090452</td>
<td>0.736128206</td>
</tr>
<tr>
<td>SPY</td>
<td>Random</td>
<td>independent_array15</td>
<td>0.850000024</td>
<td>0.662666665</td>
<td>10</td>
<td>199</td>
<td>71</td>
<td>128</td>
<td>1</td>
<td>128</td>
<td>0.64321608</td>
<td>0.756333345</td>
</tr>
<tr>
<td>SPY</td>
<td>Random</td>
<td>independent_array15</td>
<td>0.820512831</td>
<td>0.64825641</td>
<td>15</td>
<td>199</td>
<td>73</td>
<td>126</td>
<td>1</td>
<td>126</td>
<td>0.633165829</td>
<td>0.734384621</td>
</tr>
<tr>
<td>SPY</td>
<td>Random</td>
<td>independent_array15</td>
<td>0.899999976</td>
<td>0.832910252</td>
<td>1</td>
<td>199</td>
<td>165</td>
<td>34</td>
<td>1.01</td>
<td>165</td>
<td>0.829145729</td>
<td>0.866455114</td>
</tr>
<tr>
<td>SPY</td>
<td>Random</td>
<td>independent_array15</td>
<td>0.794871807</td>
<td>0.708025651</td>
<td>3</td>
<td>199</td>
<td>138</td>
<td>61</td>
<td>1.01</td>
<td>138</td>
<td>0.693467337</td>
<td>0.751448729</td>
</tr>
<tr>
<td>SPY</td>
<td>Random</td>
<td>independent_array15</td>
<td>0.794871807</td>
<td>0.651641024</td>
<td>5</td>
<td>199</td>
<td>122</td>
<td>77</td>
<td>1.01</td>
<td>122</td>
<td>0.613065327</td>
<td>0.723256416</td>
</tr>
<tr>
<td>SPY</td>
<td>Random</td>
<td>independent_array15</td>
<td>0.774999976</td>
<td>0.635089748</td>
<td>10</td>
<td>199</td>
<td>105</td>
<td>94</td>
<td>1.01</td>
<td>105</td>
<td>0.527638191</td>
<td>0.705044862</td>
</tr>
<tr>
<td>SPY</td>
<td>Random</td>
<td>independent_array15</td>
<td>0.800000012</td>
<td>0.604820516</td>
<td>15</td>
<td>199</td>
<td>100</td>
<td>99</td>
<td>1.01</td>
<td>100</td>
<td>0.502512563</td>
<td>0.702410264</td>
</tr>
<tr>
<td>SPY</td>
<td>Random</td>
<td>independent_array15</td>
<td>0.974358976</td>
<td>0.954371786</td>
<td>1</td>
<td>199</td>
<td>190</td>
<td>9</td>
<td>1.02</td>
<td>190</td>
<td>0.954773869</td>
<td>0.964365381</td>
</tr>
<tr>
<td>SPY</td>
<td>Random</td>
<td>independent_array15</td>
<td>0.925000012</td>
<td>0.890512816</td>
<td>3</td>
<td>199</td>
<td>176</td>
<td>23</td>
<td>1.02</td>
<td>176</td>
<td>0.884422111</td>
<td>0.907756414</td>
</tr>
<tr>
<td>SPY</td>
<td>Random</td>
<td>independent_array15</td>
<td>0.899999976</td>
<td>0.859897449</td>
<td>5</td>
<td>199</td>
<td>169</td>
<td>30</td>
<td>1.02</td>
<td>169</td>
<td>0.849246231</td>
<td>0.879948713</td>
</tr>
<tr>
<td>SPY</td>
<td>Random</td>
<td>independent_array15</td>
<td>0.820512831</td>
<td>0.739974356</td>
<td>10</td>
<td>199</td>
<td>138</td>
<td>61</td>
<td>1.02</td>
<td>138</td>
<td>0.693467337</td>
<td>0.780243593</td>
</tr>
<tr>
<td>SPY</td>
<td>Random</td>
<td>independent_array15</td>
<td>0.794871807</td>
<td>0.670179485</td>
<td>15</td>
<td>199</td>
<td>127</td>
<td>72</td>
<td>1.02</td>
<td>127</td>
<td>0.638190955</td>
<td>0.732525646</td>
</tr>
</tbody>
</table>
<ul>
<li>Hammer Pattern Occurrence</li>
</ul>
<table>
<thead>
<tr>
<th>Ticker</th>
<th>Pattern</th>
<th>Independent Array</th>
<th>Best Accuracy</th>
<th>Avg Accuracy</th>
<th>Days Out</th>
<th>Total Observations</th>
<th>Negative Observations</th>
<th>Positive Observations</th>
<th>Percent Increase Parameter</th>
<th>Most Frequent Class</th>
<th>Classification Highest Frequency</th>
<th>Classification Weighted Score</th>
</tr>
</thead>
<tbody>
<tr>
<td>SPY</td>
<td>Hammer</td>
<td>independent_array15</td>
<td>1</td>
<td>0.733736265</td>
<td>1</td>
<td>69</td>
<td>37</td>
<td>32</td>
<td>1</td>
<td>37</td>
<td>0.536231884</td>
<td>0.866868132</td>
</tr>
<tr>
<td>SPY</td>
<td>Hammer</td>
<td>independent_array15</td>
<td>0.923076928</td>
<td>0.659780228</td>
<td>3</td>
<td>69</td>
<td>25</td>
<td>44</td>
<td>1</td>
<td>44</td>
<td>0.637681159</td>
<td>0.791428578</td>
</tr>
<tr>
<td>SPY</td>
<td>Hammer</td>
<td>independent_array15</td>
<td>0.928571403</td>
<td>0.742307696</td>
<td>5</td>
<td>69</td>
<td>25</td>
<td>44</td>
<td>1</td>
<td>44</td>
<td>0.637681159</td>
<td>0.83543955</td>
</tr>
<tr>
<td>SPY</td>
<td>Hammer</td>
<td>independent_array15</td>
<td>0.923076928</td>
<td>0.685164844</td>
<td>10</td>
<td>68</td>
<td>27</td>
<td>41</td>
<td>1</td>
<td>41</td>
<td>0.602941176</td>
<td>0.804120886</td>
</tr>
<tr>
<td>SPY</td>
<td>Hammer</td>
<td>independent_array15</td>
<td>0.857142866</td>
<td>0.707362645</td>
<td>15</td>
<td>68</td>
<td>31</td>
<td>37</td>
<td>1</td>
<td>37</td>
<td>0.544117647</td>
<td>0.782252755</td>
</tr>
<tr>
<td>SPY</td>
<td>Hammer</td>
<td>independent_array15</td>
<td>1</td>
<td>0.868571429</td>
<td>1</td>
<td>69</td>
<td>57</td>
<td>12</td>
<td>1.01</td>
<td>57</td>
<td>0.826086957</td>
<td>0.934285715</td>
</tr>
<tr>
<td>SPY</td>
<td>Hammer</td>
<td>independent_array15</td>
<td>0.785714269</td>
<td>0.669450558</td>
<td>3</td>
<td>69</td>
<td>43</td>
<td>26</td>
<td>1.01</td>
<td>43</td>
<td>0.623188406</td>
<td>0.727582414</td>
</tr>
<tr>
<td>SPY</td>
<td>Hammer</td>
<td>independent_array15</td>
<td>1</td>
<td>0.640659345</td>
<td>5</td>
<td>69</td>
<td>37</td>
<td>32</td>
<td>1.01</td>
<td>37</td>
<td>0.536231884</td>
<td>0.820329673</td>
</tr>
<tr>
<td>SPY</td>
<td>Hammer</td>
<td>independent_array15</td>
<td>0.928571403</td>
<td>0.700769237</td>
<td>10</td>
<td>68</td>
<td>35</td>
<td>33</td>
<td>1.01</td>
<td>35</td>
<td>0.514705882</td>
<td>0.81467032</td>
</tr>
<tr>
<td>SPY</td>
<td>Hammer</td>
<td>independent_array15</td>
<td>1</td>
<td>0.742417589</td>
<td>15</td>
<td>68</td>
<td>36</td>
<td>32</td>
<td>1.01</td>
<td>36</td>
<td>0.529411765</td>
<td>0.871208794</td>
</tr>
<tr>
<td>SPY</td>
<td>Hammer</td>
<td>independent_array15</td>
<td>1</td>
<td>0.957142842</td>
<td>1</td>
<td>69</td>
<td>66</td>
<td>3</td>
<td>1.02</td>
<td>66</td>
<td>0.956521739</td>
<td>0.978571421</td>
</tr>
<tr>
<td>SPY</td>
<td>Hammer</td>
<td>independent_array15</td>
<td>1</td>
<td>0.858351647</td>
<td>3</td>
<td>69</td>
<td>58</td>
<td>11</td>
<td>1.02</td>
<td>58</td>
<td>0.84057971</td>
<td>0.929175823</td>
</tr>
<tr>
<td>SPY</td>
<td>Hammer</td>
<td>independent_array15</td>
<td>0.785714269</td>
<td>0.728021987</td>
<td>5</td>
<td>69</td>
<td>51</td>
<td>18</td>
<td>1.02</td>
<td>51</td>
<td>0.739130435</td>
<td>0.756868128</td>
</tr>
<tr>
<td>SPY</td>
<td>Hammer</td>
<td>independent_array15</td>
<td>1</td>
<td>0.736593409</td>
<td>10</td>
<td>68</td>
<td>46</td>
<td>22</td>
<td>1.02</td>
<td>46</td>
<td>0.676470588</td>
<td>0.868296704</td>
</tr>
<tr>
<td>SPY</td>
<td>Hammer</td>
<td>independent_array15</td>
<td>1</td>
<td>0.67626375</td>
<td>15</td>
<td>68</td>
<td>41</td>
<td>27</td>
<td>1.02</td>
<td>41</td>
<td>0.602941176</td>
<td>0.838131875</td>
</tr>
</tbody>
</table>
<ul>
<li>Inverted Hammer Occurrence</li>
</ul>
<table>
<thead>
<tr>
<th>Ticker</th>
<th>Pattern</th>
<th>Independent Array</th>
<th>Best Accuracy</th>
<th>Avg Accuracy</th>
<th>Days Out</th>
<th>Total Observations</th>
<th>Negative Observations</th>
<th>Positive Observations</th>
<th>Percent Increase Parameter</th>
<th>Most Frequent Class</th>
<th>Classification Highest Frequency</th>
<th>Classification Weighted Score</th>
</tr>
</thead>
<tbody>
<tr>
<td>SPY</td>
<td>InvertedHammer</td>
<td>independent_array15</td>
<td>0.818181813</td>
<td>0.656545464</td>
<td>1</td>
<td>52</td>
<td>22</td>
<td>30</td>
<td>1</td>
<td>30</td>
<td>0.576923077</td>
<td>0.737363639</td>
</tr>
<tr>
<td>SPY</td>
<td>InvertedHammer</td>
<td>independent_array15</td>
<td>0.899999976</td>
<td>0.705818185</td>
<td>3</td>
<td>52</td>
<td>19</td>
<td>33</td>
<td>1</td>
<td>33</td>
<td>0.634615385</td>
<td>0.802909081</td>
</tr>
<tr>
<td>SPY</td>
<td>InvertedHammer</td>
<td>independent_array15</td>
<td>0.899999976</td>
<td>0.707454543</td>
<td>5</td>
<td>52</td>
<td>18</td>
<td>34</td>
<td>1</td>
<td>34</td>
<td>0.653846154</td>
<td>0.803727259</td>
</tr>
<tr>
<td>SPY</td>
<td>InvertedHammer</td>
<td>independent_array15</td>
<td>0.899999976</td>
<td>0.672545449</td>
<td>10</td>
<td>52</td>
<td>18</td>
<td>34</td>
<td>1</td>
<td>34</td>
<td>0.653846154</td>
<td>0.786272712</td>
</tr>
<tr>
<td>SPY</td>
<td>InvertedHammer</td>
<td>independent_array15</td>
<td>0.800000012</td>
<td>0.69054545</td>
<td>15</td>
<td>52</td>
<td>19</td>
<td>33</td>
<td>1</td>
<td>33</td>
<td>0.634615385</td>
<td>0.745272731</td>
</tr>
<tr>
<td>SPY</td>
<td>InvertedHammer</td>
<td>independent_array15</td>
<td>1</td>
<td>0.850181819</td>
<td>1</td>
<td>52</td>
<td>40</td>
<td>12</td>
<td>1.01</td>
<td>40</td>
<td>0.769230769</td>
<td>0.92509091</td>
</tr>
<tr>
<td>SPY</td>
<td>InvertedHammer</td>
<td>independent_array15</td>
<td>0.899999976</td>
<td>0.656181821</td>
<td>3</td>
<td>52</td>
<td>28</td>
<td>24</td>
<td>1.01</td>
<td>28</td>
<td>0.538461538</td>
<td>0.778090898</td>
</tr>
<tr>
<td>SPY</td>
<td>InvertedHammer</td>
<td>independent_array15</td>
<td>0.899999976</td>
<td>0.688181824</td>
<td>5</td>
<td>52</td>
<td>26</td>
<td>26</td>
<td>1.01</td>
<td>26</td>
<td>0.5</td>
<td>0.7940909</td>
</tr>
<tr>
<td>SPY</td>
<td>InvertedHammer</td>
<td>independent_array15</td>
<td>0.899999976</td>
<td>0.618181826</td>
<td>10</td>
<td>52</td>
<td>21</td>
<td>31</td>
<td>1.01</td>
<td>31</td>
<td>0.596153846</td>
<td>0.759090901</td>
</tr>
<tr>
<td>SPY</td>
<td>InvertedHammer</td>
<td>independent_array15</td>
<td>0.899999976</td>
<td>0.649636369</td>
<td>15</td>
<td>52</td>
<td>22</td>
<td>30</td>
<td>1.01</td>
<td>30</td>
<td>0.576923077</td>
<td>0.774818172</td>
</tr>
<tr>
<td>SPY</td>
<td>InvertedHammer</td>
<td>independent_array15</td>
<td>1</td>
<td>0.941818187</td>
<td>1</td>
<td>52</td>
<td>47</td>
<td>5</td>
<td>1.02</td>
<td>47</td>
<td>0.903846154</td>
<td>0.970909094</td>
</tr>
<tr>
<td>SPY</td>
<td>InvertedHammer</td>
<td>independent_array15</td>
<td>1</td>
<td>0.75036364</td>
<td>3</td>
<td>52</td>
<td>39</td>
<td>13</td>
<td>1.02</td>
<td>39</td>
<td>0.75</td>
<td>0.87518182</td>
</tr>
<tr>
<td>SPY</td>
<td>InvertedHammer</td>
<td>independent_array15</td>
<td>1</td>
<td>0.726363633</td>
<td>5</td>
<td>52</td>
<td>35</td>
<td>17</td>
<td>1.02</td>
<td>35</td>
<td>0.673076923</td>
<td>0.863181816</td>
</tr>
<tr>
<td>SPY</td>
<td>InvertedHammer</td>
<td>independent_array15</td>
<td>0.899999976</td>
<td>0.605454546</td>
<td>10</td>
<td>52</td>
<td>26</td>
<td>26</td>
<td>1.02</td>
<td>26</td>
<td>0.5</td>
<td>0.752727261</td>
</tr>
<tr>
<td>SPY</td>
<td>InvertedHammer</td>
<td>independent_array15</td>
<td>1</td>
<td>0.677272734</td>
<td>15</td>
<td>52</td>
<td>26</td>
<td>26</td>
<td>1.02</td>
<td>26</td>
<td>0.5</td>
<td>0.838636367</td>
</tr>
</tbody>
</table>
<p>Based on the tables above, it appears that the model returns higher accuracy scores when trained on candlestick patterns. For example, for each table if you take the sum of the "Classification Highest Frequency" and the "Classification Weighted Score" columns and subtract them this will give us the distance of our weighted accuracy to predicting the majority class. For each of these tables, the values are as follows:</p>
<ul>
<li>Random Occurrence: SUM(Classification_Weighted_score) - SUM(Classification_Highest_frequency) = 1.411378076</li>
<li>Hammer Occurrence: SUM(Classification_Weighted_score) - SUM(Classification_Highest_frequency) = 2.815309199</li>
<li>Inverted Hammer Occurrence: SUM(Classification_Weighted_score) - SUM(Classification_Highest_frequency) = 2.7458251</li>
</ul>
<p>Now, if I take each of these values and divide them by 15 (which is the total number of parameter combinations), I get the average weighted average accuracy scores across all 15 parameter combinations.</p>
<ul>
<li>Random Occurrence: 1.411378076 / 15 = 0.09409187173</li>
<li>Hammer Occurrence: 2.815309199 / 15 = 0.18768727993</li>
<li>Inverted Hammer Occurrence: 2.7458251 / 15 = 0.18305500666</li>
</ul>
<p><strong>This shows that, for each parameter combination where a true candlestick pattern is present, the model performs approximately 9-10% better in predicting the outcome compared to when a random occurrence is used.</strong></p>
<p>It is possible that because the total number of observations for random occurrences was set higher than for the hammer and inverted hammer patterns (which are fixed values), the model may have struggled to learn the patterns effectively due to the larger data set for random occurrences. However, upon reviewing the results, it seems that when a true candlestick pattern is present, the model is able to predict the future closing price with significantly higher accuracy compared to when a candlestick pattern is not present or not likely to be present (which represents the randomly generated occurrence).</p>
<h4 id="Building-off-my-research-question">Building off my research question<a class="anchor-link" href="#Building-off-my-research-question"></a></h4><p>Because there was a small sample size for my true candlestick patterns, I was a little disappointed. The reason being is that I wanted to deploy my model for real-world scenarios, especially for swing trading, where accurate predictions based on candlestick patterns could lead to more informed and timely trading decisions. However, as I have shown although I have received high accuracy scores, the frequency for these patterns are quite rare, occurring once every hundred days, or about a 1% occurrence.</p>
<p>Later in this document, I increased the number of randomly generated sequences in order to make my model more adaptable for use on any given day. The goal was to train the model on over 2,000 randomly generated sequences, hoping that even when a true candlestick pattern is not present, the model could still make reliable predictions for future closing prices.</p>
<p>Through this approach, I found that by increasing the sample size for training, using stratified 5-fold cross-validation, and incorporating ensemble learning methods, my model was able to deliver reliable results. These results were significantly more accurate than simply predicting the majority class. For these predictions, I used a single parameter combination: the closing price 10 days out, with a price increase greater than 1% on that day to be considered a positive class. I used independent array #15 as the basis for my independent variables which has 8 different features (Normalized_close, Normalized_open, Normalized_low, Normalized_high, RSI, MFI, MACD, Signal Line).</p>
<p>The results were as follows:</p>
<ul>
<li>Number of total observations in entire dependent array dataset: 2188</li>
<li>Number of actual false labels in entire dependent array dataset: 1230</li>
<li>Number of actual true labels in entire dependent array dataset: 958</li>
<li>Number of total observations in test dataset: 438</li>
<li>Number of incorrect predictions on test dataset: 137</li>
<li>Number of correct predictions on test dataset: 301</li>
<li>Prediction accuracy on test dataset: 68.72%</li>
<li>Number of actual false labels in test dataset: 256</li>
<li>Number of actual true labels test dataset: 182</li>
<li>Majority class label percent in test dataset: 58.45%</li>
</ul>
<p>The results of using ensemble methods are significant as we can see the accuracy scores. Based on this data sample, if we always predicted that the future closing price would be false, we would be correct 1230 out of 2188 times or 56.22% of the time. However, our model performs better, as our model has 301 correct predictions out of 438 observations or a correct prediction rate of 68.72%. Also, when looking at the actual false labels in the test dataset, which is the majority class, we see 256 observations; 256 out of 438 is 58.45%. Our model clearly performs better when compared against the actual labels in the entire dependent array (dataset used for training/validation) dataset and when compared against the actual labels in the test dataset.</p>
<p>The results are significant, as we can outperform the expected market outcome (which assumes predicting the future price as false) by around 10% with this parameter combination when comparing against the actual values of the test set labels. This demonstrates the effectiveness of the model in identifying patterns and making predictions that exceed a baseline strategy of predicting no price increase of greater than 1% over the next ten days (which was the parameter combination I used for this ensemble model (pct_increase=1.01)).</p>
<p>I have demonstrated the implementation of the ensemble learning method (with stratified 5-fold cross-validation used to train these models) using the following parameters: The dependent variable represents the closing price 10 days in the future. The label for the dependent variable is assigned as 0 (false) if the future closing price is less than or equal to a 1% increase from the closing price of the last identified candle. It is labeled as 1 (true) if the future closing price is greater than a 1% increase from the closing price of the last identified candle.</p>
<p>I will run this implementation multiple times through multiple parameter combinations using independent array #15 as mentioned before, I found it to be the best performing combination of independent variables. I will run this separately outside of this document, for the submission for the next part of this project. Specifically, I will build a web application which will allow a user to easily implement this ensemble model, using the same independent variables as found on independent array #15.</p>
<p>So, to answer the last part on the WGU grading rubric in which I am to propose a directions or approach for future study of the data set. The main thing I want to do is train an ensemble model for other stock tickers rather than just 'SPY' as shown in this project, and run those models on different parameter combinations as well to test the results. Again, this will be accomplished by creating a web application which allows me and any user to easily implement a trained model to predict stock price, for any stock ticker, for any parameter combination that is requested.</p>
<h1 id="Sources">Sources<a class="anchor-link" href="#Sources"></a></h1><p>Fidelity. (n.d.). RSI: Relative strength index. Fidelity. Retrieved December 26, 2024, from <a href="https://www.fidelity.com/learning-center/trading-investing/technical-analysis/technical-indicator-guide/RSI">https://www.fidelity.com/learning-center/trading-investing/technical-analysis/technical-indicator-guide/RSI</a></p>
<p>Wilder, J. W. (1978). New concepts in technical trading systems. Trend Research.</p>
<p>Fidelity. (n.d.). Money Flow Index (MFI). Fidelity. Retrieved December 26, 2024, from <a href="https://www.fidelity.com/learning-center/trading-investing/technical-analysis/technical-indicator-guide/mfi">https://www.fidelity.com/learning-center/trading-investing/technical-analysis/technical-indicator-guide/mfi</a></p>
<p>Fidelity. (n.d.). MACD: Moving average convergence divergence. Fidelity. Retrieved December 26, 2024, from <a href="https://www.fidelity.com/learning-center/trading-investing/technical-analysis/technical-indicator-guide/macd">https://www.fidelity.com/learning-center/trading-investing/technical-analysis/technical-indicator-guide/macd</a></p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=09705f84-a759-4bc9-bbb1-c84a6493f0d2">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span> 
</pre></div>
</div>
</div>
</div>
</div>
</div>
</main>
</body>
</html>
